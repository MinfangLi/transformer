{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11300,"status":"ok","timestamp":1695660493121,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"O_CScM4-fuFc","outputId":"502f2d25-15d1-441b-e874-89dd4b01f747"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# 挂在云盘\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# 扫描到相应的路径\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks')"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9954,"status":"ok","timestamp":1695660503072,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"1AMnplI3oiIA"},"outputs":[],"source":["from model import TransformerRegressor\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn as nn\n","import torch.optim as optim\n","from matplotlib import pyplot\n","import numpy as np\n","import os\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import pandas as pd\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1695660505939,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"PyZUS6OOo3F_"},"outputs":[],"source":["# lagged GHI values\n","LAG = 10\n","\n","# prediction horizon\n","K = 24\n","\n","# use exogenous inputs\n","EXOGENOUS = True\n","\n","# features\n","if(EXOGENOUS):\n","    features = ['K','uvIndex','cloudCover','sunshineDuration','windBearing','humidity','temperature','hour','dewPoint']\n","else:\n","    features = ['K']\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1695660509876,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"meRvkk2ipSEm"},"outputs":[],"source":["\n","# metrics\n","def mad(y_pred,y_test):\n","    return 100 / y_test.mean() * np.absolute(y_pred - y_test).sum() / y_pred.size\n","\n","def mdb(y_pred,y_test):\n","    return 100 / y_test.mean() * (y_pred - y_test).sum() / y_pred.size\n","\n","def r2(y_pred,y_test):\n","    return r2_score(y_test, y_pred)\n","\n","def rmsd(y_pred,y_test):\n","    return 100 / y_test.mean() * np.sqrt(np.sum(np.power(y_pred - y_test, 2)) / y_pred.size)\n","\n","def mae(y_pred,y_test):\n","    return mean_absolute_error(y_test, y_pred)\n","\n","def mse(y_pred,y_test):\n","    return mean_squared_error(y_test, y_pred)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1727,"status":"ok","timestamp":1695660514655,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"3L5xh4lIo74G","outputId":"558e0bb7-bf94-4d74-dc1e-b0aa62a539ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","<ipython-input-6-809853a42fcf>:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n","  df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n"]}],"source":["# load dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/datasets/clean_dataset.csv\",header=0, index_col=0, parse_dates=True).sort_index()\n","# split exogenous input and clear sky index (K)\n","df_GHI = df[['K']].copy()\n","\n","# create exogenous regressors\n","for feature in features:\n","    df_GHI[feature] = df[feature]\n","    for i in range(LAG-1):\n","        df_GHI[feature+'-'+str(i+1)] = df[feature].shift(i+1)\n","\n","# create target values\n","for i in range(1,K+1):\n","    #  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","    df_k= df['K'].shift(-i).rename('K+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_k],axis=1)\n","    \n","# create clear sky target values\n","for i in range(1,K+1):\n","    # df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","    df_cs = df['GHI_cs'].shift(-i).rename('GHI_cs+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_cs],axis=1)\n","\n","# drop nan due to shifting\n","df_GHI = df_GHI.dropna()\n","\n","# create training set\n","X_train = df_GHI['2010-1-1':'2014-6-30'].values[:,:-K*2]\n","y_train = df_GHI['2010-1-1':'2014-6-30'].values[:,-K*2:-K]\n","\n","# create validation set\n","X_val = df_GHI['2014-7-1':'2014-12-31'].values[:,:-K*2]\n","y_val = df_GHI['2014-7-1':'2014-12-31'].values[:,-K*2:-K]\n","\n","# create test set\n","X_test = df_GHI['2015-1-1':'2015-12-31'].values[:,:-K*2]\n","y_test = df_GHI['2015-1-1':'2015-12-31'].values[:,-K*2:-K]\n","\n","# get clear sky target values\n","y_cs = df_GHI['2015-1-1':'2015-12-31'].values[:,-K:]\n","\n","# scale features\n","scaler = MinMaxScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1695660520476,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"ZOFNy7Udo-zX"},"outputs":[],"source":["\n","epochs = 100\n","batch = 32\n","best_model = None\n","# best_mse = 99999999"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1695660522544,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"4-F34VsHpIGA"},"outputs":[],"source":["data = torch.from_numpy(X_train).type(torch.float32)\n","label = torch.from_numpy(y_train).type(torch.float32)\n","dataset_train = TensorDataset(data,label)\n","data_loader = DataLoader(dataset_train,batch_size=batch,shuffle=False)\n","data_val = torch.from_numpy(X_val).type(torch.float32)\n","label_val = torch.from_numpy(y_val).type(torch.float32)\n","dataset_val = TensorDataset(data_val,label_val)\n","dataval_loader = DataLoader(dataset_val,batch_size=batch,shuffle=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10728,"status":"ok","timestamp":1695660535861,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"OyjZ28KepWeV"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device =torch.device(\"cuda\")\n","writer = SummaryWriter('/content/drive/MyDrive/Colab Notebooks/lagslogs/')\n","input_d = 9*LAG\n","model = TransformerRegressor(input_dim=input_d,output_dim=1,num_heads=8,d_model=512).to(device)\n","criterion = nn.MSELoss().to(device)     # 忽略 占位符 索引为0.\n","optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n","val_loss = []\n","train_loss = []\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1695660539364,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"sOZw-HEOlfOf"},"outputs":[],"source":["data_test =torch.from_numpy(X_test).type(torch.float32)\n","label_test = torch.from_numpy(y_test).type(torch.float32)\n","dataset_test = TensorDataset(data_test,label_test)\n","datatest_loader = DataLoader(dataset_test,batch_size=1,shuffle=False)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1695660541473,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"8vC9MV4prmNk","outputId":"4e6c9622-6926-4056-f1ec-21cabe897115"},"outputs":[{"data":{"text/plain":["(24,)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\n","y_preds = np.zeros(y_cs.shape)\n","best_loss = np.zeros(y_cs.shape[1])\n","best_loss.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6438232,"status":"ok","timestamp":1695418792729,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"xz8BvxsPpjLl","outputId":"5873a77b-0939-4610-bd35-46f3042a1c12"},"outputs":[{"name":"stdout","output_type":"stream","text":["第120min的epoch: 0 train_epoch_loss: 0.01986568048596382 val_epoch_loss: 0.08183309960050805\n","1\n","best_test_loss: 0.08183309960050805\n","第120min的epoch: 1 train_epoch_loss: 0.0278899185359478 val_epoch_loss: 0.08134212778489766\n","1\n","best_test_loss: 0.08134212778489766\n","第120min的epoch: 2 train_epoch_loss: 0.019898880273103714 val_epoch_loss: 0.08145070125760538\n","1\n","第120min的epoch: 3 train_epoch_loss: 0.02127779647707939 val_epoch_loss: 0.08060426497325787\n","2\n","best_test_loss: 0.08060426497325787\n","第120min的epoch: 4 train_epoch_loss: 0.025701886042952538 val_epoch_loss: 0.0765514788032253\n","1\n","best_test_loss: 0.0765514788032253\n","第120min的epoch: 5 train_epoch_loss: 0.009328166022896767 val_epoch_loss: 0.05762451023079136\n","1\n","best_test_loss: 0.05762451023079136\n","第120min的epoch: 6 train_epoch_loss: 0.010848002508282661 val_epoch_loss: 0.05851716566271073\n","1\n","第120min的epoch: 7 train_epoch_loss: 0.00040337926475331187 val_epoch_loss: 0.04753810998789814\n","2\n","best_test_loss: 0.04753810998789814\n","第120min的epoch: 8 train_epoch_loss: 0.00040738077950663865 val_epoch_loss: 0.046795204170288394\n","1\n","best_test_loss: 0.046795204170288394\n","第120min的epoch: 9 train_epoch_loss: 0.0001177976664621383 val_epoch_loss: 0.04532184542444341\n","1\n","best_test_loss: 0.04532184542444341\n","第120min的epoch: 10 train_epoch_loss: 0.0003627026453614235 val_epoch_loss: 0.04436196877057135\n","1\n","best_test_loss: 0.04436196877057135\n","第120min的epoch: 11 train_epoch_loss: 0.000233746730373241 val_epoch_loss: 0.04345549303083106\n","1\n","best_test_loss: 0.04345549303083106\n","第120min的epoch: 12 train_epoch_loss: 0.00011447500583017245 val_epoch_loss: 0.0427110691548412\n","1\n","best_test_loss: 0.0427110691548412\n","第120min的epoch: 13 train_epoch_loss: 7.184866262832657e-05 val_epoch_loss: 0.042122082705599743\n","1\n","best_test_loss: 0.042122082705599743\n","第120min的epoch: 14 train_epoch_loss: 6.784826109651476e-05 val_epoch_loss: 0.04166204317549484\n","1\n","best_test_loss: 0.04166204317549484\n","第120min的epoch: 15 train_epoch_loss: 8.179701399058104e-05 val_epoch_loss: 0.04129313062955022\n","1\n","best_test_loss: 0.04129313062955022\n","第120min的epoch: 16 train_epoch_loss: 0.00010070036660181358 val_epoch_loss: 0.04100111854644097\n","1\n","best_test_loss: 0.04100111854644097\n","第120min的epoch: 17 train_epoch_loss: 0.00012175547453807667 val_epoch_loss: 0.04078358323656597\n","1\n","best_test_loss: 0.04078358323656597\n","第120min的epoch: 18 train_epoch_loss: 0.00014568497135769576 val_epoch_loss: 0.04063709601806571\n","1\n","best_test_loss: 0.04063709601806571\n","第120min的epoch: 19 train_epoch_loss: 0.00016758211131673306 val_epoch_loss: 0.040544208190099656\n","1\n","best_test_loss: 0.040544208190099656\n","第120min的epoch: 20 train_epoch_loss: 0.00018744473345577717 val_epoch_loss: 0.04049867786484949\n","1\n","best_test_loss: 0.04049867786484949\n","第120min的epoch: 21 train_epoch_loss: 0.00019872777920681983 val_epoch_loss: 0.04048936620053438\n","1\n","best_test_loss: 0.04048936620053438\n","第120min的epoch: 22 train_epoch_loss: 0.00020744527864735574 val_epoch_loss: 0.04052320615689324\n","1\n","第120min的epoch: 23 train_epoch_loss: 0.0002209214580943808 val_epoch_loss: 0.04061969078680124\n","2\n","第120min的epoch: 24 train_epoch_loss: 0.0002355572796659544 val_epoch_loss: 0.040765037417838976\n","3\n","第120min的epoch: 25 train_epoch_loss: 0.0002435890055494383 val_epoch_loss: 0.04095513201749834\n","4\n","第120min的epoch: 26 train_epoch_loss: 0.0002465124998707324 val_epoch_loss: 0.04121421927708281\n","5\n","第120min的epoch: 27 train_epoch_loss: 0.00025475703296251595 val_epoch_loss: 0.04154472350103169\n","6\n","第120min的epoch: 28 train_epoch_loss: 0.00026758856256492436 val_epoch_loss: 0.041951080827059915\n","7\n","第120min的epoch: 29 train_epoch_loss: 0.0002881459950003773 val_epoch_loss: 0.04242801264041006\n","8\n","第120min的epoch: 30 train_epoch_loss: 0.00030270114075392485 val_epoch_loss: 0.0429802587711321\n","9\n","第120min的epoch: 31 train_epoch_loss: 0.0003103679628111422 val_epoch_loss: 0.04358377011972745\n","10\n","第120min的epoch: 32 train_epoch_loss: 0.00030920319841243327 val_epoch_loss: 0.04424084354796714\n","11\n","第120min的epoch: 33 train_epoch_loss: 0.00029372729477472603 val_epoch_loss: 0.04494866879222304\n","12\n","第120min的epoch: 34 train_epoch_loss: 0.0002720580087043345 val_epoch_loss: 0.045690926944782564\n","13\n","第120min的epoch: 35 train_epoch_loss: 0.0002468703023623675 val_epoch_loss: 0.04646081491632619\n","14\n","第120min的epoch: 36 train_epoch_loss: 0.0002148753555957228 val_epoch_loss: 0.04725946879045586\n","15\n","第135min的epoch: 0 train_epoch_loss: 0.0004586828872561455 val_epoch_loss: 0.04887195244186117\n","1\n","best_test_loss: 0.04887195244186117\n","第135min的epoch: 1 train_epoch_loss: 0.00048224450438283384 val_epoch_loss: 0.049544069684016256\n","1\n","第135min的epoch: 2 train_epoch_loss: 0.00037443172186613083 val_epoch_loss: 0.0504545583145454\n","2\n","第135min的epoch: 3 train_epoch_loss: 0.00036724581150338054 val_epoch_loss: 0.05095846462992758\n","3\n","第135min的epoch: 4 train_epoch_loss: 0.00034001818858087063 val_epoch_loss: 0.05147563302684369\n","4\n","第135min的epoch: 5 train_epoch_loss: 0.00031938287429511547 val_epoch_loss: 0.05193546162102565\n","5\n","第135min的epoch: 6 train_epoch_loss: 0.00030040176352486014 val_epoch_loss: 0.05236633833594195\n","6\n","第135min的epoch: 7 train_epoch_loss: 0.0002754261367954314 val_epoch_loss: 0.05273633586078348\n","7\n","第135min的epoch: 8 train_epoch_loss: 0.00024780668900348246 val_epoch_loss: 0.05306684566988659\n","8\n","第135min的epoch: 9 train_epoch_loss: 0.00022221251856535673 val_epoch_loss: 0.053337971688302205\n","9\n","第135min的epoch: 10 train_epoch_loss: 0.00019670416077133268 val_epoch_loss: 0.05357379086895573\n","10\n","第135min的epoch: 11 train_epoch_loss: 0.00016923568909987807 val_epoch_loss: 0.05376092305216488\n","11\n","第135min的epoch: 12 train_epoch_loss: 0.0001412282435921952 val_epoch_loss: 0.05390016190270124\n","12\n","第135min的epoch: 13 train_epoch_loss: 0.0001151091419160366 val_epoch_loss: 0.05400100222561668\n","13\n","第135min的epoch: 14 train_epoch_loss: 9.589640103513375e-05 val_epoch_loss: 0.05411204793604749\n","14\n","第135min的epoch: 15 train_epoch_loss: 8.015758794499561e-05 val_epoch_loss: 0.05424655035404362\n","15\n","第150min的epoch: 0 train_epoch_loss: 0.0003397368418518454 val_epoch_loss: 0.05432632397244468\n","1\n","best_test_loss: 0.05432632397244468\n","第150min的epoch: 1 train_epoch_loss: 0.0006252220482565463 val_epoch_loss: 0.055041578094064986\n","1\n","第150min的epoch: 2 train_epoch_loss: 0.00045675906585529447 val_epoch_loss: 0.05509486514523999\n","2\n","第150min的epoch: 3 train_epoch_loss: 0.0004426308732945472 val_epoch_loss: 0.055183175456055535\n","3\n","第150min的epoch: 4 train_epoch_loss: 0.000391831825254485 val_epoch_loss: 0.055170894411249734\n","4\n","第150min的epoch: 5 train_epoch_loss: 0.00036465353332459927 val_epoch_loss: 0.05517794530910117\n","5\n","第150min的epoch: 6 train_epoch_loss: 0.00033895205706357956 val_epoch_loss: 0.05515943313198016\n","6\n","第150min的epoch: 7 train_epoch_loss: 0.00032219235436059535 val_epoch_loss: 0.05515647677413548\n","7\n","第150min的epoch: 8 train_epoch_loss: 0.0003078179433941841 val_epoch_loss: 0.055160701455663555\n","8\n","第150min的epoch: 9 train_epoch_loss: 0.000296776182949543 val_epoch_loss: 0.05518097231422713\n","9\n","第150min的epoch: 10 train_epoch_loss: 0.00029056024504825473 val_epoch_loss: 0.05523394744939625\n","10\n","第150min的epoch: 11 train_epoch_loss: 0.00028369645588099957 val_epoch_loss: 0.055292053760227194\n","11\n","第150min的epoch: 12 train_epoch_loss: 0.00028229402960278094 val_epoch_loss: 0.05537878801956338\n","12\n","第150min的epoch: 13 train_epoch_loss: 0.00028140749782323837 val_epoch_loss: 0.05547964816819792\n","13\n","第150min的epoch: 14 train_epoch_loss: 0.00027813282213173807 val_epoch_loss: 0.05559423093952806\n","14\n","第150min的epoch: 15 train_epoch_loss: 0.0002750598650891334 val_epoch_loss: 0.055726511041016696\n","15\n","第165min的epoch: 0 train_epoch_loss: 0.00045362577657215297 val_epoch_loss: 0.055604217401960865\n","1\n","best_test_loss: 0.055604217401960865\n","第165min的epoch: 1 train_epoch_loss: 0.0005438225925900042 val_epoch_loss: 0.056130422465250136\n","1\n","第165min的epoch: 2 train_epoch_loss: 0.0005398763460107148 val_epoch_loss: 0.056311385367735056\n","2\n","第165min的epoch: 3 train_epoch_loss: 0.0005535376840271056 val_epoch_loss: 0.05647904423587033\n","3\n","第165min的epoch: 4 train_epoch_loss: 0.0005719445762224495 val_epoch_loss: 0.05661976843570617\n","4\n","第165min的epoch: 5 train_epoch_loss: 0.0005912533961236477 val_epoch_loss: 0.05676885606389724\n","5\n","第165min的epoch: 6 train_epoch_loss: 0.000607708003371954 val_epoch_loss: 0.056909689130723404\n","6\n","第165min的epoch: 7 train_epoch_loss: 0.0006224780227057636 val_epoch_loss: 0.05705703171512089\n","7\n","第165min的epoch: 8 train_epoch_loss: 0.0006370048504322767 val_epoch_loss: 0.05721105047094803\n","8\n","第165min的epoch: 9 train_epoch_loss: 0.0006518110749311745 val_epoch_loss: 0.057369554548685024\n","9\n","第165min的epoch: 10 train_epoch_loss: 0.0006627202965319157 val_epoch_loss: 0.05752477988499922\n","10\n","第165min的epoch: 11 train_epoch_loss: 0.0006735713686794043 val_epoch_loss: 0.05769299603570539\n","11\n","第165min的epoch: 12 train_epoch_loss: 0.0006869030767120421 val_epoch_loss: 0.0578728167632674\n","12\n","第165min的epoch: 13 train_epoch_loss: 0.0006977158482186496 val_epoch_loss: 0.05804214229209575\n","13\n","第165min的epoch: 14 train_epoch_loss: 0.0007089475402608514 val_epoch_loss: 0.05821523073889008\n","14\n","第165min的epoch: 15 train_epoch_loss: 0.0007170932949520648 val_epoch_loss: 0.058380627168673156\n","15\n","第180min的epoch: 0 train_epoch_loss: 0.0008688378729857504 val_epoch_loss: 0.0573632172048404\n","1\n","best_test_loss: 0.0573632172048404\n","第180min的epoch: 1 train_epoch_loss: 0.0008433096809312701 val_epoch_loss: 0.057497614480372\n","1\n","第180min的epoch: 2 train_epoch_loss: 0.0008342071669176221 val_epoch_loss: 0.05764337982712449\n","2\n","第180min的epoch: 3 train_epoch_loss: 0.0008360052597709 val_epoch_loss: 0.05780941297669652\n","3\n","第180min的epoch: 4 train_epoch_loss: 0.0008325186790898442 val_epoch_loss: 0.05795114248093856\n","4\n","第180min的epoch: 5 train_epoch_loss: 0.0008257351582869887 val_epoch_loss: 0.05807424847918573\n","5\n","第180min的epoch: 6 train_epoch_loss: 0.0008206611964851618 val_epoch_loss: 0.05822377343576881\n","6\n","第180min的epoch: 7 train_epoch_loss: 0.0008135677198879421 val_epoch_loss: 0.05836977556559166\n","7\n","第180min的epoch: 8 train_epoch_loss: 0.0008077331585809588 val_epoch_loss: 0.05851383093239206\n","8\n","第180min的epoch: 9 train_epoch_loss: 0.0007971702725626528 val_epoch_loss: 0.058653644423812046\n","9\n","第180min的epoch: 10 train_epoch_loss: 0.0007883045473136008 val_epoch_loss: 0.058817044860178073\n","10\n","第180min的epoch: 11 train_epoch_loss: 0.0007821784820407629 val_epoch_loss: 0.0589925469177905\n","11\n","第180min的epoch: 12 train_epoch_loss: 0.0007732762023806572 val_epoch_loss: 0.05914526976625169\n","12\n","第180min的epoch: 13 train_epoch_loss: 0.0007647076272405684 val_epoch_loss: 0.05931517864392726\n","13\n","第180min的epoch: 14 train_epoch_loss: 0.0007582478574477136 val_epoch_loss: 0.05947392574634632\n","14\n","第180min的epoch: 15 train_epoch_loss: 0.0007498777122236788 val_epoch_loss: 0.05964518423697091\n","15\n","第195min的epoch: 0 train_epoch_loss: 0.0006865206523798406 val_epoch_loss: 0.05749165805225831\n","1\n","best_test_loss: 0.05749165805225831\n","第195min的epoch: 1 train_epoch_loss: 0.0005777368787676096 val_epoch_loss: 0.0572364419530841\n","1\n","best_test_loss: 0.0572364419530841\n","第195min的epoch: 2 train_epoch_loss: 0.0005370475118979812 val_epoch_loss: 0.057230391848176056\n","1\n","best_test_loss: 0.057230391848176056\n","第195min的epoch: 3 train_epoch_loss: 0.0005138113629072905 val_epoch_loss: 0.057346849229203224\n","1\n","第195min的epoch: 4 train_epoch_loss: 0.0005021708202548325 val_epoch_loss: 0.057536964002582056\n","2\n","第195min的epoch: 5 train_epoch_loss: 0.0004963691462762654 val_epoch_loss: 0.057771610848315016\n","3\n","第195min的epoch: 6 train_epoch_loss: 0.00048447976587340236 val_epoch_loss: 0.057992813817321985\n","4\n","第195min的epoch: 7 train_epoch_loss: 0.000474131345981732 val_epoch_loss: 0.05821430849246626\n","5\n","第195min的epoch: 8 train_epoch_loss: 0.00046733958879485726 val_epoch_loss: 0.05846745187789012\n","6\n","第195min的epoch: 9 train_epoch_loss: 0.00045863163541071117 val_epoch_loss: 0.05871807924014992\n","7\n","第195min的epoch: 10 train_epoch_loss: 0.0004491497529670596 val_epoch_loss: 0.05899712953731174\n","8\n","第195min的epoch: 11 train_epoch_loss: 0.00044230162166059017 val_epoch_loss: 0.05925813409598102\n","9\n","第195min的epoch: 12 train_epoch_loss: 0.0004354126285761595 val_epoch_loss: 0.05950096751355201\n","10\n","第195min的epoch: 13 train_epoch_loss: 0.00042856388608925045 val_epoch_loss: 0.05975123908164265\n","11\n","第195min的epoch: 14 train_epoch_loss: 0.0004231593629810959 val_epoch_loss: 0.05998621230553355\n","12\n","第195min的epoch: 15 train_epoch_loss: 0.0004217508831061423 val_epoch_loss: 0.06024142653563024\n","13\n","第195min的epoch: 16 train_epoch_loss: 0.0004195288347546011 val_epoch_loss: 0.06048783680795379\n","14\n","第195min的epoch: 17 train_epoch_loss: 0.000415758608141914 val_epoch_loss: 0.06072358024249442\n","15\n","第210min的epoch: 0 train_epoch_loss: 0.0003197470505256206 val_epoch_loss: 0.058695339949512\n","1\n","best_test_loss: 0.058695339949512\n","第210min的epoch: 1 train_epoch_loss: 0.0002448581508360803 val_epoch_loss: 0.05863201446304036\n","1\n","best_test_loss: 0.05863201446304036\n","第210min的epoch: 2 train_epoch_loss: 0.0002305672678630799 val_epoch_loss: 0.05873994636240768\n","1\n","第210min的epoch: 3 train_epoch_loss: 0.00022394253755919635 val_epoch_loss: 0.05891256320279828\n","2\n","第210min的epoch: 4 train_epoch_loss: 0.00021636481687892228 val_epoch_loss: 0.059165042013226914\n","3\n","第210min的epoch: 5 train_epoch_loss: 0.0002104065933963284 val_epoch_loss: 0.059465838153735844\n","4\n","第210min的epoch: 6 train_epoch_loss: 0.0002075223601423204 val_epoch_loss: 0.05982947410856192\n","5\n","第210min的epoch: 7 train_epoch_loss: 0.00019919105398003012 val_epoch_loss: 0.06011827800214804\n","6\n","第210min的epoch: 8 train_epoch_loss: 0.0001941544032888487 val_epoch_loss: 0.060472612667858215\n","7\n","第210min的epoch: 9 train_epoch_loss: 0.00018992627155967057 val_epoch_loss: 0.06084999749995457\n","8\n","第210min的epoch: 10 train_epoch_loss: 0.0001864809892140329 val_epoch_loss: 0.061279309409313275\n","9\n","第210min的epoch: 11 train_epoch_loss: 0.0001831581466831267 val_epoch_loss: 0.061750912364162934\n","10\n","第210min的epoch: 12 train_epoch_loss: 0.00018122153414878994 val_epoch_loss: 0.0622689731253163\n","11\n","第210min的epoch: 13 train_epoch_loss: 0.0001785468775779009 val_epoch_loss: 0.0628047813075761\n","12\n","第210min的epoch: 14 train_epoch_loss: 0.00017582240980118513 val_epoch_loss: 0.06335874040768681\n","13\n","第210min的epoch: 15 train_epoch_loss: 0.00017671874957159162 val_epoch_loss: 0.06392437420482315\n","14\n","第210min的epoch: 16 train_epoch_loss: 0.00017861068772617728 val_epoch_loss: 0.06451320095550107\n","15\n","第225min的epoch: 0 train_epoch_loss: 0.0001474326418247074 val_epoch_loss: 0.06219376910307208\n","1\n","best_test_loss: 0.06219376910307208\n","第225min的epoch: 1 train_epoch_loss: 0.00011146627366542816 val_epoch_loss: 0.06252789806358519\n","1\n","第225min的epoch: 2 train_epoch_loss: 0.00010106363333761692 val_epoch_loss: 0.06294787957952303\n","2\n","第225min的epoch: 3 train_epoch_loss: 8.8321408838965e-05 val_epoch_loss: 0.06363012875581789\n","3\n","第225min的epoch: 4 train_epoch_loss: 7.826374348951504e-05 val_epoch_loss: 0.06445459605993824\n","4\n","第225min的epoch: 5 train_epoch_loss: 7.295325485756621e-05 val_epoch_loss: 0.06552336703451647\n","5\n","第225min的epoch: 6 train_epoch_loss: 6.406856118701398e-05 val_epoch_loss: 0.06672722425555153\n","6\n","第225min的epoch: 7 train_epoch_loss: 5.9597037761704996e-05 val_epoch_loss: 0.0680116552776728\n","7\n","第225min的epoch: 8 train_epoch_loss: 5.146459807292558e-05 val_epoch_loss: 0.0692752213229181\n","8\n","第225min的epoch: 9 train_epoch_loss: 4.673016519518569e-05 val_epoch_loss: 0.0704828245765552\n","9\n","第225min的epoch: 10 train_epoch_loss: 4.426049781613983e-05 val_epoch_loss: 0.07169635489374715\n","10\n","第225min的epoch: 11 train_epoch_loss: 4.0684004488866776e-05 val_epoch_loss: 0.0726807939646868\n","11\n","第225min的epoch: 12 train_epoch_loss: 4.295398321119137e-05 val_epoch_loss: 0.0736944472605336\n","12\n","第225min的epoch: 13 train_epoch_loss: 4.698921475210227e-05 val_epoch_loss: 0.07468107030154383\n","13\n","第225min的epoch: 14 train_epoch_loss: 4.9978774768533185e-05 val_epoch_loss: 0.07578095489943767\n","14\n","第225min的epoch: 15 train_epoch_loss: 5.7271601690445095e-05 val_epoch_loss: 0.07700553825028744\n","15\n","第240min的epoch: 0 train_epoch_loss: 0.00012010148930130526 val_epoch_loss: 0.07574627585193847\n","1\n","best_test_loss: 0.07574627585193847\n","第240min的epoch: 1 train_epoch_loss: 9.52438495005481e-05 val_epoch_loss: 0.07694699706087414\n","1\n","第240min的epoch: 2 train_epoch_loss: 0.00011469051241874695 val_epoch_loss: 0.0787870664536961\n","2\n","第240min的epoch: 3 train_epoch_loss: 0.00012096016871510074 val_epoch_loss: 0.08081553430515273\n","3\n","第240min的epoch: 4 train_epoch_loss: 0.0001396209408994764 val_epoch_loss: 0.08319845078976605\n","4\n","第240min的epoch: 5 train_epoch_loss: 0.00016565197438467294 val_epoch_loss: 0.08570261047138363\n","5\n","第240min的epoch: 6 train_epoch_loss: 0.00015705759869888425 val_epoch_loss: 0.08741504352246261\n","6\n","第240min的epoch: 7 train_epoch_loss: 0.00012394454097375274 val_epoch_loss: 0.08820216570943623\n","7\n","第240min的epoch: 8 train_epoch_loss: 0.00011784236994571984 val_epoch_loss: 0.0900722448852347\n","8\n","第240min的epoch: 9 train_epoch_loss: 0.0001373337145196274 val_epoch_loss: 0.09234654313872127\n","9\n","第240min的epoch: 10 train_epoch_loss: 0.00015200866619125009 val_epoch_loss: 0.09454942487868918\n","10\n","第240min的epoch: 11 train_epoch_loss: 0.00017010330338962376 val_epoch_loss: 0.09684439464581374\n","11\n","第240min的epoch: 12 train_epoch_loss: 0.0001982023532036692 val_epoch_loss: 0.09899680406983796\n","12\n","第240min的epoch: 13 train_epoch_loss: 0.0002157847338821739 val_epoch_loss: 0.10087979093805188\n","13\n","第240min的epoch: 14 train_epoch_loss: 0.0002444287820253521 val_epoch_loss: 0.10296889232167682\n","14\n","第240min的epoch: 15 train_epoch_loss: 0.00026228823116980493 val_epoch_loss: 0.10463205495007982\n","15\n","第255min的epoch: 0 train_epoch_loss: 0.0006395518430508673 val_epoch_loss: 0.10859109955925016\n","1\n","best_test_loss: 0.10859109955925016\n","第255min的epoch: 1 train_epoch_loss: 0.0005466684233397245 val_epoch_loss: 0.10858055437943287\n","1\n","best_test_loss: 0.10858055437943287\n","第255min的epoch: 2 train_epoch_loss: 0.0005152525845915079 val_epoch_loss: 0.1097158707657861\n","1\n","第255min的epoch: 3 train_epoch_loss: 0.0005181847955100238 val_epoch_loss: 0.11158950958496712\n","2\n","第255min的epoch: 4 train_epoch_loss: 0.0005173119134269655 val_epoch_loss: 0.11330543270109363\n","3\n","第255min的epoch: 5 train_epoch_loss: 0.000521675799973309 val_epoch_loss: 0.1152735046021205\n","4\n","第255min的epoch: 6 train_epoch_loss: 0.0005384956602938473 val_epoch_loss: 0.11714053561339877\n","5\n","第255min的epoch: 7 train_epoch_loss: 0.0005580533179454505 val_epoch_loss: 0.11897920217429139\n","6\n","第255min的epoch: 8 train_epoch_loss: 0.000585115747526288 val_epoch_loss: 0.12075379474043839\n","7\n","第255min的epoch: 9 train_epoch_loss: 0.0006138388416729867 val_epoch_loss: 0.12248909162872883\n","8\n","第255min的epoch: 10 train_epoch_loss: 0.0006406654720194638 val_epoch_loss: 0.1240831940245909\n","9\n","第255min的epoch: 11 train_epoch_loss: 0.0006657943595200777 val_epoch_loss: 0.12552201596227477\n","10\n","第255min的epoch: 12 train_epoch_loss: 0.0006816860404796898 val_epoch_loss: 0.12677161171545184\n","11\n","第255min的epoch: 13 train_epoch_loss: 0.0006902370951138437 val_epoch_loss: 0.1278742880083197\n","12\n","第255min的epoch: 14 train_epoch_loss: 0.0007042602519504726 val_epoch_loss: 0.12908635389201506\n","13\n","第255min的epoch: 15 train_epoch_loss: 0.0007289901841431856 val_epoch_loss: 0.13041451899007897\n","14\n","第255min的epoch: 16 train_epoch_loss: 0.000743249140214175 val_epoch_loss: 0.13169135373048618\n","15\n","第270min的epoch: 0 train_epoch_loss: 0.0014590768842026591 val_epoch_loss: 0.1427052441762551\n","1\n","best_test_loss: 0.1427052441762551\n","第270min的epoch: 1 train_epoch_loss: 0.0013751945225521922 val_epoch_loss: 0.1419825873570095\n","1\n","best_test_loss: 0.1419825873570095\n","第270min的epoch: 2 train_epoch_loss: 0.0013114380417391658 val_epoch_loss: 0.1410763698013793\n","1\n","best_test_loss: 0.1410763698013793\n","第270min的epoch: 3 train_epoch_loss: 0.0012583083007484674 val_epoch_loss: 0.14046780093918063\n","1\n","best_test_loss: 0.14046780093918063\n","第270min的epoch: 4 train_epoch_loss: 0.0012200921773910522 val_epoch_loss: 0.14021816178886354\n","1\n","best_test_loss: 0.14021816178886354\n","第270min的epoch: 5 train_epoch_loss: 0.001175626995973289 val_epoch_loss: 0.14018541281921384\n","1\n","best_test_loss: 0.14018541281921384\n","第270min的epoch: 6 train_epoch_loss: 0.0011571520008146763 val_epoch_loss: 0.14044289290968268\n","1\n","第270min的epoch: 7 train_epoch_loss: 0.001139756408520043 val_epoch_loss: 0.14080112259387903\n","2\n","第270min的epoch: 8 train_epoch_loss: 0.0011178407585248351 val_epoch_loss: 0.1409471129112374\n","3\n","第270min的epoch: 9 train_epoch_loss: 0.0010894585866481066 val_epoch_loss: 0.14127596053573221\n","4\n","第270min的epoch: 10 train_epoch_loss: 0.0010653635254129767 val_epoch_loss: 0.14162756459542003\n","5\n","第270min的epoch: 11 train_epoch_loss: 0.0010462679201737046 val_epoch_loss: 0.14212135604685644\n","6\n","第270min的epoch: 12 train_epoch_loss: 0.0010217080125585198 val_epoch_loss: 0.14261329412081555\n","7\n","第270min的epoch: 13 train_epoch_loss: 0.0009983538184314966 val_epoch_loss: 0.14315919911588434\n","8\n","第270min的epoch: 14 train_epoch_loss: 0.0009737564250826836 val_epoch_loss: 0.1437831580235228\n","9\n","第270min的epoch: 15 train_epoch_loss: 0.0009487069910392165 val_epoch_loss: 0.1444617364232081\n","10\n","第270min的epoch: 16 train_epoch_loss: 0.00092176697216928 val_epoch_loss: 0.14524116333674444\n","11\n","第270min的epoch: 17 train_epoch_loss: 0.0009017443517223 val_epoch_loss: 0.14615648448523352\n","12\n","第270min的epoch: 18 train_epoch_loss: 0.0008720053592696786 val_epoch_loss: 0.14696551024425772\n","13\n","第270min的epoch: 19 train_epoch_loss: 0.000853417965117842 val_epoch_loss: 0.14789690708046005\n","14\n","第270min的epoch: 20 train_epoch_loss: 0.0008319941116496921 val_epoch_loss: 0.1488287039731707\n","15\n","第285min的epoch: 0 train_epoch_loss: 0.0009591436246410012 val_epoch_loss: 0.14896930464401256\n","1\n","best_test_loss: 0.14896930464401256\n","第285min的epoch: 1 train_epoch_loss: 0.0008981163264252245 val_epoch_loss: 0.14583810401603367\n","1\n","best_test_loss: 0.14583810401603367\n","第285min的epoch: 2 train_epoch_loss: 0.0008936389349400997 val_epoch_loss: 0.14378153171792687\n","1\n","best_test_loss: 0.14378153171792687\n","第285min的epoch: 3 train_epoch_loss: 0.0008945469162426889 val_epoch_loss: 0.14197610679632594\n","1\n","best_test_loss: 0.14197610679632594\n","第285min的epoch: 4 train_epoch_loss: 0.0008999642450362444 val_epoch_loss: 0.1402531339521427\n","1\n","best_test_loss: 0.1402531339521427\n","第285min的epoch: 5 train_epoch_loss: 0.0009059648727998137 val_epoch_loss: 0.13880932707020227\n","1\n","best_test_loss: 0.13880932707020227\n","第285min的epoch: 6 train_epoch_loss: 0.000910066650249064 val_epoch_loss: 0.13755962463693466\n","1\n","best_test_loss: 0.13755962463693466\n","第285min的epoch: 7 train_epoch_loss: 0.0009246279369108379 val_epoch_loss: 0.1364675942912573\n","1\n","best_test_loss: 0.1364675942912573\n","第285min的epoch: 8 train_epoch_loss: 0.0009338370873592794 val_epoch_loss: 0.13557860094189653\n","1\n","best_test_loss: 0.13557860094189653\n","第285min的epoch: 9 train_epoch_loss: 0.0009355911170132458 val_epoch_loss: 0.1348238285545317\n","1\n","best_test_loss: 0.1348238285545317\n","第285min的epoch: 10 train_epoch_loss: 0.000937506090849638 val_epoch_loss: 0.1343224778267567\n","1\n","best_test_loss: 0.1343224778267567\n","第285min的epoch: 11 train_epoch_loss: 0.0009422287694178522 val_epoch_loss: 0.13407072807108372\n","1\n","best_test_loss: 0.13407072807108372\n","第285min的epoch: 12 train_epoch_loss: 0.0009356355294585228 val_epoch_loss: 0.13387532038008879\n","1\n","best_test_loss: 0.13387532038008879\n","第285min的epoch: 13 train_epoch_loss: 0.0009337818482890725 val_epoch_loss: 0.13402025593550998\n","1\n","第285min的epoch: 14 train_epoch_loss: 0.0009170315461233258 val_epoch_loss: 0.1342315186406452\n","2\n","第285min的epoch: 15 train_epoch_loss: 0.00090960250236094 val_epoch_loss: 0.13470869504480093\n","3\n","第285min的epoch: 16 train_epoch_loss: 0.0008898329106159508 val_epoch_loss: 0.1352923556346534\n","4\n","第285min的epoch: 17 train_epoch_loss: 0.0008855517371557653 val_epoch_loss: 0.13609745029374398\n","5\n","第285min的epoch: 18 train_epoch_loss: 0.0008704316569492221 val_epoch_loss: 0.13706257572725325\n","6\n","第285min的epoch: 19 train_epoch_loss: 0.0008507895981892943 val_epoch_loss: 0.13795012952070226\n","7\n","第285min的epoch: 20 train_epoch_loss: 0.0008292293059639633 val_epoch_loss: 0.13883481722028143\n","8\n","第285min的epoch: 21 train_epoch_loss: 0.0008078201208263636 val_epoch_loss: 0.13991589642956978\n","9\n","第285min的epoch: 22 train_epoch_loss: 0.0007955683977343142 val_epoch_loss: 0.14103735057300504\n","10\n","第285min的epoch: 23 train_epoch_loss: 0.0007782088941894472 val_epoch_loss: 0.14220623107489108\n","11\n","第285min的epoch: 24 train_epoch_loss: 0.0007558317738585174 val_epoch_loss: 0.1434508879610803\n","12\n","第285min的epoch: 25 train_epoch_loss: 0.0007365950150415301 val_epoch_loss: 0.1447058161814483\n","13\n","第285min的epoch: 26 train_epoch_loss: 0.0007119086803868413 val_epoch_loss: 0.1458585195324976\n","14\n","第285min的epoch: 27 train_epoch_loss: 0.0006880327127873898 val_epoch_loss: 0.1470220310317624\n","15\n","第300min的epoch: 0 train_epoch_loss: 0.0007414129795506597 val_epoch_loss: 0.14007841915973468\n","1\n","best_test_loss: 0.14007841915973468\n","第300min的epoch: 1 train_epoch_loss: 0.0007188046001829207 val_epoch_loss: 0.13572503382406928\n","1\n","best_test_loss: 0.13572503382406928\n","第300min的epoch: 2 train_epoch_loss: 0.0007380230817943811 val_epoch_loss: 0.1330368358623304\n","1\n","best_test_loss: 0.1330368358623304\n","第300min的epoch: 3 train_epoch_loss: 0.0007823417545296252 val_epoch_loss: 0.1310944273476628\n","1\n","best_test_loss: 0.1310944273476628\n","第300min的epoch: 4 train_epoch_loss: 0.0008174031972885132 val_epoch_loss: 0.12954453441193767\n","1\n","best_test_loss: 0.12954453441193767\n","第300min的epoch: 5 train_epoch_loss: 0.000845329777803272 val_epoch_loss: 0.12843424413238055\n","1\n","best_test_loss: 0.12843424413238055\n","第300min的epoch: 6 train_epoch_loss: 0.0008660158491693437 val_epoch_loss: 0.12775394527800044\n","1\n","best_test_loss: 0.12775394527800044\n","第300min的epoch: 7 train_epoch_loss: 0.0008864977862685919 val_epoch_loss: 0.12738207489630862\n","1\n","best_test_loss: 0.12738207489630862\n","第300min的epoch: 8 train_epoch_loss: 0.0009005500469356775 val_epoch_loss: 0.12739234464801932\n","1\n","第300min的epoch: 9 train_epoch_loss: 0.0008978927507996559 val_epoch_loss: 0.12750094164157813\n","2\n","第300min的epoch: 10 train_epoch_loss: 0.0008962877909652889 val_epoch_loss: 0.12796049951514538\n","3\n","第300min的epoch: 11 train_epoch_loss: 0.0008827783167362213 val_epoch_loss: 0.12859034219592644\n","4\n","第300min的epoch: 12 train_epoch_loss: 0.0008845084812492132 val_epoch_loss: 0.12953067037354535\n","5\n","第300min的epoch: 13 train_epoch_loss: 0.0008719913312233984 val_epoch_loss: 0.13037643342038896\n","6\n","第300min的epoch: 14 train_epoch_loss: 0.0008571406942792237 val_epoch_loss: 0.1312814523345509\n","7\n","第300min的epoch: 15 train_epoch_loss: 0.0008498558308929205 val_epoch_loss: 0.1321749110366395\n","8\n","第300min的epoch: 16 train_epoch_loss: 0.0008443563128821552 val_epoch_loss: 0.13307214549454133\n","9\n","第300min的epoch: 17 train_epoch_loss: 0.0008359815692529082 val_epoch_loss: 0.13397011226840297\n","10\n","第300min的epoch: 18 train_epoch_loss: 0.0008197835995815694 val_epoch_loss: 0.13483548405275686\n","11\n","第300min的epoch: 19 train_epoch_loss: 0.0007992158643901348 val_epoch_loss: 0.13562394404405073\n","12\n","第300min的epoch: 20 train_epoch_loss: 0.0007872906280681491 val_epoch_loss: 0.13630782539025793\n","13\n","第300min的epoch: 21 train_epoch_loss: 0.0007901630597189069 val_epoch_loss: 0.1369020480911865\n","14\n","第300min的epoch: 22 train_epoch_loss: 0.0007998640649020672 val_epoch_loss: 0.13754231043192017\n","15\n","第315min的epoch: 0 train_epoch_loss: 0.0009473703103139997 val_epoch_loss: 0.1240558940593526\n","1\n","best_test_loss: 0.1240558940593526\n","第315min的epoch: 1 train_epoch_loss: 0.000951003166846931 val_epoch_loss: 0.11628949609081062\n","1\n","best_test_loss: 0.11628949609081062\n","第315min的epoch: 2 train_epoch_loss: 0.0009523927583359182 val_epoch_loss: 0.11077824367342712\n","1\n","best_test_loss: 0.11077824367342712\n","第315min的epoch: 3 train_epoch_loss: 0.0009391698404215276 val_epoch_loss: 0.10692283844010503\n","1\n","best_test_loss: 0.10692283844010503\n","第315min的epoch: 4 train_epoch_loss: 0.0009396254317834973 val_epoch_loss: 0.10419606021805784\n","1\n","best_test_loss: 0.10419606021805784\n","第315min的epoch: 5 train_epoch_loss: 0.0009441083529964089 val_epoch_loss: 0.10226858368960033\n","1\n","best_test_loss: 0.10226858368960033\n","第315min的epoch: 6 train_epoch_loss: 0.0009695227490738034 val_epoch_loss: 0.10141793762907754\n","1\n","best_test_loss: 0.10141793762907754\n","第315min的epoch: 7 train_epoch_loss: 0.0009935316629707813 val_epoch_loss: 0.10084618764675975\n","1\n","best_test_loss: 0.10084618764675975\n","第315min的epoch: 8 train_epoch_loss: 0.0010048056719824672 val_epoch_loss: 0.10061941787073696\n","1\n","best_test_loss: 0.10061941787073696\n","第315min的epoch: 9 train_epoch_loss: 0.0010233265347778797 val_epoch_loss: 0.10045622673836381\n","1\n","best_test_loss: 0.10045622673836381\n","第315min的epoch: 10 train_epoch_loss: 0.0010381658794358373 val_epoch_loss: 0.100315550896124\n","1\n","best_test_loss: 0.100315550896124\n","第315min的epoch: 11 train_epoch_loss: 0.001050678314641118 val_epoch_loss: 0.10038079314783344\n","1\n","第315min的epoch: 12 train_epoch_loss: 0.001065101707354188 val_epoch_loss: 0.10041419668024004\n","2\n","第315min的epoch: 13 train_epoch_loss: 0.0010681907879188657 val_epoch_loss: 0.10044388081671744\n","3\n","第315min的epoch: 14 train_epoch_loss: 0.001069517689757049 val_epoch_loss: 0.10054337392079027\n","4\n","第315min的epoch: 15 train_epoch_loss: 0.001083247596397996 val_epoch_loss: 0.10068714314442384\n","5\n","第315min的epoch: 16 train_epoch_loss: 0.0010910432320088148 val_epoch_loss: 0.10094084525632713\n","6\n","第315min的epoch: 17 train_epoch_loss: 0.001095005078241229 val_epoch_loss: 0.10105369608124644\n","7\n","第315min的epoch: 18 train_epoch_loss: 0.0010903681395575404 val_epoch_loss: 0.10122587482223078\n","8\n","第315min的epoch: 19 train_epoch_loss: 0.0010913672158494592 val_epoch_loss: 0.10135912466743636\n","9\n","第315min的epoch: 20 train_epoch_loss: 0.0010961295338347554 val_epoch_loss: 0.10157647128706204\n","10\n","第315min的epoch: 21 train_epoch_loss: 0.0011022643884643912 val_epoch_loss: 0.10185427171350254\n","11\n","第315min的epoch: 22 train_epoch_loss: 0.0011015187483280897 val_epoch_loss: 0.10215322721784975\n","12\n","第315min的epoch: 23 train_epoch_loss: 0.0011150274658575654 val_epoch_loss: 0.10249104228041951\n","13\n","第315min的epoch: 24 train_epoch_loss: 0.0011236502323299646 val_epoch_loss: 0.10288627955373343\n","14\n","第315min的epoch: 25 train_epoch_loss: 0.0011206545168533921 val_epoch_loss: 0.10322765588985615\n","15\n","第330min的epoch: 0 train_epoch_loss: 0.0012808396713808179 val_epoch_loss: 0.09226680510091946\n","1\n","best_test_loss: 0.09226680510091946\n","第330min的epoch: 1 train_epoch_loss: 0.0012484000762924552 val_epoch_loss: 0.08707759145624511\n","1\n","best_test_loss: 0.08707759145624511\n","第330min的epoch: 2 train_epoch_loss: 0.0012320574605837464 val_epoch_loss: 0.08420077491736384\n","1\n","best_test_loss: 0.08420077491736384\n","第330min的epoch: 3 train_epoch_loss: 0.0012254018802195787 val_epoch_loss: 0.08236511668128778\n","1\n","best_test_loss: 0.08236511668128778\n","第330min的epoch: 4 train_epoch_loss: 0.001227197702974081 val_epoch_loss: 0.08121446373980439\n","1\n","best_test_loss: 0.08121446373980439\n","第330min的epoch: 5 train_epoch_loss: 0.0012366505106911063 val_epoch_loss: 0.08053630816204427\n","1\n","best_test_loss: 0.08053630816204427\n","第330min的epoch: 6 train_epoch_loss: 0.0012510789092630148 val_epoch_loss: 0.08008724258072925\n","1\n","best_test_loss: 0.08008724258072925\n","第330min的epoch: 7 train_epoch_loss: 0.0012652160366997123 val_epoch_loss: 0.07975932892346148\n","1\n","best_test_loss: 0.07975932892346148\n","第330min的epoch: 8 train_epoch_loss: 0.0012723385589197278 val_epoch_loss: 0.0796168763496309\n","1\n","best_test_loss: 0.0796168763496309\n","第330min的epoch: 9 train_epoch_loss: 0.001281015807762742 val_epoch_loss: 0.07954856214295748\n","1\n","best_test_loss: 0.07954856214295748\n","第330min的epoch: 10 train_epoch_loss: 0.0012934585101902485 val_epoch_loss: 0.07945715664718064\n","1\n","best_test_loss: 0.07945715664718064\n","第330min的epoch: 11 train_epoch_loss: 0.001306726597249508 val_epoch_loss: 0.0794279993173457\n","1\n","best_test_loss: 0.0794279993173457\n","第330min的epoch: 12 train_epoch_loss: 0.0013141613453626633 val_epoch_loss: 0.07939177186315224\n","1\n","best_test_loss: 0.07939177186315224\n","第330min的epoch: 13 train_epoch_loss: 0.0013314266689121723 val_epoch_loss: 0.07946043963792086\n","1\n","第330min的epoch: 14 train_epoch_loss: 0.0013429647078737617 val_epoch_loss: 0.0795959986744624\n","2\n","第330min的epoch: 15 train_epoch_loss: 0.0013571273302659392 val_epoch_loss: 0.07978355127662602\n","3\n","第330min的epoch: 16 train_epoch_loss: 0.0013735541142523289 val_epoch_loss: 0.07998772863272503\n","4\n","第330min的epoch: 17 train_epoch_loss: 0.001389228506013751 val_epoch_loss: 0.08016736084254512\n","5\n","第330min的epoch: 18 train_epoch_loss: 0.0014115020167082548 val_epoch_loss: 0.08038809364707972\n","6\n","第330min的epoch: 19 train_epoch_loss: 0.001424148096702993 val_epoch_loss: 0.08070908144746562\n","7\n","第330min的epoch: 20 train_epoch_loss: 0.0014323570067062974 val_epoch_loss: 0.08092816002402063\n","8\n","第330min的epoch: 21 train_epoch_loss: 0.0014519724063575268 val_epoch_loss: 0.08123243735526162\n","9\n","第330min的epoch: 22 train_epoch_loss: 0.0014733969001099467 val_epoch_loss: 0.08161117484690993\n","10\n","第330min的epoch: 23 train_epoch_loss: 0.0014948828611522913 val_epoch_loss: 0.08205959429640484\n","11\n","第330min的epoch: 24 train_epoch_loss: 0.00151120126247406 val_epoch_loss: 0.08246942126003189\n","12\n","第330min的epoch: 25 train_epoch_loss: 0.0015237069455906749 val_epoch_loss: 0.08293500230723547\n","13\n","第330min的epoch: 26 train_epoch_loss: 0.001540811499580741 val_epoch_loss: 0.08345415034758237\n","14\n","第330min的epoch: 27 train_epoch_loss: 0.0015639460179954767 val_epoch_loss: 0.08401950792471906\n","15\n","第345min的epoch: 0 train_epoch_loss: 0.002052153227850795 val_epoch_loss: 0.0775439656924615\n","1\n","best_test_loss: 0.0775439656924615\n","第345min的epoch: 1 train_epoch_loss: 0.0020482896361500025 val_epoch_loss: 0.0744728640924407\n","1\n","best_test_loss: 0.0744728640924407\n","第345min的epoch: 2 train_epoch_loss: 0.0020397580228745937 val_epoch_loss: 0.07272585192033043\n","1\n","best_test_loss: 0.07272585192033043\n","第345min的epoch: 3 train_epoch_loss: 0.0020392006263136864 val_epoch_loss: 0.07168617927569701\n","1\n","best_test_loss: 0.07168617927569701\n","第345min的epoch: 4 train_epoch_loss: 0.0020435331389307976 val_epoch_loss: 0.07101670476781757\n","1\n","best_test_loss: 0.07101670476781757\n","第345min的epoch: 5 train_epoch_loss: 0.002061617560684681 val_epoch_loss: 0.07070800337322103\n","1\n","best_test_loss: 0.07070800337322103\n","第345min的epoch: 6 train_epoch_loss: 0.0020836584735661745 val_epoch_loss: 0.07062800545026075\n","1\n","best_test_loss: 0.07062800545026075\n","第345min的epoch: 7 train_epoch_loss: 0.0021001892164349556 val_epoch_loss: 0.07067198399960509\n","1\n","第345min的epoch: 8 train_epoch_loss: 0.002130355918779969 val_epoch_loss: 0.07091374995914099\n","2\n","第345min的epoch: 9 train_epoch_loss: 0.0021596772130578756 val_epoch_loss: 0.07121582901985991\n","3\n","第345min的epoch: 10 train_epoch_loss: 0.002187061356380582 val_epoch_loss: 0.07154254531243182\n","4\n","第345min的epoch: 11 train_epoch_loss: 0.0022181582171469927 val_epoch_loss: 0.07193651408981194\n","5\n","第345min的epoch: 12 train_epoch_loss: 0.0022564667742699385 val_epoch_loss: 0.07237702255867813\n","6\n","第345min的epoch: 13 train_epoch_loss: 0.0022881622426211834 val_epoch_loss: 0.07285740512526136\n","7\n","第345min的epoch: 14 train_epoch_loss: 0.0023206272162497044 val_epoch_loss: 0.07332360944913133\n","8\n","第345min的epoch: 15 train_epoch_loss: 0.0023578910622745752 val_epoch_loss: 0.07389326277779469\n","9\n","第345min的epoch: 16 train_epoch_loss: 0.0024034325033426285 val_epoch_loss: 0.07451832299567826\n","10\n","第345min的epoch: 17 train_epoch_loss: 0.002450087107717991 val_epoch_loss: 0.07510909916574905\n","11\n","第345min的epoch: 18 train_epoch_loss: 0.002498096786439419 val_epoch_loss: 0.07578055488377594\n","12\n","第345min的epoch: 19 train_epoch_loss: 0.002543526003137231 val_epoch_loss: 0.07647598770816567\n","13\n","第345min的epoch: 20 train_epoch_loss: 0.002599666826426983 val_epoch_loss: 0.07727716690430458\n","14\n","第345min的epoch: 21 train_epoch_loss: 0.0026643103919923306 val_epoch_loss: 0.07810455432542991\n","15\n","第360min的epoch: 0 train_epoch_loss: 0.002558252075687051 val_epoch_loss: 0.07466288926493228\n","1\n","best_test_loss: 0.07466288926493228\n","第360min的epoch: 1 train_epoch_loss: 0.002645158674567938 val_epoch_loss: 0.0725535972088582\n","1\n","best_test_loss: 0.0725535972088582\n","第360min的epoch: 2 train_epoch_loss: 0.00270533817820251 val_epoch_loss: 0.07146221971513865\n","1\n","best_test_loss: 0.07146221971513865\n","第360min的epoch: 3 train_epoch_loss: 0.002787942299619317 val_epoch_loss: 0.07099907865547035\n","1\n","best_test_loss: 0.07099907865547035\n","第360min的epoch: 4 train_epoch_loss: 0.0028846566565334797 val_epoch_loss: 0.07091259900598611\n","1\n","best_test_loss: 0.07091259900598611\n","第360min的epoch: 5 train_epoch_loss: 0.0029912132304161787 val_epoch_loss: 0.0710777493726894\n","1\n","第360min的epoch: 6 train_epoch_loss: 0.003107815282419324 val_epoch_loss: 0.0714505416681936\n","2\n","第360min的epoch: 7 train_epoch_loss: 0.003239140845835209 val_epoch_loss: 0.07201420553526301\n","3\n","第360min的epoch: 8 train_epoch_loss: 0.003393904073163867 val_epoch_loss: 0.07281519343192433\n","4\n","第360min的epoch: 9 train_epoch_loss: 0.003561748191714287 val_epoch_loss: 0.07386619481764248\n","5\n","第360min的epoch: 10 train_epoch_loss: 0.0037360561545938253 val_epoch_loss: 0.07491265116740302\n","6\n","第360min的epoch: 11 train_epoch_loss: 0.003936885390430689 val_epoch_loss: 0.07620065647727418\n","7\n","第360min的epoch: 12 train_epoch_loss: 0.004147882107645273 val_epoch_loss: 0.07756510616762671\n","8\n","第360min的epoch: 13 train_epoch_loss: 0.00439762556925416 val_epoch_loss: 0.07934089765320465\n","9\n","第360min的epoch: 14 train_epoch_loss: 0.004656412173062563 val_epoch_loss: 0.08114779217884052\n","10\n","第360min的epoch: 15 train_epoch_loss: 0.004944933112710714 val_epoch_loss: 0.08317337165067\n","11\n","第360min的epoch: 16 train_epoch_loss: 0.005280667915940285 val_epoch_loss: 0.08549837103076414\n","12\n","第360min的epoch: 17 train_epoch_loss: 0.005633966997265816 val_epoch_loss: 0.0879847027523362\n","13\n","第360min的epoch: 18 train_epoch_loss: 0.006068202666938305 val_epoch_loss: 0.09076223778116611\n","14\n","第360min的epoch: 19 train_epoch_loss: 0.006514762528240681 val_epoch_loss: 0.09375576714586899\n","15\n"]}],"source":["for i in range(7,K):\n","  best_loss[i]=100000\n","  best_test_loss = best_loss[i]\n","  j=0#j作为判断是否是bestloss，需不需要跳出循环\n","  for epoch in range(epochs):\n","    train_epoch_loss = []\n","    val_epoch_loss = []\n","    if j >=15:\n","      break\n","    for X_train,y_train in data_loader:\n","        y = y_train[:,i]  #y:torch.size(32)\n","        y = y.unsqueeze(1) #y:32*1\n","        # print(y.shape)\n","        # print(\"************************************\")\n","        X_tr = np.expand_dims(X_train,0)\n","        y_tr = np.expand_dims(y,0)\n","        # print(y_tr.shape)\n","        # print(\"______________________________\")\n","        # print(X_tr)\n","        # print(\"______________________________\")\n","        input_xtr = X_tr.transpose(1, 0, 2) #数组转维度，(1,32,9) -->(32,1,9)\n","        input_ytr = y_tr.transpose(1, 0, 2) #(32,1,1)\n","        input_xtr = torch.tensor(input_xtr).to(device)\n","        input_ytr = torch.tensor(input_ytr).to(device)\n","        optimizer.zero_grad()\n","        output = model(input_xtr)#训练\n","        trainloss = criterion(output, input_ytr)\n","        # print(output.shape)\n","        # print(trainloss.shape)\n","        trainloss.backward()\n","        optimizer.step()\n","    train_epoch_loss.append(trainloss.item())\n","    train_loss.append(np.mean(train_epoch_loss))\n","    writer.add_scalar(\"train_loss\", np.mean(train_epoch_loss))\n","    for X_val, y_val in dataval_loader:\n","      y_val = y_val[:,i]\n","      y_val = y_val.unsqueeze(1)\n","      x_v = np.expand_dims(X_val, 0)\n","      y_v = np.expand_dims(y_val, 0)\n","      input_xval = x_v.transpose(1, 0, 2)\n","      input_yval = y_v.transpose(1, 0, 2)\n","      X_val = torch.tensor(input_xval).to(device)\n","      Y_val = torch.tensor(input_yval).to(device)\n","      output = model(X_val)\n","      valloss = criterion(output, Y_val)\n","      val_epoch_loss.append(valloss.item())\n","    val_loss.append(np.mean(val_epoch_loss))\n","    writer.add_scalar(\"val_loss\", np.mean(val_epoch_loss), epoch)\n","    print(\"第{0}min的epoch:\".format(15*(i+1)), epoch, \"train_epoch_loss:\", np.mean(train_epoch_loss), \"val_epoch_loss:\", np.mean(val_epoch_loss))\n","    j +=1\n","    print(j)\n","    # 保存下来最好的模型：\n","    if np.mean(val_epoch_loss) < best_test_loss:\n","        best_test_loss = np.mean(val_epoch_loss)\n","        j = 0\n","        best_model = model\n","        print(\"best_test_loss:\", best_test_loss)\n","        torch.save(best_model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/lagslogs/best_Transformer_trainModel_{i}.pth')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vs-eQHR-hPSU"},"outputs":[],"source":["for i in range(K):\n","  # model.load_state_dict(torch.load(f'/content/drive/MyDrive/Colab Notebooks/lagslogs/best_Transformer_trainModel_{i}.pth')) #GPU\n","  model.load_state_dict(torch.load(f'/content/drive/MyDrive/Colab Notebooks/lagslogs/best_Transformer_trainModel_{i}.pth',map_location=torch.device('cpu')))\n","  model.to(device)\n","  model.eval()\n","  num_clo = y_cs.shape[0]\n","  y_pred = []\n","  for X_test,y_test in datatest_loader:\n","    y_test = y_test[:,i]\n","    y_test = y_test.unsqueeze(1)\n","    x_tt = np.expand_dims(X_test, 0)\n","    y_tt = np.expand_dims(y_test, 0)\n","    input_xtt = x_tt.transpose(1, 0, 2)\n","    input_ytt = y_tt.transpose(1, 0, 2)\n","    # print(input_ytt.shape)\n","    X_tt = torch.tensor(input_xtt).to(device)\n","    Y_tt = torch.tensor(input_ytt).to(device)\n","    # print(Y_tt.shape)\n","    output = model(X_tt)\n","    # output = output.cpu().detach().numpy()[0]\n","    y_pred.append(output)\n","    Y_tt = Y_tt.cpu().detach().numpy()[0]\n","  # print(\"y_pred:\",y_pred)\n","  pred = [tensor.detach().cpu().numpy() for tensor in y_pred]\n","  pred = np.array(pred)\n","  pred =pred.reshape(num_clo)\n","  y_preds[:,i] = pred\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXLxk1sTsijy"},"outputs":[],"source":["y_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Umb4xdDcYglx"},"outputs":[],"source":["# transform to GHI\n","\n","y_pred2 = np.multiply(y_preds, y_cs)\n","\n","y_test2 = np.multiply(label_test,y_cs)\n","\n","# save results\n","results = {'K':[],'Time[min]':[],'MAD[%]':[],'R2':[],'RMSD[%]':[]}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGbAdGsnbNyf"},"outputs":[],"source":["y_test2 =y_test2.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4359bxZbJw3"},"outputs":[],"source":["\n","for i in range(K):\n","    results['K'].append(i+1)\n","    results['Time[min]'].append((i+1)*15)\n","    results['MAD[%]'].append(np.round(mad(y_pred2[:,i],y_test2[:,i]),2))\n","    results['R2'].append(np.round(r2(y_pred2[:,i],y_test2[:,i]),2))\n","    results['RMSD[%]'].append(np.round(rmsd(y_pred2[:,i],y_test2[:,i]),2))\n","\n","# create results dataframe\n","results = pd.DataFrame(results)\n","results = results.set_index('K')\n","\n","# save results\n","#if(EXOGENOUS):\n","#    results.to_csv('./results/'+model.name+'_exogenous.csv')\n","#else:\n","#    results.to_csv('./results/'+model.name+'.csv')\n","\n","# print results\n","results.head(K)"]},{"cell_type":"markdown","metadata":{"id":"SbcCSaC6jqLw"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNt93uFzdc08HzPKGSuBr5j","mount_file_id":"15NtdkpsuHug0zzdX7Kv3KVjF0qZNNaP_","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}
