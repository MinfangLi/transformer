{"cells":[{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"elapsed":249,"status":"error","timestamp":1698056061152,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"WA2gyXIZi4U5","outputId":"ca672d70-c9b4-4ec2-90f4-7e2e58a0e2ab"},"outputs":[],"source":["########################## dataload part###############\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import  MinMaxScaler\n","import torch\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","import os\n","# disable CPU warning\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","# lagged GHI values\n","LAG = 1\n","\n","# prediction horizon\n","K = 24\n","\n","EXOGENOUS = True\n","\n","# features\n","if(EXOGENOUS):\n","    features = ['K','uvIndex','cloudCover','sunshineDuration','windBearing','humidity','temperature','hour','dewPoint']\n","else:\n","    features = ['K']\n","\n","df = pd.read_csv(\"../clean_dataset.csv\",header=0, index_col=0, parse_dates=True).sort_index()\n","df_GHI = df[['K']].copy()\n","# create exogenous regressors\n","for feature in features:\n","    df_GHI[feature] = df[feature]\n","    for i in range(LAG-1):\n","        df_GHI[feature+'-'+str(i+1)] = df[feature].shift(i+1)\n","# create target values\n","for i in range(1,K+1):\n","    #  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","    df_k= df['K'].shift(-i).rename('K+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_k],axis=1)\n","\n","# create clear sky target values\n","for i in range(1,K+1):\n","    # df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","    df_cs = df['GHI_cs'].shift(-i).rename('GHI_cs+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_cs],axis=1)\n","\n","# drop nan due to shifting\n","df_GHI = df_GHI.dropna()\n","    # create training set\n","X_train = df_GHI['2010-1-1':'2014-6-30'].values[:,:-K*2]\n","y_train = df_GHI['2010-1-1':'2014-6-30'].values[:,-K*2:-K]\n","\n","# create validation set\n","X_val = df_GHI['2014-7-1':'2014-12-31'].values[:,:-K*2]\n","y_val = df_GHI['2014-7-1':'2014-12-31'].values[:,-K*2:-K]\n","# create test set\n","X_test = df_GHI['2015-1-1':'2015-12-31'].values[:,:-K*2]\n","y_test = df_GHI['2015-1-1':'2015-12-31'].values[:,-K*2:-K]\n","# get clear sky target values\n","y_cs = df_GHI['2015-1-1':'2015-12-31'].values[:,-K:]\n","# scale features\n","scaler = MinMaxScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n","X=[]\n","Y=[]\n","for i in range(0, X_train.shape[0]-K, K):\n","  # print('i:',i)\n","  data_X = np.expand_dims(X_train[i:i + K], 0)\n","  data_Y = np.expand_dims(y_train[i:i + K,0], 0)\n","  # print(data_X.shape)  # (1, 24, 9)\n","  # print(data_Y.shape)  # (1, 24, 24)\n","  X.append(data_X)\n","  Y.append(data_Y)\n","data_X= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","data_Y=np.concatenate(Y, axis=0)\n","X=[]\n","Y=[]\n","for i in range(0, X_val.shape[0]-K, K):\n","  datav_X = np.expand_dims(X_val[i:i + K], 0)\n","  datav_Y = np.expand_dims(y_val[i:i + K,0], 0)\n","  X.append(datav_X)\n","  Y.append(datav_Y)\n","X_val= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","y_val=np.concatenate(Y, axis=0)\n","\n","def data1_load(batch):\n","    data_train = torch.from_numpy(data_X).type(torch.float32)\n","    label_train = torch.from_numpy(data_Y).type(torch.float32)\n","    data_val = torch.from_numpy(X_val).type(torch.float32)\n","    label_val = torch.from_numpy(y_val).type(torch.float32)\n","    # data_test = torch.from_numpy(X_test).type(torch.float32)\n","    # label_test = torch.from_numpy(y_test).type(torch.float32)\n","\n","    dataset_train = TensorDataset(data_train,label_train)\n","    datatrain_loader = DataLoader(dataset_train,batch_size=batch,shuffle=False)  # 数据迭代器DataLoader\n","    dataset_val = TensorDataset(data_val,label_val)\n","    dataval_loader = DataLoader(dataset_val,batch_size=batch,shuffle=False)\n","    # dataset_test = TensorDataset(data_test,label_test)\n","\n","\n","    return datatrain_loader,dataval_loader,LAG\n","    # return datatrain_loader\n","\n","X=[]\n","Y=[]\n","Y_cs = []\n","for i in range(0, X_test.shape[0]-K, K):\n","  # print('i:',i)\n","  datat_X = np.expand_dims(X_test[i:i + K], 0)\n","  datat_Y = np.expand_dims(y_test[i:i + K,0], 0)\n","  datacs_Y = np.expand_dims(y_cs[i:i + K,0], 0)\n","  X.append(datat_X)\n","  Y.append(datat_Y)\n","  Y_cs.append(datacs_Y)\n","X_test= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","y_test=np.concatenate(Y, axis=0)\n","y_cs= np.concatenate(Y_cs, axis=0)\n","\n","def data2_load(batchtest):\n","    data_test = torch.from_numpy(X_test).type(torch.float32)\n","    label_test = torch.from_numpy(y_test).type(torch.float32)\n","\n","\n","    dataset_test = TensorDataset(data_test,label_test)\n","    datatest_loader = DataLoader(dataset_test,batch_size=batchtest,shuffle=False)\n","    return datatest_loader,label_test,y_cs"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"_EXuo8-LMrcz"},"outputs":[],"source":["###########model part####################################\n","import torch,math\n","import torch.nn as nn\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=64):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)    #64*512\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)    #64*1\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))    #256   model/2\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)# pe.requires_grad = False\n","        self.register_buffer('pe', pe)   #64*1*512\n","    def forward(self, x):     #[seq,batch,d_model]\n","        return x + self.pe[:x.size(0), :]   #64*64*512\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","          ########################自己修改过的########################\n","            # attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","            attn_scores = attn_scores.masked_fill(torch.tensor(mask == 0), -1e9)\n","          #############################################################\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def split_heads(self, x):\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output\n","class PositionWiseFeedForward(nn.Module):\n","    def __init__(self, d_model, dim_feedforward):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, dim_feedforward)\n","        self.fc2 = nn.Linear(dim_feedforward, d_model)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads,dim_feedforward=2048, dropout=0):\n","        super(EncoderLayer, self).__init__()\n","        self.d_model = d_model\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, dim_feedforward)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.w0 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.w1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.w2 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.linear0 = nn.Linear(d_model, d_model)\n","        self.linear1 = nn.Linear(d_model, d_model)\n","        self.linear2 = nn.Linear(d_model, d_model)\n","        self.linear3 = nn.Linear(12288, d_model)  # x.size()[1]*x.size()[2]\n","        self.linear4 = nn.Linear(9216, d_model)  # attn_output.size()[1]*attn_output.size()[2]\n","    def forward(self, x, mask=None):\n","        n_x1 = x.shape[1]\n","        n_x2 = x.shape[2]\n","#############################################\n","\n","        i = 3\n","        nn_x = n_x1 - i * (i + 1) // 2\n","        if nn_x <= 0 or nn_x > n_x1:\n","            attn_output = self.self_attn(x, x, x, mask)\n","            x = self.norm1(x + self.dropout(attn_output))\n","            ff_output = self.feed_forward(x)\n","            x = self.norm2(x + self.dropout(ff_output))\n","            return x\n","        else:\n","            x_part0 = x[:,:nn_x,:]\n","            # print('x_part0',x_part0.shape)\n","\n","            attn_output0 = self.self_attn(x_part0, x_part0, x_part0, mask)  # torch.Size([4, 10, 512])\n","            x_part1 = x[:,3:nn_x+3,:]\n","            attn_output1 = self.self_attn(x_part1, x_part1, x_part1, mask)\n","            x_part2 = x[:,6:,:]\n","            # print('权重：',self.w0,self.w1,self.w2)\n","            # print('x_part2:',x_part2.shape)\n","\n","            attn_output2 = self.self_attn(x_part2, x_part2, x_part2, mask)\n","            # print('attn_output012的结果已经计算好了')\n","            attn_output = torch.mul(self.w0, self.linear0(attn_output0)) + \\\n","                      torch.mul(self.w1, self.linear1(attn_output1)) + \\\n","                      torch.mul(self.w2, self.linear2(attn_output2))\n","            # print('attn_output的结果已经计算好了')\n","            l_dim3 = n_x1*n_x2\n","            l_dim4 = attn_output.shape[1]*attn_output.shape[2]\n","            # print('l_dim3:',l_dim3,'l_dim4:',l_dim4)\n","            x = self.linear3(x.view(x.size()[0],x.size()[1]*x.size()[2]))\n","            attn_output = self.linear4(attn_output.view(attn_output.size()[0],attn_output.size()[1]*attn_output.size()[2]))\n","            x = self.norm1(x + attn_output)\n","            ff_output = self.feed_forward(x)\n","            x = self.norm2(x + self.dropout(ff_output))\n","            return x\n","\n","\n","class TransformerRegressor(nn.Module):\n","    def __init__(self, input_dim, output_dim, num_heads, d_model):\n","        super(TransformerRegressor, self).__init__()\n","        self.embedding = nn.Linear(input_dim, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        ####这里就要开始考虑x.shape\n","        self.transformer = EncoderLayer(d_model,num_heads)\n","        self.linear = nn.Linear(d_model, output_dim)\n","\n","    def forward(self, x):\n","        # print(x.shape)  # torch.Size([32, 1, 9])\n","        x = self.embedding(x)  # torch.Size([32, 1, 512])\n","        # print('输入的shape:',x.shape)\n","        x = self.pos_encoder(x)  \n","        x = self.transformer(x)  \n","        x = self.linear(x)\n","        return x\n"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"HadsU4b1MZJr"},"outputs":[],"source":["###################train part##########################\n","# from model import TransformerRegressor\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.optim as optim\n","from matplotlib import pyplot\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","from datetime import datetime\n","# from data import datatrain_load,dataval_load\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","writer = SummaryWriter('./logs/')\n","\n","\n","#epochs\n","epochs = 500\n","\n","\n","\n","# metrics\n","def mad(y_pred,y_test):\n","    return 100 / y_test.mean() * np.absolute(y_pred - y_test).sum() / y_pred.size\n","\n","def mdb(y_pred,y_test):\n","    return 100 / y_test.mean() * (y_pred - y_test).sum() / y_pred.size\n","\n","def r2(y_pred,y_test):\n","    return r2_score(y_test, y_pred)\n","\n","def rmsd(y_pred,y_test):\n","    return 100 / y_test.mean() * np.sqrt(np.sum(np.power(y_pred - y_test, 2)) / y_pred.size)\n","\n","def mae(y_pred,y_test):\n","    return mean_absolute_error(y_test, y_pred)\n","\n","def mse(y_pred,y_test):\n","    return mean_squared_error(y_test, y_pred)"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"ushHakcx0st-"},"outputs":[],"source":["train_loader,val_loader,lag= data1_load(32)\n","test_loader,label_test,y_cs= data2_load(1)\n","num_clo = y_cs.shape[0]\n","Num_f = len(features)\n","input_d= Num_f*lag\n","model = TransformerRegressor(input_dim=input_d,output_dim=K,num_heads=8,d_model=512).to(device)\n","criterion = nn.MSELoss().to(device)     # 忽略 占位符 索引为0.\n","optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n","val_loss = []\n","train_loss = []\n","best_loss = 10000\n","y_preds = np.zeros(y_cs.shape)"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"4jKk-RRC03P6"},"outputs":[],"source":["def train_val():\n","    # j = 0\n","    best_test_loss = 9999999\n","    for epoch in range(epochs):\n","        # if j >=100:\n","        #     break\n","        # else: j +=1\n","        train_epoch_loss = []\n","        val_epoch_loss = []\n","        for X_train, Y_train in train_loader:\n","            input_xtr=X_train.clone().detach().requires_grad_(True)\n","            # print('input_xtr:',input_xtr.shape)\n","            input_ytr = Y_train.clone().detach().requires_grad_(True)\n","            input_xtr = input_xtr.clone().detach().to(device)\n","            input_ytr = input_ytr.clone().detach().to(device)\n","            optimizer.zero_grad()\n","            output = model(input_xtr).to(device)#训练\n","            trainloss = criterion(output, input_ytr).to(device)\n","            trainloss.backward()\n","            optimizer.step()\n","        train_epoch_loss.append(trainloss.item())\n","        train_loss.append(np.mean(train_epoch_loss))\n","        writer.add_scalar(\"train_loss\", np.mean(train_epoch_loss))\n","        for X_val, y_val in val_loader:\n","            X_val = X_val.clone().detach().to(device)\n","            Y_val = y_val.clone().detach().to(device)\n","            output = model(X_val).to(device)\n","            valloss = criterion(output, Y_val)\n","            val_epoch_loss.append(valloss.item())\n","        val_loss.append(np.mean(val_epoch_loss))\n","        writer.add_scalar(\"val_loss\", np.mean(val_epoch_loss), epoch)\n","        datetimeStr = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        print(datetimeStr +\"  Epoch【\" + str(epoch) +  \"】 \" +  \"train_loss:\" + str(round(np.mean(train_epoch_loss), 5)) + \"  val_loss:\" + str(round(np.mean(val_epoch_loss), 5)))\n","\n","        if np.mean(val_epoch_loss) < best_test_loss:\n","            # j = 0\n","            best_test_loss = np.mean(val_epoch_loss)\n","            best_model = model\n","            print(\"best_test_loss:\", best_test_loss)\n","            torch.save(best_model.state_dict(), './logs/best_Transformer_trainModel.pth')"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"k8bS1Q3-06Km"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-25 22:20:34  Epoch【0】 train_loss:0.1296  val_loss:0.11757\n","best_test_loss: 0.11756946729577106\n","2023-10-25 22:20:36  Epoch【1】 train_loss:0.06911  val_loss:0.05438\n","best_test_loss: 0.05438001340498095\n","2023-10-25 22:20:38  Epoch【2】 train_loss:0.04167  val_loss:0.08187\n","2023-10-25 22:20:40  Epoch【3】 train_loss:0.03766  val_loss:0.07989\n","2023-10-25 22:20:42  Epoch【4】 train_loss:0.06113  val_loss:0.23583\n","2023-10-25 22:20:44  Epoch【5】 train_loss:0.58238  val_loss:2.80486\n","2023-10-25 22:20:46  Epoch【6】 train_loss:0.33265  val_loss:0.29932\n","2023-10-25 22:20:48  Epoch【7】 train_loss:0.09264  val_loss:0.08589\n","2023-10-25 22:20:50  Epoch【8】 train_loss:0.08428  val_loss:0.08037\n","2023-10-25 22:20:52  Epoch【9】 train_loss:0.0723  val_loss:0.06617\n","2023-10-25 22:20:54  Epoch【10】 train_loss:0.07836  val_loss:0.05639\n","2023-10-25 22:20:55  Epoch【11】 train_loss:0.05461  val_loss:0.04709\n","best_test_loss: 0.0470899511128664\n","2023-10-25 22:20:57  Epoch【12】 train_loss:0.04642  val_loss:0.04035\n","best_test_loss: 0.0403500711950271\n","2023-10-25 22:20:59  Epoch【13】 train_loss:0.04069  val_loss:0.03485\n","best_test_loss: 0.03485374411810999\n","2023-10-25 22:21:01  Epoch【14】 train_loss:0.03591  val_loss:0.03068\n","best_test_loss: 0.030682156186388886\n","2023-10-25 22:21:03  Epoch【15】 train_loss:0.0319  val_loss:0.02784\n","best_test_loss: 0.027840795076411705\n","2023-10-25 22:21:05  Epoch【16】 train_loss:0.02961  val_loss:0.02595\n","best_test_loss: 0.025949184661326202\n","2023-10-25 22:21:07  Epoch【17】 train_loss:0.02808  val_loss:0.02465\n","best_test_loss: 0.024649665650466213\n","2023-10-25 22:21:09  Epoch【18】 train_loss:0.02695  val_loss:0.0236\n","best_test_loss: 0.023598688087709572\n","2023-10-25 22:21:11  Epoch【19】 train_loss:0.02593  val_loss:0.0227\n","best_test_loss: 0.022699949534043022\n","2023-10-25 22:21:13  Epoch【20】 train_loss:0.02498  val_loss:0.02198\n","best_test_loss: 0.021981383349908436\n","2023-10-25 22:21:15  Epoch【21】 train_loss:0.02395  val_loss:0.0212\n","best_test_loss: 0.021203835859246876\n","2023-10-25 22:21:17  Epoch【22】 train_loss:0.02295  val_loss:0.02055\n","best_test_loss: 0.020546041429042816\n","2023-10-25 22:21:19  Epoch【23】 train_loss:0.02195  val_loss:0.01987\n","best_test_loss: 0.019872311786141083\n","2023-10-25 22:21:21  Epoch【24】 train_loss:0.02101  val_loss:0.01921\n","best_test_loss: 0.019214497028809528\n","2023-10-25 22:21:23  Epoch【25】 train_loss:0.02023  val_loss:0.01861\n","best_test_loss: 0.018610447199772232\n","2023-10-25 22:21:25  Epoch【26】 train_loss:0.0196  val_loss:0.01807\n","best_test_loss: 0.018066968890311924\n","2023-10-25 22:21:27  Epoch【27】 train_loss:0.0191  val_loss:0.01754\n","best_test_loss: 0.01754408175854579\n","2023-10-25 22:21:29  Epoch【28】 train_loss:0.01867  val_loss:0.01703\n","best_test_loss: 0.017029453271433063\n","2023-10-25 22:21:31  Epoch【29】 train_loss:0.01829  val_loss:0.01653\n","best_test_loss: 0.016532168845119682\n","2023-10-25 22:21:33  Epoch【30】 train_loss:0.01796  val_loss:0.01605\n","best_test_loss: 0.016052386763951054\n","2023-10-25 22:21:35  Epoch【31】 train_loss:0.01765  val_loss:0.01559\n","best_test_loss: 0.015585978171261757\n","2023-10-25 22:21:37  Epoch【32】 train_loss:0.01755  val_loss:0.01528\n","best_test_loss: 0.015281853954429213\n","2023-10-25 22:21:39  Epoch【33】 train_loss:0.0173  val_loss:0.01489\n","best_test_loss: 0.014893625802158014\n","2023-10-25 22:21:41  Epoch【34】 train_loss:0.01695  val_loss:0.01443\n","best_test_loss: 0.014429918125919674\n","2023-10-25 22:21:43  Epoch【35】 train_loss:0.01672  val_loss:0.01406\n","best_test_loss: 0.014055955486939005\n","2023-10-25 22:21:44  Epoch【36】 train_loss:0.01653  val_loss:0.01375\n","best_test_loss: 0.013749547302722931\n","2023-10-25 22:21:46  Epoch【37】 train_loss:0.01636  val_loss:0.01348\n","best_test_loss: 0.013476976861610361\n","2023-10-25 22:21:48  Epoch【38】 train_loss:0.01621  val_loss:0.01324\n","best_test_loss: 0.013235555455574522\n","2023-10-25 22:21:50  Epoch【39】 train_loss:0.01606  val_loss:0.01301\n","best_test_loss: 0.013014698765524055\n","2023-10-25 22:21:52  Epoch【40】 train_loss:0.01592  val_loss:0.01281\n","best_test_loss: 0.01281138685653391\n","2023-10-25 22:21:54  Epoch【41】 train_loss:0.01579  val_loss:0.01264\n","best_test_loss: 0.012636867351830006\n","2023-10-25 22:21:56  Epoch【42】 train_loss:0.01564  val_loss:0.01246\n","best_test_loss: 0.012462276600949142\n","2023-10-25 22:21:58  Epoch【43】 train_loss:0.01553  val_loss:0.01232\n","best_test_loss: 0.012320750554942566\n","2023-10-25 22:22:00  Epoch【44】 train_loss:0.01542  val_loss:0.0122\n","best_test_loss: 0.012195115989964941\n","2023-10-25 22:22:02  Epoch【45】 train_loss:0.01531  val_loss:0.01208\n","best_test_loss: 0.012081410067722849\n","2023-10-25 22:22:04  Epoch【46】 train_loss:0.0152  val_loss:0.01197\n","best_test_loss: 0.011973711560763742\n","2023-10-25 22:22:06  Epoch【47】 train_loss:0.01509  val_loss:0.01187\n","best_test_loss: 0.011865385281650917\n","2023-10-25 22:22:08  Epoch【48】 train_loss:0.015  val_loss:0.01177\n","best_test_loss: 0.011772056049464838\n","2023-10-25 22:22:10  Epoch【49】 train_loss:0.0149  val_loss:0.01168\n","best_test_loss: 0.011675073886695116\n","2023-10-25 22:22:12  Epoch【50】 train_loss:0.0148  val_loss:0.01158\n","best_test_loss: 0.011578916936464932\n","2023-10-25 22:22:14  Epoch【51】 train_loss:0.01469  val_loss:0.01148\n","best_test_loss: 0.011480972656737204\n","2023-10-25 22:22:16  Epoch【52】 train_loss:0.01459  val_loss:0.01138\n","best_test_loss: 0.01138253090903163\n","2023-10-25 22:22:18  Epoch【53】 train_loss:0.01447  val_loss:0.01128\n","best_test_loss: 0.011284847986762938\n","2023-10-25 22:22:20  Epoch【54】 train_loss:0.01438  val_loss:0.01119\n","best_test_loss: 0.011188789331556662\n","2023-10-25 22:22:22  Epoch【55】 train_loss:0.01427  val_loss:0.01109\n","best_test_loss: 0.011091545892312475\n","2023-10-25 22:22:24  Epoch【56】 train_loss:0.01416  val_loss:0.01099\n","best_test_loss: 0.01099250362376156\n","2023-10-25 22:22:26  Epoch【57】 train_loss:0.01405  val_loss:0.01089\n","best_test_loss: 0.010893954692975334\n","2023-10-25 22:22:28  Epoch【58】 train_loss:0.01394  val_loss:0.01079\n","best_test_loss: 0.010792828488933004\n","2023-10-25 22:22:30  Epoch【59】 train_loss:0.01382  val_loss:0.01069\n","best_test_loss: 0.010692622998486395\n","2023-10-25 22:22:32  Epoch【60】 train_loss:0.01372  val_loss:0.01059\n","best_test_loss: 0.010592563867407001\n","2023-10-25 22:22:34  Epoch【61】 train_loss:0.01361  val_loss:0.01049\n","best_test_loss: 0.010494795465922874\n","2023-10-25 22:22:36  Epoch【62】 train_loss:0.01351  val_loss:0.01039\n","best_test_loss: 0.010393671205510263\n","2023-10-25 22:22:38  Epoch【63】 train_loss:0.01339  val_loss:0.01029\n","best_test_loss: 0.01029419888863745\n","2023-10-25 22:22:40  Epoch【64】 train_loss:0.01328  val_loss:0.01019\n","best_test_loss: 0.010190594192270351\n","2023-10-25 22:22:42  Epoch【65】 train_loss:0.01315  val_loss:0.01009\n","best_test_loss: 0.01008741792453372\n","2023-10-25 22:22:44  Epoch【66】 train_loss:0.01305  val_loss:0.00998\n","best_test_loss: 0.009983143809696903\n","2023-10-25 22:22:46  Epoch【67】 train_loss:0.01292  val_loss:0.00988\n","best_test_loss: 0.009880457113942375\n","2023-10-25 22:22:48  Epoch【68】 train_loss:0.01281  val_loss:0.00978\n","best_test_loss: 0.009775149716955164\n","2023-10-25 22:22:50  Epoch【69】 train_loss:0.01268  val_loss:0.00967\n","best_test_loss: 0.009671862561093725\n","2023-10-25 22:22:52  Epoch【70】 train_loss:0.01257  val_loss:0.00957\n","best_test_loss: 0.009565930165674376\n","2023-10-25 22:22:54  Epoch【71】 train_loss:0.01243  val_loss:0.00946\n","best_test_loss: 0.009462782384260841\n","2023-10-25 22:22:56  Epoch【72】 train_loss:0.01231  val_loss:0.00936\n","best_test_loss: 0.009357590926811099\n","2023-10-25 22:22:58  Epoch【73】 train_loss:0.01217  val_loss:0.00925\n","best_test_loss: 0.00925368223460796\n","2023-10-25 22:23:00  Epoch【74】 train_loss:0.01205  val_loss:0.00915\n","best_test_loss: 0.009150564215024528\n","2023-10-25 22:23:02  Epoch【75】 train_loss:0.0119  val_loss:0.00905\n","best_test_loss: 0.009045890947479917\n","2023-10-25 22:23:04  Epoch【76】 train_loss:0.01179  val_loss:0.00895\n","best_test_loss: 0.008946674217915406\n","2023-10-25 22:23:06  Epoch【77】 train_loss:0.01164  val_loss:0.00884\n","best_test_loss: 0.008841132654039107\n","2023-10-25 22:23:08  Epoch【78】 train_loss:0.01153  val_loss:0.00874\n","best_test_loss: 0.008742452781323505\n","2023-10-25 22:23:10  Epoch【79】 train_loss:0.01139  val_loss:0.00864\n","best_test_loss: 0.008642620660121675\n","2023-10-25 22:23:12  Epoch【80】 train_loss:0.01128  val_loss:0.00855\n","best_test_loss: 0.008546391208453671\n","2023-10-25 22:23:14  Epoch【81】 train_loss:0.01113  val_loss:0.00845\n","best_test_loss: 0.008449987872787144\n","2023-10-25 22:23:16  Epoch【82】 train_loss:0.01102  val_loss:0.00835\n","best_test_loss: 0.008353309432530532\n","2023-10-25 22:23:19  Epoch【83】 train_loss:0.01089  val_loss:0.00826\n","best_test_loss: 0.008260122652206084\n","2023-10-25 22:23:21  Epoch【84】 train_loss:0.01077  val_loss:0.00816\n","best_test_loss: 0.008162107328762826\n","2023-10-25 22:23:23  Epoch【85】 train_loss:0.01065  val_loss:0.00807\n","best_test_loss: 0.008066854652260308\n","2023-10-25 22:23:24  Epoch【86】 train_loss:0.01053  val_loss:0.00797\n","best_test_loss: 0.00797064952633303\n","2023-10-25 22:23:26  Epoch【87】 train_loss:0.01041  val_loss:0.00788\n","best_test_loss: 0.007875481529323302\n","2023-10-25 22:23:28  Epoch【88】 train_loss:0.01031  val_loss:0.00778\n","best_test_loss: 0.00778197668209348\n","2023-10-25 22:23:30  Epoch【89】 train_loss:0.01019  val_loss:0.00769\n","best_test_loss: 0.007690749525942881\n","2023-10-25 22:23:32  Epoch【90】 train_loss:0.01007  val_loss:0.0076\n","best_test_loss: 0.0076035512510039234\n","2023-10-25 22:23:34  Epoch【91】 train_loss:0.00997  val_loss:0.00752\n","best_test_loss: 0.007518365473041068\n","2023-10-25 22:23:36  Epoch【92】 train_loss:0.00987  val_loss:0.00744\n","best_test_loss: 0.007435486317895677\n","2023-10-25 22:23:38  Epoch【93】 train_loss:0.00977  val_loss:0.00735\n","best_test_loss: 0.007353967541585798\n","2023-10-25 22:23:40  Epoch【94】 train_loss:0.00968  val_loss:0.00728\n","best_test_loss: 0.0072763273694916916\n","2023-10-25 22:23:42  Epoch【95】 train_loss:0.00959  val_loss:0.0072\n","best_test_loss: 0.007199506548678745\n","2023-10-25 22:23:44  Epoch【96】 train_loss:0.0095  val_loss:0.00712\n","best_test_loss: 0.0071249526075047\n","2023-10-25 22:23:46  Epoch【97】 train_loss:0.00942  val_loss:0.00705\n","best_test_loss: 0.007054434765292251\n","2023-10-25 22:23:48  Epoch【98】 train_loss:0.00934  val_loss:0.00699\n","best_test_loss: 0.006985176314154397\n","2023-10-25 22:23:50  Epoch【99】 train_loss:0.00926  val_loss:0.00692\n","best_test_loss: 0.006917913484832515\n","2023-10-25 22:23:52  Epoch【100】 train_loss:0.00918  val_loss:0.00685\n","best_test_loss: 0.006851250609463971\n","2023-10-25 22:23:54  Epoch【101】 train_loss:0.0091  val_loss:0.00679\n","best_test_loss: 0.00678921891781299\n","2023-10-25 22:23:56  Epoch【102】 train_loss:0.00902  val_loss:0.00672\n","best_test_loss: 0.006723653413760273\n","2023-10-25 22:23:58  Epoch【103】 train_loss:0.00894  val_loss:0.00666\n","best_test_loss: 0.0066615568939596415\n","2023-10-25 22:24:00  Epoch【104】 train_loss:0.00884  val_loss:0.00659\n","best_test_loss: 0.006593751832437904\n","2023-10-25 22:24:02  Epoch【105】 train_loss:0.00878  val_loss:0.00654\n","best_test_loss: 0.006541681607537295\n","2023-10-25 22:24:03  Epoch【106】 train_loss:0.00869  val_loss:0.00648\n","best_test_loss: 0.0064780418052459545\n","2023-10-25 22:24:05  Epoch【107】 train_loss:0.00862  val_loss:0.00643\n","best_test_loss: 0.006427068850430457\n","2023-10-25 22:24:07  Epoch【108】 train_loss:0.00854  val_loss:0.00636\n","best_test_loss: 0.006364205011935986\n","2023-10-25 22:24:09  Epoch【109】 train_loss:0.00848  val_loss:0.00631\n","best_test_loss: 0.006311867872010107\n","2023-10-25 22:24:11  Epoch【110】 train_loss:0.00841  val_loss:0.00625\n","best_test_loss: 0.006252871623829655\n","2023-10-25 22:24:13  Epoch【111】 train_loss:0.0083  val_loss:0.00619\n","best_test_loss: 0.0061927813753161745\n","2023-10-25 22:24:15  Epoch【112】 train_loss:0.00827  val_loss:0.00616\n","best_test_loss: 0.006160361257017306\n","2023-10-25 22:24:17  Epoch【113】 train_loss:0.00819  val_loss:0.0061\n","best_test_loss: 0.006104507448111215\n","2023-10-25 22:24:19  Epoch【114】 train_loss:0.00815  val_loss:0.00607\n","best_test_loss: 0.006068520683228322\n","2023-10-25 22:24:21  Epoch【115】 train_loss:0.00811  val_loss:0.00603\n","best_test_loss: 0.0060342900029829016\n","2023-10-25 22:24:23  Epoch【116】 train_loss:0.00806  val_loss:0.006\n","best_test_loss: 0.005996750333629872\n","2023-10-25 22:24:25  Epoch【117】 train_loss:0.00796  val_loss:0.00595\n","best_test_loss: 0.005945118027739227\n","2023-10-25 22:24:27  Epoch【118】 train_loss:0.00787  val_loss:0.00589\n","best_test_loss: 0.005894873949253689\n","2023-10-25 22:24:29  Epoch【119】 train_loss:0.00769  val_loss:0.0058\n","best_test_loss: 0.005803303529872843\n","2023-10-25 22:24:31  Epoch【120】 train_loss:0.00762  val_loss:0.00575\n","best_test_loss: 0.005753494136075935\n","2023-10-25 22:24:33  Epoch【121】 train_loss:0.00755  val_loss:0.00571\n","best_test_loss: 0.005707643060621036\n","2023-10-25 22:24:35  Epoch【122】 train_loss:0.00743  val_loss:0.00565\n","best_test_loss: 0.005652466034719154\n","2023-10-25 22:24:37  Epoch【123】 train_loss:0.0073  val_loss:0.0056\n","best_test_loss: 0.005596424240375991\n","2023-10-25 22:24:39  Epoch【124】 train_loss:0.00723  val_loss:0.00554\n","best_test_loss: 0.005542600497036525\n","2023-10-25 22:24:41  Epoch【125】 train_loss:0.0072  val_loss:0.00549\n","best_test_loss: 0.0054935135868499465\n","2023-10-25 22:24:43  Epoch【126】 train_loss:0.00718  val_loss:0.00546\n","best_test_loss: 0.005461540903248217\n","2023-10-25 22:24:45  Epoch【127】 train_loss:0.00705  val_loss:0.00541\n","best_test_loss: 0.00540573019331888\n","2023-10-25 22:24:46  Epoch【128】 train_loss:0.00695  val_loss:0.00536\n","best_test_loss: 0.0053638966207433005\n","2023-10-25 22:24:48  Epoch【129】 train_loss:0.00693  val_loss:0.00533\n","best_test_loss: 0.005330569763748866\n","2023-10-25 22:24:50  Epoch【130】 train_loss:0.00689  val_loss:0.00529\n","best_test_loss: 0.005288731458637378\n","2023-10-25 22:24:52  Epoch【131】 train_loss:0.00676  val_loss:0.00524\n","best_test_loss: 0.00523958772258914\n","2023-10-25 22:24:54  Epoch【132】 train_loss:0.00674  val_loss:0.00521\n","best_test_loss: 0.005206093789361741\n","2023-10-25 22:24:56  Epoch【133】 train_loss:0.00667  val_loss:0.00516\n","best_test_loss: 0.005163501801333674\n","2023-10-25 22:24:58  Epoch【134】 train_loss:0.00655  val_loss:0.00512\n","best_test_loss: 0.005121326468803961\n","2023-10-25 22:25:00  Epoch【135】 train_loss:0.00656  val_loss:0.0051\n","best_test_loss: 0.005096701267377838\n","2023-10-25 22:25:02  Epoch【136】 train_loss:0.00642  val_loss:0.00505\n","best_test_loss: 0.0050489157843201056\n","2023-10-25 22:25:04  Epoch【137】 train_loss:0.00641  val_loss:0.00501\n","best_test_loss: 0.005013879246847785\n","2023-10-25 22:25:06  Epoch【138】 train_loss:0.00629  val_loss:0.00497\n","best_test_loss: 0.004971644289427153\n","2023-10-25 22:25:08  Epoch【139】 train_loss:0.00626  val_loss:0.00493\n","best_test_loss: 0.004934767175101391\n","2023-10-25 22:25:10  Epoch【140】 train_loss:0.00617  val_loss:0.0049\n","best_test_loss: 0.004898154328617713\n","2023-10-25 22:25:12  Epoch【141】 train_loss:0.00612  val_loss:0.00486\n","best_test_loss: 0.004863393336089085\n","2023-10-25 22:25:14  Epoch【142】 train_loss:0.00605  val_loss:0.00483\n","best_test_loss: 0.004828115424100795\n","2023-10-25 22:25:16  Epoch【143】 train_loss:0.00599  val_loss:0.00479\n","best_test_loss: 0.004794349494066251\n","2023-10-25 22:25:18  Epoch【144】 train_loss:0.00594  val_loss:0.00476\n","best_test_loss: 0.004762381127954502\n","2023-10-25 22:25:20  Epoch【145】 train_loss:0.00587  val_loss:0.00473\n","best_test_loss: 0.00473079693240478\n","2023-10-25 22:25:22  Epoch【146】 train_loss:0.00582  val_loss:0.0047\n","best_test_loss: 0.004699489324475112\n","2023-10-25 22:25:24  Epoch【147】 train_loss:0.00575  val_loss:0.00467\n","best_test_loss: 0.004666485378275747\n","2023-10-25 22:25:26  Epoch【148】 train_loss:0.00572  val_loss:0.00464\n","best_test_loss: 0.004637590310860263\n","2023-10-25 22:25:28  Epoch【149】 train_loss:0.00561  val_loss:0.0046\n","best_test_loss: 0.004604162324381911\n","2023-10-25 22:25:30  Epoch【150】 train_loss:0.00564  val_loss:0.00457\n","best_test_loss: 0.004572451691669615\n","2023-10-25 22:25:32  Epoch【151】 train_loss:0.00549  val_loss:0.00455\n","best_test_loss: 0.004547289152548689\n","2023-10-25 22:25:33  Epoch【152】 train_loss:0.00565  val_loss:0.00454\n","best_test_loss: 0.004542954169902141\n","2023-10-25 22:25:35  Epoch【153】 train_loss:0.00547  val_loss:0.00457\n","2023-10-25 22:25:37  Epoch【154】 train_loss:0.0058  val_loss:0.00462\n","2023-10-25 22:25:39  Epoch【155】 train_loss:0.00538  val_loss:0.00452\n","best_test_loss: 0.004520077248225393\n","2023-10-25 22:25:41  Epoch【156】 train_loss:0.00536  val_loss:0.00443\n","best_test_loss: 0.00442756034190888\n","2023-10-25 22:25:43  Epoch【157】 train_loss:0.00531  val_loss:0.00444\n","2023-10-25 22:25:45  Epoch【158】 train_loss:0.00519  val_loss:0.00433\n","best_test_loss: 0.004329925190414424\n","2023-10-25 22:25:47  Epoch【159】 train_loss:0.00511  val_loss:0.0043\n","best_test_loss: 0.004303828883997124\n","2023-10-25 22:25:49  Epoch【160】 train_loss:0.00512  val_loss:0.00431\n","2023-10-25 22:25:51  Epoch【161】 train_loss:0.00508  val_loss:0.00425\n","best_test_loss: 0.0042546754601694966\n","2023-10-25 22:25:53  Epoch【162】 train_loss:0.00497  val_loss:0.00426\n","2023-10-25 22:25:55  Epoch【163】 train_loss:0.00498  val_loss:0.00421\n","best_test_loss: 0.00420977000106612\n","2023-10-25 22:25:56  Epoch【164】 train_loss:0.00488  val_loss:0.00419\n","best_test_loss: 0.00419049526301577\n","2023-10-25 22:25:58  Epoch【165】 train_loss:0.00483  val_loss:0.00415\n","best_test_loss: 0.004154460317374248\n","2023-10-25 22:26:00  Epoch【166】 train_loss:0.00483  val_loss:0.00414\n","best_test_loss: 0.004143822836973097\n","2023-10-25 22:26:02  Epoch【167】 train_loss:0.00475  val_loss:0.00411\n","best_test_loss: 0.004108748899813256\n","2023-10-25 22:26:04  Epoch【168】 train_loss:0.00472  val_loss:0.00409\n","best_test_loss: 0.004088470307381257\n","2023-10-25 22:26:06  Epoch【169】 train_loss:0.00469  val_loss:0.00407\n","best_test_loss: 0.004071850299268313\n","2023-10-25 22:26:08  Epoch【170】 train_loss:0.00462  val_loss:0.00404\n","best_test_loss: 0.0040372406314734535\n","2023-10-25 22:26:10  Epoch【171】 train_loss:0.0046  val_loss:0.00402\n","best_test_loss: 0.004018143259758211\n","2023-10-25 22:26:12  Epoch【172】 train_loss:0.00455  val_loss:0.00399\n","best_test_loss: 0.003990622328432358\n","2023-10-25 22:26:14  Epoch【173】 train_loss:0.0045  val_loss:0.00397\n","best_test_loss: 0.003965914155781755\n","2023-10-25 22:26:16  Epoch【174】 train_loss:0.00446  val_loss:0.00394\n","best_test_loss: 0.0039351593839693005\n","2023-10-25 22:26:18  Epoch【175】 train_loss:0.00441  val_loss:0.00392\n","best_test_loss: 0.003918240950240389\n","2023-10-25 22:26:20  Epoch【176】 train_loss:0.00436  val_loss:0.00389\n","best_test_loss: 0.00389164828432157\n","2023-10-25 22:26:22  Epoch【177】 train_loss:0.00443  val_loss:0.00391\n","2023-10-25 22:26:24  Epoch【178】 train_loss:0.00434  val_loss:0.00388\n","best_test_loss: 0.0038789963025761686\n","2023-10-25 22:26:26  Epoch【179】 train_loss:0.00443  val_loss:0.00392\n","2023-10-25 22:26:28  Epoch【180】 train_loss:0.00459  val_loss:0.004\n","2023-10-25 22:26:30  Epoch【181】 train_loss:0.00437  val_loss:0.00396\n","2023-10-25 22:26:32  Epoch【182】 train_loss:0.00408  val_loss:0.00377\n","best_test_loss: 0.0037676595687947197\n","2023-10-25 22:26:34  Epoch【183】 train_loss:0.004  val_loss:0.00373\n","best_test_loss: 0.0037280889025524907\n","2023-10-25 22:26:35  Epoch【184】 train_loss:0.00402  val_loss:0.00372\n","best_test_loss: 0.00371706447786773\n","2023-10-25 22:26:37  Epoch【185】 train_loss:0.0039  val_loss:0.00368\n","best_test_loss: 0.0036779075238408277\n","2023-10-25 22:26:39  Epoch【186】 train_loss:0.00382  val_loss:0.00365\n","best_test_loss: 0.0036514002662759435\n","2023-10-25 22:26:41  Epoch【187】 train_loss:0.00386  val_loss:0.00365\n","best_test_loss: 0.00365039133011242\n","2023-10-25 22:26:43  Epoch【188】 train_loss:0.00373  val_loss:0.00361\n","best_test_loss: 0.0036084720935753507\n","2023-10-25 22:26:45  Epoch【189】 train_loss:0.00369  val_loss:0.00359\n","best_test_loss: 0.00358707604818451\n","2023-10-25 22:26:47  Epoch【190】 train_loss:0.00375  val_loss:0.00359\n","2023-10-25 22:26:49  Epoch【191】 train_loss:0.0036  val_loss:0.00356\n","best_test_loss: 0.003562565758804102\n","2023-10-25 22:26:51  Epoch【192】 train_loss:0.00365  val_loss:0.00357\n","2023-10-25 22:26:53  Epoch【193】 train_loss:0.00355  val_loss:0.00351\n","best_test_loss: 0.0035113319577446773\n","2023-10-25 22:26:55  Epoch【194】 train_loss:0.00348  val_loss:0.00349\n","best_test_loss: 0.0034940819706484353\n","2023-10-25 22:26:57  Epoch【195】 train_loss:0.00357  val_loss:0.00351\n","2023-10-25 22:26:59  Epoch【196】 train_loss:0.00344  val_loss:0.00348\n","best_test_loss: 0.00348365960298511\n","2023-10-25 22:27:01  Epoch【197】 train_loss:0.00359  val_loss:0.00354\n","2023-10-25 22:27:03  Epoch【198】 train_loss:0.00338  val_loss:0.00343\n","best_test_loss: 0.003429732834616595\n","2023-10-25 22:27:05  Epoch【199】 train_loss:0.00333  val_loss:0.00342\n","best_test_loss: 0.0034173932533873162\n","2023-10-25 22:27:07  Epoch【200】 train_loss:0.00358  val_loss:0.00351\n","2023-10-25 22:27:09  Epoch【201】 train_loss:0.00327  val_loss:0.00337\n","best_test_loss: 0.0033668853145133217\n","2023-10-25 22:27:11  Epoch【202】 train_loss:0.00326  val_loss:0.00337\n","best_test_loss: 0.00336536515817937\n","2023-10-25 22:27:13  Epoch【203】 train_loss:0.00326  val_loss:0.00337\n","2023-10-25 22:27:14  Epoch【204】 train_loss:0.00325  val_loss:0.00334\n","best_test_loss: 0.0033412147920740686\n","2023-10-25 22:27:16  Epoch【205】 train_loss:0.00314  val_loss:0.00332\n","best_test_loss: 0.0033190876711159945\n","2023-10-25 22:27:18  Epoch【206】 train_loss:0.00314  val_loss:0.0033\n","best_test_loss: 0.003300860721577445\n","2023-10-25 22:27:20  Epoch【207】 train_loss:0.00315  val_loss:0.00331\n","2023-10-25 22:27:22  Epoch【208】 train_loss:0.00306  val_loss:0.00327\n","best_test_loss: 0.0032691658428951127\n","2023-10-25 22:27:24  Epoch【209】 train_loss:0.00297  val_loss:0.0032\n","best_test_loss: 0.0032046684241124794\n","2023-10-25 22:27:26  Epoch【210】 train_loss:0.00314  val_loss:0.0033\n","2023-10-25 22:27:28  Epoch【211】 train_loss:0.00295  val_loss:0.00322\n","2023-10-25 22:27:30  Epoch【212】 train_loss:0.00296  val_loss:0.00317\n","best_test_loss: 0.0031714570258096183\n","2023-10-25 22:27:32  Epoch【213】 train_loss:0.00294  val_loss:0.00319\n","2023-10-25 22:27:34  Epoch【214】 train_loss:0.00303  val_loss:0.0032\n","2023-10-25 22:27:36  Epoch【215】 train_loss:0.00286  val_loss:0.00315\n","best_test_loss: 0.003154725479164525\n","2023-10-25 22:27:38  Epoch【216】 train_loss:0.003  val_loss:0.00318\n","2023-10-25 22:27:40  Epoch【217】 train_loss:0.00287  val_loss:0.00314\n","best_test_loss: 0.0031392479836738303\n","2023-10-25 22:27:42  Epoch【218】 train_loss:0.00295  val_loss:0.00317\n","2023-10-25 22:27:43  Epoch【219】 train_loss:0.0028  val_loss:0.00315\n","2023-10-25 22:27:45  Epoch【220】 train_loss:0.00274  val_loss:0.00307\n","best_test_loss: 0.003066623892189692\n","2023-10-25 22:27:47  Epoch【221】 train_loss:0.00264  val_loss:0.003\n","best_test_loss: 0.0030036341834246464\n","2023-10-25 22:27:49  Epoch【222】 train_loss:0.00265  val_loss:0.00301\n","2023-10-25 22:27:51  Epoch【223】 train_loss:0.00263  val_loss:0.00298\n","best_test_loss: 0.002981849634797191\n","2023-10-25 22:27:53  Epoch【224】 train_loss:0.00256  val_loss:0.00297\n","best_test_loss: 0.002974645730674915\n","2023-10-25 22:27:55  Epoch【225】 train_loss:0.00263  val_loss:0.00297\n","best_test_loss: 0.002971350336852281\n","2023-10-25 22:27:57  Epoch【226】 train_loss:0.00254  val_loss:0.00292\n","best_test_loss: 0.0029243142090980773\n","2023-10-25 22:27:59  Epoch【227】 train_loss:0.00251  val_loss:0.00291\n","best_test_loss: 0.0029122038977220654\n","2023-10-25 22:28:01  Epoch【228】 train_loss:0.00267  val_loss:0.00301\n","2023-10-25 22:28:03  Epoch【229】 train_loss:0.00243  val_loss:0.00289\n","best_test_loss: 0.002890194566292769\n","2023-10-25 22:28:05  Epoch【230】 train_loss:0.00242  val_loss:0.00284\n","best_test_loss: 0.002840812555919199\n","2023-10-25 22:28:07  Epoch【231】 train_loss:0.00266  val_loss:0.00295\n","2023-10-25 22:28:08  Epoch【232】 train_loss:0.00243  val_loss:0.0029\n","2023-10-25 22:28:10  Epoch【233】 train_loss:0.00243  val_loss:0.00285\n","2023-10-25 22:28:12  Epoch【234】 train_loss:0.00249  val_loss:0.00286\n","2023-10-25 22:28:14  Epoch【235】 train_loss:0.0024  val_loss:0.00282\n","best_test_loss: 0.0028174083361037724\n","2023-10-25 22:28:16  Epoch【236】 train_loss:0.00232  val_loss:0.0028\n","best_test_loss: 0.002798057432063734\n","2023-10-25 22:28:18  Epoch【237】 train_loss:0.0026  val_loss:0.00285\n","2023-10-25 22:28:20  Epoch【238】 train_loss:0.00237  val_loss:0.00283\n","2023-10-25 22:28:22  Epoch【239】 train_loss:0.00248  val_loss:0.00282\n","2023-10-25 22:28:24  Epoch【240】 train_loss:0.0024  val_loss:0.0028\n","best_test_loss: 0.0027980474937383247\n","2023-10-25 22:28:26  Epoch【241】 train_loss:0.00235  val_loss:0.00273\n","best_test_loss: 0.002734201648981189\n","2023-10-25 22:28:28  Epoch【242】 train_loss:0.00219  val_loss:0.0027\n","best_test_loss: 0.002697376964573303\n","2023-10-25 22:28:30  Epoch【243】 train_loss:0.0023  val_loss:0.0027\n","2023-10-25 22:28:32  Epoch【244】 train_loss:0.00216  val_loss:0.00264\n","best_test_loss: 0.002643162397283089\n","2023-10-25 22:28:34  Epoch【245】 train_loss:0.00213  val_loss:0.00262\n","best_test_loss: 0.0026181350467200186\n","2023-10-25 22:28:36  Epoch【246】 train_loss:0.00215  val_loss:0.0026\n","best_test_loss: 0.002601155143170415\n","2023-10-25 22:28:38  Epoch【247】 train_loss:0.00212  val_loss:0.0026\n","best_test_loss: 0.0025997846691018863\n","2023-10-25 22:28:39  Epoch【248】 train_loss:0.00209  val_loss:0.00257\n","best_test_loss: 0.0025737553850099766\n","2023-10-25 22:28:41  Epoch【249】 train_loss:0.00205  val_loss:0.00255\n","best_test_loss: 0.002554070834657582\n","2023-10-25 22:28:43  Epoch【250】 train_loss:0.0021  val_loss:0.00255\n","best_test_loss: 0.0025493410370393617\n","2023-10-25 22:28:45  Epoch【251】 train_loss:0.00202  val_loss:0.00253\n","best_test_loss: 0.0025302698208100123\n","2023-10-25 22:28:47  Epoch【252】 train_loss:0.00212  val_loss:0.00257\n","2023-10-25 22:28:49  Epoch【253】 train_loss:0.00198  val_loss:0.00249\n","best_test_loss: 0.002487183146117984\n","2023-10-25 22:28:51  Epoch【254】 train_loss:0.00209  val_loss:0.0025\n","2023-10-25 22:28:53  Epoch【255】 train_loss:0.00209  val_loss:0.00256\n","2023-10-25 22:28:55  Epoch【256】 train_loss:0.00237  val_loss:0.00258\n","2023-10-25 22:28:57  Epoch【257】 train_loss:0.00236  val_loss:0.00271\n","2023-10-25 22:28:59  Epoch【258】 train_loss:0.00269  val_loss:0.0029\n","2023-10-25 22:29:01  Epoch【259】 train_loss:0.00206  val_loss:0.00249\n","2023-10-25 22:29:02  Epoch【260】 train_loss:0.00188  val_loss:0.0024\n","best_test_loss: 0.002402933323300322\n","2023-10-25 22:29:04  Epoch【261】 train_loss:0.00187  val_loss:0.00238\n","best_test_loss: 0.0023825163295006623\n","2023-10-25 22:29:06  Epoch【262】 train_loss:0.00189  val_loss:0.00239\n","2023-10-25 22:29:08  Epoch【263】 train_loss:0.00184  val_loss:0.00235\n","best_test_loss: 0.002351133394530853\n","2023-10-25 22:29:10  Epoch【264】 train_loss:0.00183  val_loss:0.00234\n","best_test_loss: 0.0023372650004761376\n","2023-10-25 22:29:12  Epoch【265】 train_loss:0.00182  val_loss:0.00235\n","2023-10-25 22:29:14  Epoch【266】 train_loss:0.00181  val_loss:0.00232\n","best_test_loss: 0.0023224031699456923\n","2023-10-25 22:29:16  Epoch【267】 train_loss:0.00179  val_loss:0.00231\n","best_test_loss: 0.0023058823400946417\n","2023-10-25 22:29:18  Epoch【268】 train_loss:0.00177  val_loss:0.00231\n","2023-10-25 22:29:20  Epoch【269】 train_loss:0.00175  val_loss:0.00228\n","best_test_loss: 0.002279444078854083\n","2023-10-25 22:29:22  Epoch【270】 train_loss:0.00172  val_loss:0.00226\n","best_test_loss: 0.00225537185400279\n","2023-10-25 22:29:24  Epoch【271】 train_loss:0.00174  val_loss:0.00228\n","2023-10-25 22:29:25  Epoch【272】 train_loss:0.00171  val_loss:0.00224\n","best_test_loss: 0.0022439567850518\n","2023-10-25 22:29:27  Epoch【273】 train_loss:0.00169  val_loss:0.00224\n","best_test_loss: 0.0022367989586200565\n","2023-10-25 22:29:29  Epoch【274】 train_loss:0.0017  val_loss:0.00224\n","2023-10-25 22:29:31  Epoch【275】 train_loss:0.00166  val_loss:0.00221\n","best_test_loss: 0.002209274871426675\n","2023-10-25 22:29:33  Epoch【276】 train_loss:0.00163  val_loss:0.00218\n","best_test_loss: 0.0021804059936624508\n","2023-10-25 22:29:35  Epoch【277】 train_loss:0.0017  val_loss:0.00223\n","2023-10-25 22:29:37  Epoch【278】 train_loss:0.00167  val_loss:0.0022\n","2023-10-25 22:29:39  Epoch【279】 train_loss:0.00168  val_loss:0.00222\n","2023-10-25 22:29:41  Epoch【280】 train_loss:0.00167  val_loss:0.0022\n","2023-10-25 22:29:42  Epoch【281】 train_loss:0.00166  val_loss:0.00219\n","2023-10-25 22:29:44  Epoch【282】 train_loss:0.00157  val_loss:0.00212\n","best_test_loss: 0.002118156730632662\n","2023-10-25 22:29:46  Epoch【283】 train_loss:0.00176  val_loss:0.00222\n","2023-10-25 22:29:48  Epoch【284】 train_loss:0.00168  val_loss:0.00216\n","2023-10-25 22:29:50  Epoch【285】 train_loss:0.00202  val_loss:0.00239\n","2023-10-25 22:29:52  Epoch【286】 train_loss:0.00193  val_loss:0.00236\n","2023-10-25 22:29:54  Epoch【287】 train_loss:0.00174  val_loss:0.00225\n","2023-10-25 22:29:56  Epoch【288】 train_loss:0.00172  val_loss:0.00219\n","2023-10-25 22:29:57  Epoch【289】 train_loss:0.00166  val_loss:0.00218\n","2023-10-25 22:29:59  Epoch【290】 train_loss:0.00153  val_loss:0.00206\n","best_test_loss: 0.002064611977877338\n","2023-10-25 22:30:01  Epoch【291】 train_loss:0.00166  val_loss:0.00218\n","2023-10-25 22:30:03  Epoch【292】 train_loss:0.00147  val_loss:0.00204\n","best_test_loss: 0.0020356388033732123\n","2023-10-25 22:30:05  Epoch【293】 train_loss:0.00148  val_loss:0.00202\n","best_test_loss: 0.0020208735026560885\n","2023-10-25 22:30:07  Epoch【294】 train_loss:0.00151  val_loss:0.00206\n","2023-10-25 22:30:09  Epoch【295】 train_loss:0.00148  val_loss:0.00203\n","2023-10-25 22:30:11  Epoch【296】 train_loss:0.00145  val_loss:0.002\n","best_test_loss: 0.0019986459595637152\n","2023-10-25 22:30:13  Epoch【297】 train_loss:0.00142  val_loss:0.00198\n","best_test_loss: 0.001983971995524252\n","2023-10-25 22:30:15  Epoch【298】 train_loss:0.00151  val_loss:0.00202\n","2023-10-25 22:30:17  Epoch【299】 train_loss:0.00137  val_loss:0.00195\n","best_test_loss: 0.0019514467478628553\n","2023-10-25 22:30:18  Epoch【300】 train_loss:0.00136  val_loss:0.00192\n","best_test_loss: 0.001923374512799732\n","2023-10-25 22:30:20  Epoch【301】 train_loss:0.0015  val_loss:0.00202\n","2023-10-25 22:30:22  Epoch【302】 train_loss:0.00134  val_loss:0.00192\n","best_test_loss: 0.0019193954054650892\n","2023-10-25 22:30:24  Epoch【303】 train_loss:0.00137  val_loss:0.0019\n","best_test_loss: 0.0019027252590445721\n","2023-10-25 22:30:26  Epoch【304】 train_loss:0.00154  val_loss:0.00206\n","2023-10-25 22:30:28  Epoch【305】 train_loss:0.00135  val_loss:0.00192\n","2023-10-25 22:30:30  Epoch【306】 train_loss:0.00137  val_loss:0.00191\n","2023-10-25 22:30:32  Epoch【307】 train_loss:0.00148  val_loss:0.00201\n","2023-10-25 22:30:34  Epoch【308】 train_loss:0.00139  val_loss:0.00191\n","2023-10-25 22:30:35  Epoch【309】 train_loss:0.00145  val_loss:0.00198\n","2023-10-25 22:30:37  Epoch【310】 train_loss:0.0013  val_loss:0.00187\n","best_test_loss: 0.0018728169126679068\n","2023-10-25 22:30:39  Epoch【311】 train_loss:0.00156  val_loss:0.00201\n","2023-10-25 22:30:41  Epoch【312】 train_loss:0.00127  val_loss:0.00184\n","best_test_loss: 0.001839961750311372\n","2023-10-25 22:30:43  Epoch【313】 train_loss:0.0014  val_loss:0.0019\n","2023-10-25 22:30:45  Epoch【314】 train_loss:0.00132  val_loss:0.00188\n","2023-10-25 22:30:47  Epoch【315】 train_loss:0.00141  val_loss:0.00192\n","2023-10-25 22:30:49  Epoch【316】 train_loss:0.00125  val_loss:0.0018\n","best_test_loss: 0.0018047361771333153\n","2023-10-25 22:30:51  Epoch【317】 train_loss:0.00125  val_loss:0.00182\n","2023-10-25 22:30:52  Epoch【318】 train_loss:0.00129  val_loss:0.00182\n","2023-10-25 22:30:54  Epoch【319】 train_loss:0.00129  val_loss:0.00185\n","2023-10-25 22:30:56  Epoch【320】 train_loss:0.00127  val_loss:0.00179\n","best_test_loss: 0.0017939156462924311\n","2023-10-25 22:30:58  Epoch【321】 train_loss:0.00122  val_loss:0.0018\n","2023-10-25 22:31:00  Epoch【322】 train_loss:0.00128  val_loss:0.00178\n","best_test_loss: 0.0017827865401910537\n","2023-10-25 22:31:02  Epoch【323】 train_loss:0.00121  val_loss:0.00178\n","best_test_loss: 0.0017826486310577425\n","2023-10-25 22:31:04  Epoch【324】 train_loss:0.00127  val_loss:0.00182\n","2023-10-25 22:31:06  Epoch【325】 train_loss:0.00124  val_loss:0.00179\n","2023-10-25 22:31:08  Epoch【326】 train_loss:0.00127  val_loss:0.00178\n","2023-10-25 22:31:10  Epoch【327】 train_loss:0.00134  val_loss:0.00178\n","best_test_loss: 0.0017805903107332795\n","2023-10-25 22:31:12  Epoch【328】 train_loss:0.0015  val_loss:0.00197\n","2023-10-25 22:31:14  Epoch【329】 train_loss:0.0015  val_loss:0.00194\n","2023-10-25 22:31:15  Epoch【330】 train_loss:0.00123  val_loss:0.00183\n","2023-10-25 22:31:17  Epoch【331】 train_loss:0.00114  val_loss:0.00169\n","best_test_loss: 0.0016944435420041175\n","2023-10-25 22:31:19  Epoch【332】 train_loss:0.00129  val_loss:0.00176\n","2023-10-25 22:31:21  Epoch【333】 train_loss:0.00124  val_loss:0.00183\n","2023-10-25 22:31:23  Epoch【334】 train_loss:0.00114  val_loss:0.00168\n","best_test_loss: 0.001680729885423637\n","2023-10-25 22:31:25  Epoch【335】 train_loss:0.00137  val_loss:0.00181\n","2023-10-25 22:31:27  Epoch【336】 train_loss:0.00108  val_loss:0.00171\n","2023-10-25 22:31:29  Epoch【337】 train_loss:0.00138  val_loss:0.0018\n","2023-10-25 22:31:31  Epoch【338】 train_loss:0.00121  val_loss:0.00173\n","2023-10-25 22:31:33  Epoch【339】 train_loss:0.0011  val_loss:0.00164\n","best_test_loss: 0.0016385650176190488\n","2023-10-25 22:31:34  Epoch【340】 train_loss:0.00136  val_loss:0.00189\n","2023-10-25 22:31:36  Epoch【341】 train_loss:0.00112  val_loss:0.00165\n","2023-10-25 22:31:38  Epoch【342】 train_loss:0.00122  val_loss:0.00169\n","2023-10-25 22:31:40  Epoch【343】 train_loss:0.001  val_loss:0.00159\n","best_test_loss: 0.0015939498768167814\n","2023-10-25 22:31:42  Epoch【344】 train_loss:0.00134  val_loss:0.00183\n","2023-10-25 22:31:44  Epoch【345】 train_loss:0.001  val_loss:0.00156\n","best_test_loss: 0.001564209090049743\n","2023-10-25 22:31:46  Epoch【346】 train_loss:0.00107  val_loss:0.00159\n","2023-10-25 22:31:48  Epoch【347】 train_loss:0.00099  val_loss:0.00156\n","best_test_loss: 0.001555013753797697\n","2023-10-25 22:31:50  Epoch【348】 train_loss:0.00115  val_loss:0.00168\n","2023-10-25 22:31:51  Epoch【349】 train_loss:0.00099  val_loss:0.00155\n","best_test_loss: 0.001547705241651072\n","2023-10-25 22:31:53  Epoch【350】 train_loss:0.00099  val_loss:0.00152\n","best_test_loss: 0.0015240187906032509\n","2023-10-25 22:31:55  Epoch【351】 train_loss:0.001  val_loss:0.00155\n","2023-10-25 22:31:57  Epoch【352】 train_loss:0.00108  val_loss:0.0016\n","2023-10-25 22:31:59  Epoch【353】 train_loss:0.00098  val_loss:0.00153\n","2023-10-25 22:32:01  Epoch【354】 train_loss:0.00095  val_loss:0.00148\n","best_test_loss: 0.0014831084396143485\n","2023-10-25 22:32:03  Epoch【355】 train_loss:0.00106  val_loss:0.00161\n","2023-10-25 22:32:05  Epoch【356】 train_loss:0.00107  val_loss:0.00158\n","2023-10-25 22:32:07  Epoch【357】 train_loss:0.00099  val_loss:0.00153\n","2023-10-25 22:32:08  Epoch【358】 train_loss:0.00095  val_loss:0.00146\n","best_test_loss: 0.001464821137589119\n","2023-10-25 22:32:10  Epoch【359】 train_loss:0.00107  val_loss:0.0016\n","2023-10-25 22:32:12  Epoch【360】 train_loss:0.00108  val_loss:0.00157\n","2023-10-25 22:32:14  Epoch【361】 train_loss:0.00095  val_loss:0.0015\n","2023-10-25 22:32:16  Epoch【362】 train_loss:0.00095  val_loss:0.00148\n","2023-10-25 22:32:18  Epoch【363】 train_loss:0.00097  val_loss:0.00153\n","2023-10-25 22:32:20  Epoch【364】 train_loss:0.00106  val_loss:0.00155\n","2023-10-25 22:32:21  Epoch【365】 train_loss:0.00089  val_loss:0.00143\n","best_test_loss: 0.0014270335719313311\n","2023-10-25 22:32:23  Epoch【366】 train_loss:0.00095  val_loss:0.00145\n","2023-10-25 22:32:25  Epoch【367】 train_loss:0.00093  val_loss:0.00149\n","2023-10-25 22:32:27  Epoch【368】 train_loss:0.00109  val_loss:0.00157\n","2023-10-25 22:32:29  Epoch【369】 train_loss:0.00086  val_loss:0.00141\n","best_test_loss: 0.0014057010861427482\n","2023-10-25 22:32:31  Epoch【370】 train_loss:0.00095  val_loss:0.00144\n","2023-10-25 22:32:33  Epoch【371】 train_loss:0.00085  val_loss:0.0014\n","best_test_loss: 0.001398290511291555\n","2023-10-25 22:32:35  Epoch【372】 train_loss:0.001  val_loss:0.00149\n","2023-10-25 22:32:37  Epoch【373】 train_loss:0.00081  val_loss:0.00136\n","best_test_loss: 0.0013584633466645914\n","2023-10-25 22:32:39  Epoch【374】 train_loss:0.00091  val_loss:0.00142\n","2023-10-25 22:32:40  Epoch【375】 train_loss:0.00085  val_loss:0.00138\n","2023-10-25 22:32:42  Epoch【376】 train_loss:0.00096  val_loss:0.00148\n","2023-10-25 22:32:44  Epoch【377】 train_loss:0.00091  val_loss:0.0014\n","2023-10-25 22:32:46  Epoch【378】 train_loss:0.00097  val_loss:0.00143\n","2023-10-25 22:32:48  Epoch【379】 train_loss:0.00107  val_loss:0.00144\n","2023-10-25 22:32:50  Epoch【380】 train_loss:0.00122  val_loss:0.00172\n","2023-10-25 22:32:52  Epoch【381】 train_loss:0.00116  val_loss:0.00156\n","2023-10-25 22:32:53  Epoch【382】 train_loss:0.00088  val_loss:0.0014\n","2023-10-25 22:32:55  Epoch【383】 train_loss:0.00079  val_loss:0.00133\n","best_test_loss: 0.001328414015638966\n","2023-10-25 22:32:57  Epoch【384】 train_loss:0.00093  val_loss:0.00143\n","2023-10-25 22:32:59  Epoch【385】 train_loss:0.0008  val_loss:0.00133\n","2023-10-25 22:33:01  Epoch【386】 train_loss:0.00091  val_loss:0.00135\n","2023-10-25 22:33:03  Epoch【387】 train_loss:0.00078  val_loss:0.00136\n","2023-10-25 22:33:05  Epoch【388】 train_loss:0.00101  val_loss:0.00145\n","2023-10-25 22:33:07  Epoch【389】 train_loss:0.0008  val_loss:0.00132\n","best_test_loss: 0.0013179260966590728\n","2023-10-25 22:33:09  Epoch【390】 train_loss:0.00091  val_loss:0.00134\n","2023-10-25 22:33:11  Epoch【391】 train_loss:0.00087  val_loss:0.00144\n","2023-10-25 22:33:12  Epoch【392】 train_loss:0.00113  val_loss:0.00152\n","2023-10-25 22:33:14  Epoch【393】 train_loss:0.00074  val_loss:0.00126\n","best_test_loss: 0.00126313092187047\n","2023-10-25 22:33:16  Epoch【394】 train_loss:0.00091  val_loss:0.00134\n","2023-10-25 22:33:18  Epoch【395】 train_loss:0.00073  val_loss:0.00126\n","best_test_loss: 0.001260280464877091\n","2023-10-25 22:33:20  Epoch【396】 train_loss:0.00104  val_loss:0.00154\n","2023-10-25 22:33:22  Epoch【397】 train_loss:0.00095  val_loss:0.00141\n","2023-10-25 22:33:24  Epoch【398】 train_loss:0.00073  val_loss:0.00124\n","best_test_loss: 0.0012448233116985016\n","2023-10-25 22:33:26  Epoch【399】 train_loss:0.00072  val_loss:0.00123\n","best_test_loss: 0.001225133363218249\n","2023-10-25 22:33:28  Epoch【400】 train_loss:0.00073  val_loss:0.00123\n","2023-10-25 22:33:29  Epoch【401】 train_loss:0.00082  val_loss:0.00133\n","2023-10-25 22:33:31  Epoch【402】 train_loss:0.00079  val_loss:0.00128\n","2023-10-25 22:33:33  Epoch【403】 train_loss:0.0007  val_loss:0.00122\n","best_test_loss: 0.0012154181503042903\n","2023-10-25 22:33:35  Epoch【404】 train_loss:0.00072  val_loss:0.00121\n","best_test_loss: 0.0012080561048249997\n","2023-10-25 22:33:37  Epoch【405】 train_loss:0.00068  val_loss:0.0012\n","best_test_loss: 0.0011969291245686295\n","2023-10-25 22:33:39  Epoch【406】 train_loss:0.00076  val_loss:0.00125\n","2023-10-25 22:33:41  Epoch【407】 train_loss:0.00069  val_loss:0.00119\n","best_test_loss: 0.0011900619618878093\n","2023-10-25 22:33:43  Epoch【408】 train_loss:0.00067  val_loss:0.00118\n","best_test_loss: 0.0011770833826512503\n","2023-10-25 22:33:45  Epoch【409】 train_loss:0.00068  val_loss:0.00119\n","2023-10-25 22:33:47  Epoch【410】 train_loss:0.00072  val_loss:0.00123\n","2023-10-25 22:33:49  Epoch【411】 train_loss:0.00073  val_loss:0.00121\n","2023-10-25 22:33:50  Epoch【412】 train_loss:0.00067  val_loss:0.00117\n","best_test_loss: 0.0011681389998715451\n","2023-10-25 22:33:52  Epoch【413】 train_loss:0.00071  val_loss:0.00117\n","2023-10-25 22:33:54  Epoch【414】 train_loss:0.00069  val_loss:0.0012\n","2023-10-25 22:33:56  Epoch【415】 train_loss:0.00082  val_loss:0.0013\n","2023-10-25 22:33:58  Epoch【416】 train_loss:0.00077  val_loss:0.00125\n","2023-10-25 22:34:00  Epoch【417】 train_loss:0.00068  val_loss:0.00115\n","best_test_loss: 0.001154820766279717\n","2023-10-25 22:34:02  Epoch【418】 train_loss:0.0007  val_loss:0.00118\n","2023-10-25 22:34:04  Epoch【419】 train_loss:0.00075  val_loss:0.00122\n","2023-10-25 22:34:05  Epoch【420】 train_loss:0.00076  val_loss:0.00124\n","2023-10-25 22:34:07  Epoch【421】 train_loss:0.00069  val_loss:0.00117\n","2023-10-25 22:34:09  Epoch【422】 train_loss:0.00067  val_loss:0.00115\n","best_test_loss: 0.0011457095945091999\n","2023-10-25 22:34:11  Epoch【423】 train_loss:0.0007  val_loss:0.00118\n","2023-10-25 22:34:13  Epoch【424】 train_loss:0.00069  val_loss:0.00116\n","2023-10-25 22:34:15  Epoch【425】 train_loss:0.00069  val_loss:0.00116\n","2023-10-25 22:34:17  Epoch【426】 train_loss:0.00067  val_loss:0.00112\n","best_test_loss: 0.001121288016134792\n","2023-10-25 22:34:19  Epoch【427】 train_loss:0.00065  val_loss:0.0012\n","2023-10-25 22:34:21  Epoch【428】 train_loss:0.00088  val_loss:0.00131\n","2023-10-25 22:34:22  Epoch【429】 train_loss:0.00068  val_loss:0.00115\n","2023-10-25 22:34:24  Epoch【430】 train_loss:0.00073  val_loss:0.00116\n","2023-10-25 22:34:26  Epoch【431】 train_loss:0.0006  val_loss:0.00109\n","best_test_loss: 0.0010938863586330706\n","2023-10-25 22:34:28  Epoch【432】 train_loss:0.00084  val_loss:0.0013\n","2023-10-25 22:34:30  Epoch【433】 train_loss:0.00077  val_loss:0.0012\n","2023-10-25 22:34:32  Epoch【434】 train_loss:0.00071  val_loss:0.00114\n","2023-10-25 22:34:34  Epoch【435】 train_loss:0.00062  val_loss:0.00107\n","best_test_loss: 0.0010680171658548163\n","2023-10-25 22:34:36  Epoch【436】 train_loss:0.0006  val_loss:0.00111\n","2023-10-25 22:34:37  Epoch【437】 train_loss:0.00077  val_loss:0.00122\n","2023-10-25 22:34:39  Epoch【438】 train_loss:0.00067  val_loss:0.00112\n","2023-10-25 22:34:41  Epoch【439】 train_loss:0.00065  val_loss:0.00109\n","2023-10-25 22:34:43  Epoch【440】 train_loss:0.00055  val_loss:0.00103\n","best_test_loss: 0.0010302517342927824\n","2023-10-25 22:34:45  Epoch【441】 train_loss:0.00063  val_loss:0.00112\n","2023-10-25 22:34:47  Epoch【442】 train_loss:0.00071  val_loss:0.00117\n","2023-10-25 22:34:49  Epoch【443】 train_loss:0.00066  val_loss:0.00111\n","2023-10-25 22:34:51  Epoch【444】 train_loss:0.00063  val_loss:0.00108\n","2023-10-25 22:34:52  Epoch【445】 train_loss:0.00056  val_loss:0.00103\n","2023-10-25 22:34:54  Epoch【446】 train_loss:0.00063  val_loss:0.00111\n","2023-10-25 22:34:56  Epoch【447】 train_loss:0.00072  val_loss:0.00113\n","2023-10-25 22:34:58  Epoch【448】 train_loss:0.0006  val_loss:0.00109\n","2023-10-25 22:35:00  Epoch【449】 train_loss:0.00067  val_loss:0.0011\n","2023-10-25 22:35:02  Epoch【450】 train_loss:0.00054  val_loss:0.00101\n","best_test_loss: 0.001013001714296558\n","2023-10-25 22:35:04  Epoch【451】 train_loss:0.00063  val_loss:0.00109\n","2023-10-25 22:35:06  Epoch【452】 train_loss:0.00071  val_loss:0.00113\n","2023-10-25 22:35:07  Epoch【453】 train_loss:0.00059  val_loss:0.00104\n","2023-10-25 22:35:09  Epoch【454】 train_loss:0.00056  val_loss:0.00103\n","2023-10-25 22:35:11  Epoch【455】 train_loss:0.00056  val_loss:0.00101\n","best_test_loss: 0.0010056561200737792\n","2023-10-25 22:35:13  Epoch【456】 train_loss:0.00052  val_loss:0.00101\n","2023-10-25 22:35:15  Epoch【457】 train_loss:0.00065  val_loss:0.0011\n","2023-10-25 22:35:17  Epoch【458】 train_loss:0.00061  val_loss:0.00105\n","2023-10-25 22:35:19  Epoch【459】 train_loss:0.00055  val_loss:0.00099\n","best_test_loss: 0.000990784994032963\n","2023-10-25 22:35:21  Epoch【460】 train_loss:0.00052  val_loss:0.00098\n","best_test_loss: 0.00098209972455389\n","2023-10-25 22:35:23  Epoch【461】 train_loss:0.00055  val_loss:0.001\n","2023-10-25 22:35:24  Epoch【462】 train_loss:0.00055  val_loss:0.00104\n","2023-10-25 22:35:26  Epoch【463】 train_loss:0.00067  val_loss:0.00108\n","2023-10-25 22:35:28  Epoch【464】 train_loss:0.00054  val_loss:0.00099\n","2023-10-25 22:35:30  Epoch【465】 train_loss:0.00057  val_loss:0.00103\n","2023-10-25 22:35:32  Epoch【466】 train_loss:0.00054  val_loss:0.00102\n","2023-10-25 22:35:34  Epoch【467】 train_loss:0.0006  val_loss:0.00104\n","2023-10-25 22:35:36  Epoch【468】 train_loss:0.00057  val_loss:0.001\n","2023-10-25 22:35:38  Epoch【469】 train_loss:0.00057  val_loss:0.00101\n","2023-10-25 22:35:40  Epoch【470】 train_loss:0.00056  val_loss:0.00099\n","2023-10-25 22:35:42  Epoch【471】 train_loss:0.00049  val_loss:0.00095\n","best_test_loss: 0.0009492390830849257\n","2023-10-25 22:35:44  Epoch【472】 train_loss:0.00053  val_loss:0.00096\n","2023-10-25 22:35:46  Epoch【473】 train_loss:0.00049  val_loss:0.00096\n","2023-10-25 22:35:47  Epoch【474】 train_loss:0.00058  val_loss:0.00108\n","2023-10-25 22:35:49  Epoch【475】 train_loss:0.00063  val_loss:0.00107\n","2023-10-25 22:35:51  Epoch【476】 train_loss:0.00057  val_loss:0.001\n","2023-10-25 22:35:53  Epoch【477】 train_loss:0.00057  val_loss:0.00098\n","2023-10-25 22:35:55  Epoch【478】 train_loss:0.00046  val_loss:0.00092\n","best_test_loss: 0.000916841390552809\n","2023-10-25 22:35:57  Epoch【479】 train_loss:0.00055  val_loss:0.001\n","2023-10-25 22:35:59  Epoch【480】 train_loss:0.00057  val_loss:0.00098\n","2023-10-25 22:36:01  Epoch【481】 train_loss:0.00051  val_loss:0.00095\n","2023-10-25 22:36:02  Epoch【482】 train_loss:0.00048  val_loss:0.00095\n","2023-10-25 22:36:04  Epoch【483】 train_loss:0.00053  val_loss:0.00104\n","2023-10-25 22:36:06  Epoch【484】 train_loss:0.00063  val_loss:0.00107\n","2023-10-25 22:36:08  Epoch【485】 train_loss:0.00058  val_loss:0.00099\n","2023-10-25 22:36:10  Epoch【486】 train_loss:0.00055  val_loss:0.00097\n","2023-10-25 22:36:12  Epoch【487】 train_loss:0.00047  val_loss:0.0009\n","best_test_loss: 0.0009038526515486528\n","2023-10-25 22:36:14  Epoch【488】 train_loss:0.00048  val_loss:0.00093\n","2023-10-25 22:36:16  Epoch【489】 train_loss:0.00057  val_loss:0.00102\n","2023-10-25 22:36:18  Epoch【490】 train_loss:0.00059  val_loss:0.00102\n","2023-10-25 22:36:20  Epoch【491】 train_loss:0.00051  val_loss:0.00094\n","2023-10-25 22:36:22  Epoch【492】 train_loss:0.00048  val_loss:0.00092\n","2023-10-25 22:36:23  Epoch【493】 train_loss:0.00045  val_loss:0.00089\n","best_test_loss: 0.0008895661379463728\n","2023-10-25 22:36:25  Epoch【494】 train_loss:0.00043  val_loss:0.0009\n","2023-10-25 22:36:27  Epoch【495】 train_loss:0.0005  val_loss:0.00095\n","2023-10-25 22:36:29  Epoch【496】 train_loss:0.00047  val_loss:0.00091\n","2023-10-25 22:36:31  Epoch【497】 train_loss:0.00042  val_loss:0.00087\n","best_test_loss: 0.0008730523208530782\n","2023-10-25 22:36:33  Epoch【498】 train_loss:0.00043  val_loss:0.00088\n","2023-10-25 22:36:35  Epoch【499】 train_loss:0.00044  val_loss:0.00091\n"]}],"source":["train_val()\n"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1698056020384,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"3pxGaXdMw6Tg"},"outputs":[],"source":["####test部分\n","def test():\n","    model.load_state_dict(torch.load('./logs/best_Transformer_trainModel.pth'))\n","    model.to(device)\n","    model.eval()\n","    y_pred = []\n","    for X_test,y_test in test_loader:\n","        X_tt = X_test.clone().detach().requires_grad_(True)\n","        # Y_tt = y_test.clone().detach().requires_grad_(True)\n","        output = model(X_tt.to(device)).to(device)\n","        # output = output.cpu().detach().numpy()[0]\n","        y_pred.append(output)\n","        # Y_tt = Y_tt.cpu().detach().numpy()[0] #将Y_tt从GPU移动到CPU，并释放GPU上的内存,并保存第一个值（这里本来是为了做对比留下来的，但是后面直接取值label_test，最后没用到）\n","    # print('test预测')\n","    pred = [tensor.detach().cpu().numpy() for tensor in y_pred]\n","    pred = np.array(pred)\n","    y_preds = pred\n","    return y_preds"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["lag: 1\n"]}],"source":["print('lag:',lag)"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1698056020384,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"UpjdZ3KRZKwy"},"outputs":[],"source":["preds = test()\n","preds = preds.reshape(-1, K)"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"SCUwY4ih305P"},"outputs":[{"name":"stdout","output_type":"stream","text":["preds: [[0.9883175  0.9987621  0.99559057 ... 0.99884176 1.0164876  0.9999051 ]\n"," [0.9888657  0.99628717 0.99794203 ... 0.97975    1.0172814  1.018437  ]\n"," [0.992556   0.995126   0.99916077 ... 0.99685514 1.0174336  1.0408407 ]\n"," ...\n"," [0.9917474  0.9931122  0.9995583  ... 0.99658847 1.0210694  1.0235434 ]\n"," [0.99097246 0.9951572  0.99740463 ... 1.0019301  1.0238851  0.95349777]\n"," [0.99037534 1.0007701  0.99617624 ... 0.48673558 0.5322946  0.5217639 ]]\n","preds.shape: (1458, 24)\n","y_cs: (1458, 24)\n"]}],"source":["print('preds:',preds)\n","print('preds.shape:',preds.shape)\n","print('y_cs:',y_cs.shape)"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"Ep8TOYD-3-C9"},"outputs":[{"name":"stdout","output_type":"stream","text":["    Time[min]  MAD[%]       R2  RMSD[%]\n","K                                      \n","1          15    1.66  0.99944     3.99\n","2          30    1.78  0.99933     4.32\n","3          45    1.15  0.99968     2.87\n","4          60    1.29  0.99967     2.87\n","5          75    2.20  0.99847     5.93\n","6          90    4.10  0.99302    12.27\n","7         105    4.34  0.99232    12.61\n","8         120    3.84  0.99393    11.08\n","9         135    3.82  0.99471    10.25\n","10        150    3.34  0.99586     9.07\n","11        165    2.88  0.99692     7.86\n","12        180    3.63  0.99544     9.76\n","13        195    2.86  0.99707     8.05\n","14        210    3.31  0.99706     8.25\n","15        225    2.34  0.99888     5.24\n","16        240    1.00  0.99979     2.35\n","17        255    1.38  0.99956     3.45\n","18        270    1.33  0.99957     3.51\n","19        285    1.45  0.99946     3.98\n","20        300    1.25  0.99959     3.50\n","21        315    1.54  0.99947     3.97\n","22        330    1.37  0.99957     3.59\n","23        345    1.68  0.99957     3.58\n","24        360   10.25  0.96896    30.19\n"]}],"source":["# transform to GHI\n","\n","y_pred2 = np.multiply(preds, y_cs)\n","\n","y_test2 = np.multiply(label_test,y_cs)\n","y_test2 =y_test2.numpy()\n","# save results\n","results = {'K':[],'Time[min]':[],'MAD[%]':[],'R2':[],'RMSD[%]':[]}\n","for i in range(K):\n","    results['K'].append(i+1)\n","    results['Time[min]'].append((i+1)*15)\n","    results['MAD[%]'].append(np.round(mad(y_pred2[:,i],y_test2[:,i]),2))\n","    results['R2'].append(np.round(r2(y_pred2[:,i],y_test2[:,i]),5))\n","    results['RMSD[%]'].append(np.round(rmsd(y_pred2[:,i],y_test2[:,i]),2))\n","\n","# create results dataframe\n","results = pd.DataFrame(results)\n","results = results.set_index('K')\n","print(results.head(K))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNdc/O2AIS1FgyQbqDk26vg","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
