{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"elapsed":249,"status":"error","timestamp":1698056061152,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"WA2gyXIZi4U5","outputId":"ca672d70-c9b4-4ec2-90f4-7e2e58a0e2ab"},"outputs":[],"source":["########################## dataload part###############\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import time\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","import torch\n","\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","import os\n","# disable CPU warning\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","# lagged GHI values\n","LAG = 1\n","\n","# prediction horizon\n","K = 24\n","\n","seq_len =24\n","\n","EXOGENOUS = True\n","\n","# features\n","if(EXOGENOUS):\n","    features = ['K','uvIndex','cloudCover','sunshineDuration','windBearing','humidity','temperature','hour','dewPoint']\n","else:\n","    features = ['K']\n","\n","df = pd.read_csv(\"../clean_dataset.csv\",header=0, index_col=0, parse_dates=True).sort_index()\n","df_GHI = df[['K']].copy()\n","# create exogenous regressors\n","for feature in features:\n","    df_GHI[feature] = df[feature]\n","    for i in range(LAG-1):\n","        df_GHI[feature+'-'+str(i+1)] = df[feature].shift(i+1)\n","# create target values\n","for i in range(1,K+1):\n","    #  df_GHI['K+'+str(i)] = df['K'].shift(-i)\n","    df_k= df['K'].shift(-i).rename('K+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_k],axis=1)\n","\n","# create clear sky target values\n","for i in range(1,K+1):\n","    # df_GHI['GHI_cs+'+str(i)] = df['GHI_cs'].shift(-i)\n","    df_cs = df['GHI_cs'].shift(-i).rename('GHI_cs+'+str(i))\n","    df_GHI = pd.concat([df_GHI,df_cs],axis=1)\n","\n","# drop nan due to shifting\n","df_GHI = df_GHI.dropna()\n","    # create training set\n","X_train = df_GHI['2010-1-1':'2014-6-30'].values[:,:-K*2]\n","y_train = df_GHI['2010-1-1':'2014-6-30'].values[:,-K*2:-K]\n","\n","# create validation set\n","X_val = df_GHI['2014-7-1':'2014-12-31'].values[:,:-K*2]\n","y_val = df_GHI['2014-7-1':'2014-12-31'].values[:,-K*2:-K]\n","# create test set\n","X_test = df_GHI['2015-1-1':'2015-12-31'].values[:,:-K*2]\n","y_test = df_GHI['2015-1-1':'2015-12-31'].values[:,-K*2:-K]\n","# get clear sky target values\n","y_cs = df_GHI['2015-1-1':'2015-12-31'].values[:,-K:]\n","# scale features\n","scaler = MinMaxScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n","X=[]\n","Y=[]\n","for i in range(0, X_train.shape[0]-seq_len, seq_len):\n","  # print('i:',i)\n","  data_X = np.expand_dims(X_train[i:i + seq_len], 0)\n","  data_Y = np.expand_dims(y_train[i:i + seq_len,0], 0)\n","  # print(data_X.shape)  # (1, 24, 9)\n","  # print(data_Y.shape)  # (1, 24, 24)\n","  X.append(data_X)\n","  Y.append(data_Y)\n","data_X= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","data_Y=np.concatenate(Y, axis=0)\n","X=[]\n","Y=[]\n","for i in range(0, X_val.shape[0]-seq_len, seq_len):\n","  datav_X = np.expand_dims(X_val[i:i + seq_len], 0)\n","  datav_Y = np.expand_dims(y_val[i:i + seq_len,0], 0)\n","  X.append(datav_X)\n","  Y.append(datav_Y)\n","X_val= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","y_val=np.concatenate(Y, axis=0)\n","\n","def data1_load(batch):\n","    data_train = torch.from_numpy(data_X).type(torch.float32)\n","    label_train = torch.from_numpy(data_Y).type(torch.float32)\n","    data_val = torch.from_numpy(X_val).type(torch.float32)\n","    label_val = torch.from_numpy(y_val).type(torch.float32)\n","    # data_test = torch.from_numpy(X_test).type(torch.float32)\n","    # label_test = torch.from_numpy(y_test).type(torch.float32)\n","\n","    dataset_train = TensorDataset(data_train,label_train)\n","    datatrain_loader = DataLoader(dataset_train,batch_size=batch,shuffle=False)  # 数据迭代器DataLoader\n","    dataset_val = TensorDataset(data_val,label_val)\n","    dataval_loader = DataLoader(dataset_val,batch_size=batch,shuffle=False)\n","    # dataset_test = TensorDataset(data_test,label_test)\n","\n","\n","    return datatrain_loader,dataval_loader,LAG\n","    # return datatrain_loader\n","\n","X=[]\n","Y=[]\n","Y_cs = []\n","for i in range(0, X_test.shape[0]-seq_len, seq_len):\n","  # print('i:',i)\n","  datat_X = np.expand_dims(X_test[i:i + seq_len], 0)\n","  datat_Y = np.expand_dims(y_test[i:i + seq_len,0], 0)\n","  datacs_Y = np.expand_dims(y_cs[i:i + seq_len,0], 0)\n","  X.append(datat_X)\n","  Y.append(datat_Y)\n","  Y_cs.append(datacs_Y)\n","X_test= np.concatenate(X, axis=0)  ##axis=0 按照行拼接，axis=1 按照列拼接\n","y_test=np.concatenate(Y, axis=0)\n","y_cs= np.concatenate(Y_cs, axis=0)\n","\n","def data2_load(batchtest):\n","    data_test = torch.from_numpy(X_test).type(torch.float32)\n","    label_test = torch.from_numpy(y_test).type(torch.float32)\n","\n","\n","    dataset_test = TensorDataset(data_test,label_test)\n","    datatest_loader = DataLoader(dataset_test,batch_size=batchtest,shuffle=False)\n","    return datatest_loader,label_test,y_cs"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"_EXuo8-LMrcz"},"outputs":[],"source":["###########model part####################################\n","import torch,math\n","import torch.nn as nn\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=64):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)    #64*512\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)    #64*1\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))    #256   model/2\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)# pe.requires_grad = False\n","        self.register_buffer('pe', pe)   #64*1*512\n","    def forward(self, x):     #[seq,batch,d_model]\n","        return x + self.pe[:x.size(0), :]   #64*64*512\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","          ########################自己修改过的########################\n","            # attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","            attn_scores = attn_scores.masked_fill(torch.tensor(mask == 0), -1e9)\n","          #############################################################\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def split_heads(self, x):\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output\n","class PositionWiseFeedForward(nn.Module):\n","    def __init__(self, d_model, dim_feedforward):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, dim_feedforward)\n","        self.fc2 = nn.Linear(dim_feedforward, d_model)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads,dim_feedforward=2048, dropout=0):\n","        super(EncoderLayer, self).__init__()\n","        self.d_model = d_model\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, dim_feedforward)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.w0 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.w1 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.w2 = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.linear0 = nn.Linear(d_model, d_model)\n","        self.linear1 = nn.Linear(d_model, d_model)\n","        self.linear2 = nn.Linear(d_model, d_model)\n","        self.linear3 = nn.Linear(12288, d_model)  # x.size()[1]*x.size()[2]\n","        self.linear4 = nn.Linear(9216, d_model)  # attn_output.size()[1]*attn_output.size()[2]\n","    def forward(self, x, mask=None):\n","        n_x1 = x.shape[1]\n","        n_x2 = x.shape[2]\n","#############################################\n","\n","        i = 3\n","        nn_x = n_x1 - i * (i + 1) // 2\n","        if nn_x <= 0 or nn_x > n_x1:\n","            attn_output = self.self_attn(x, x, x, mask)\n","            x = self.norm1(x + self.dropout(attn_output))\n","            ff_output = self.feed_forward(x)\n","            x = self.norm2(x + self.dropout(ff_output))\n","            return x\n","        else:\n","            x_part0 = x[:,:nn_x,:]\n","            # print('x_part0',x_part0.shape)\n","\n","            attn_output0 = self.self_attn(x_part0, x_part0, x_part0, mask)  # torch.Size([4, 10, 512])\n","            x_part1 = x[:,3:nn_x+3,:]\n","            attn_output1 = self.self_attn(x_part1, x_part1, x_part1, mask)\n","            x_part2 = x[:,6:,:]\n","            # print('权重：',self.w0,self.w1,self.w2)\n","            # print('x_part2:',x_part2.shape)\n","\n","            attn_output2 = self.self_attn(x_part2, x_part2, x_part2, mask)\n","            # print('attn_output012的结果已经计算好了')\n","            attn_output = torch.mul(self.w0, self.linear0(attn_output0)) + \\\n","                      torch.mul(self.w1, self.linear1(attn_output1)) + \\\n","                      torch.mul(self.w2, self.linear2(attn_output2))\n","            # print('attn_output的结果已经计算好了')\n","            x = self.linear3(x.view(x.size()[0],x.size()[1]*x.size()[2]))\n","            attn_output = self.linear4(attn_output.view(attn_output.size()[0],attn_output.size()[1]*attn_output.size()[2]))\n","            x = self.norm1(x + attn_output)\n","            ff_output = self.feed_forward(x)\n","            x = self.norm2(x + self.dropout(ff_output))\n","            return x\n","\n","        # x_part0 = x[:,:10,:]\n","        # attn_output0 = self.self_attn(x_part0, x_part0, x_part0, mask)  # torch.Size([4, 10, 512])\n","        # x_part1 = x[:,3:13,:]\n","        # attn_output1 = self.self_attn(x_part1, x_part1, x_part1, mask)  # torch.Size([4, 10, 512])\n","        # x_part2 = x[:,6:,:]\n","        # attn_output2 = self.self_attn(x_part2, x_part2, x_part2, mask)  # torch.Size([4, 10, 512])\n","        # attn_output = torch.mul(self.w0, self.linear0(attn_output0)) + \\\n","        #               torch.mul(self.w1, self.linear1(attn_output1)) + \\\n","        #               torch.mul(self.w2, self.linear2(attn_output2))    # torch.Size([4, 10, 512])\n","        # x = self.linear3(x.view(x.size()[0],x.size()[1]*x.size()[2]))\n","        # # print(self.w0,self.w1,self.w2)\n","        # attn_output = self.linear4(attn_output.view(attn_output.size()[0],attn_output.size()[1]*attn_output.size()[2]))\n","        # x = self.norm1(x + attn_output)\n","        # ff_output = self.feed_forward(x)\n","        # x = self.norm2(x + self.dropout(ff_output))\n","        # return x\n","\n","class TransformerRegressor(nn.Module):\n","    def __init__(self, input_dim, output_dim, num_heads, d_model):\n","        super(TransformerRegressor, self).__init__()\n","        self.embedding = nn.Linear(input_dim, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","        ####这里就要开始考虑x.shape\n","        self.transformer = EncoderLayer(d_model,num_heads)\n","        self.linear = nn.Linear(d_model, output_dim)\n","\n","    def forward(self, x):\n","        # print(x.shape)  # torch.Size([32, 1, 9])\n","        x = self.embedding(x)  # torch.Size([32, 1, 512])\n","        # print('输入的shape:',x.shape)\n","        x = self.pos_encoder(x)  # torch.Size([4, 16, 512])\n","        x = self.transformer(x)  # torch.Size([4, 512])\n","        x = self.linear(x)\n","        return x\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"HadsU4b1MZJr"},"outputs":[],"source":["###################train part##########################\n","# from model import TransformerRegressor\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn as nn\n","import torch.optim as optim\n","from matplotlib import pyplot\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","from datetime import datetime\n","import time\n","import os\n","# from data import datatrain_load,dataval_load\n","\n","# from data2 import data1_load,data2_load\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","writer = SummaryWriter('./logs/')\n","\n","# prediction horizon\n","K = 24\n","\n","\n","#epochs\n","epochs = 500\n","\n","\n","# use exogenous inputs\n","EXOGENOUS = True\n","\n","# features\n","if(EXOGENOUS):\n","    features = ['K','uvIndex','cloudCover','sunshineDuration','windBearing','humidity','temperature','hour','dewPoint']\n","else:\n","    features = ['K']\n","\n","\n","# metrics\n","def mad(y_pred,y_test):\n","    return 100 / y_test.mean() * np.absolute(y_pred - y_test).sum() / y_pred.size\n","\n","def mdb(y_pred,y_test):\n","    return 100 / y_test.mean() * (y_pred - y_test).sum() / y_pred.size\n","\n","def r2(y_pred,y_test):\n","    return r2_score(y_test, y_pred)\n","\n","def rmsd(y_pred,y_test):\n","    return 100 / y_test.mean() * np.sqrt(np.sum(np.power(y_pred - y_test, 2)) / y_pred.size)\n","\n","def mae(y_pred,y_test):\n","    return mean_absolute_error(y_test, y_pred)\n","\n","def mse(y_pred,y_test):\n","    return mean_squared_error(y_test, y_pred)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"ushHakcx0st-"},"outputs":[],"source":["train_loader,val_loader,lag= data1_load(32)\n","test_loader,label_test,y_cs= data2_load(1)\n","num_clo = y_cs.shape[0]\n","Num_f = len(features)\n","input_d= Num_f*lag\n","model = TransformerRegressor(input_dim=input_d,output_dim=K,num_heads=8,d_model=512).to(device)\n","criterion = nn.MSELoss().to(device)     # 忽略 占位符 索引为0.\n","optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n","val_loss = []\n","train_loss = []\n","best_loss = 10000\n","y_preds = np.zeros(y_cs.shape)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"4jKk-RRC03P6"},"outputs":[],"source":["def train_val():\n","    # j = 0\n","    best_test_loss = 100000\n","    for epoch in range(epochs):\n","        # if j >=100:\n","        #     break\n","        # else: j +=1\n","        train_epoch_loss = []\n","        val_epoch_loss = []\n","        for X_train, Y_train in train_loader:\n","            y= Y_train\n","            # y = Y_train[:,i]\n","            #####################################################################################\n","            # y = y.unsqueeze(1)\n","            # X_tr = np.expand_dims(X_train,0)\n","            # y_tr = np.expand_dims(y,0)\n","            # input_xtr = X_tr.transpose(1, 0, 2) #数组转维度，(1,32,9) -->(32,1,9)\n","            #########################################################################################\n","            input_xtr=X_train.clone().detach().requires_grad_(True)\n","            # print('input_xtr:',input_xtr.shape)\n","            input_ytr = Y_train.clone().detach().requires_grad_(True)\n","            input_xtr = input_xtr.clone().detach().to(device)\n","            input_ytr = input_ytr.clone().detach().to(device)\n","            optimizer.zero_grad()\n","            output = model(input_xtr).to(device)#训练\n","            trainloss = criterion(output, input_ytr).to(device)\n","            trainloss.backward()\n","            optimizer.step()\n","        train_epoch_loss.append(trainloss.item())\n","        train_loss.append(np.mean(train_epoch_loss))\n","        writer.add_scalar(\"train_loss\", np.mean(train_epoch_loss))\n","        for X_val, y_val in val_loader:\n","          ############################################################\n","            # y_val = y_val\n","            # y_val = y_val.unsqueeze(1)\n","            # x_v = np.expand_dims(X_val, 0)\n","            # y_v = np.expand_dims(y_val, 0)\n","            # input_xval = x_v.transpose(1, 0, 2)\n","            # input_yval = y_v.transpose(1, 0, 2)\n","            # X_val = torch.tensor(input_xval).to(device)\n","            # Y_val = torch.tensor(input_yval).to(device)\n","          ################################################################\n","            X_val = X_val.clone().detach().to(device)\n","            Y_val = y_val.clone().detach().to(device)\n","            output = model(X_val).to(device)\n","            valloss = criterion(output, Y_val)\n","            val_epoch_loss.append(valloss.item())\n","        val_loss.append(np.mean(val_epoch_loss))\n","        writer.add_scalar(\"val_loss\", np.mean(val_epoch_loss), epoch)\n","        datetimeStr = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        print(datetimeStr +\"  Epoch【\" + str(epoch) +  \"】 \" +  \"train_loss:\" + str(round(np.mean(train_epoch_loss), 5)) + \"  val_loss:\" + str(round(np.mean(val_epoch_loss), 5)))\n","\n","        if np.mean(val_epoch_loss) < best_test_loss:\n","            # j = 0\n","            best_test_loss = np.mean(val_epoch_loss)\n","            best_model = model\n","            print(\"best_test_loss:\", best_test_loss)\n","            torch.save(best_model.state_dict(), './logs/best_Transformer_trainModel.pth')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1698056037332,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"k8bS1Q3-06Km"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-10-25 15:47:10  Epoch【0】 train_loss:0.15393  val_loss:0.13674\n","best_test_loss: 0.13673514322094296\n","2023-10-25 15:47:12  Epoch【1】 train_loss:0.08811  val_loss:0.0929\n","best_test_loss: 0.09289665406812793\n","2023-10-25 15:47:14  Epoch【2】 train_loss:0.06286  val_loss:0.062\n","best_test_loss: 0.06200489029288292\n","2023-10-25 15:47:16  Epoch【3】 train_loss:0.04152  val_loss:0.09861\n","2023-10-25 15:47:18  Epoch【4】 train_loss:0.03368  val_loss:0.11603\n","2023-10-25 15:47:20  Epoch【5】 train_loss:0.19656  val_loss:0.73568\n","2023-10-25 15:47:22  Epoch【6】 train_loss:1.51347  val_loss:4.12466\n","2023-10-25 15:47:23  Epoch【7】 train_loss:0.19603  val_loss:0.14757\n","2023-10-25 15:47:25  Epoch【8】 train_loss:0.06234  val_loss:0.21481\n","2023-10-25 15:47:27  Epoch【9】 train_loss:0.15549  val_loss:0.12738\n","2023-10-25 15:47:30  Epoch【10】 train_loss:0.0845  val_loss:0.08103\n","2023-10-25 15:47:32  Epoch【11】 train_loss:0.04991  val_loss:0.17251\n","2023-10-25 15:47:34  Epoch【12】 train_loss:0.07517  val_loss:0.13237\n","2023-10-25 15:47:36  Epoch【13】 train_loss:0.07274  val_loss:0.06205\n","2023-10-25 15:47:38  Epoch【14】 train_loss:0.06752  val_loss:0.15563\n","2023-10-25 15:47:40  Epoch【15】 train_loss:0.04228  val_loss:0.0474\n","best_test_loss: 0.047397132396050125\n","2023-10-25 15:47:42  Epoch【16】 train_loss:0.05637  val_loss:0.06409\n","2023-10-25 15:47:44  Epoch【17】 train_loss:0.04926  val_loss:0.0489\n","2023-10-25 15:47:46  Epoch【18】 train_loss:0.03924  val_loss:0.03845\n","best_test_loss: 0.038451144876687424\n","2023-10-25 15:47:48  Epoch【19】 train_loss:0.03475  val_loss:0.02981\n","best_test_loss: 0.029811663553118706\n","2023-10-25 15:47:50  Epoch【20】 train_loss:0.02857  val_loss:0.02563\n","best_test_loss: 0.02562755297707475\n","2023-10-25 15:47:52  Epoch【21】 train_loss:0.02761  val_loss:0.02378\n","best_test_loss: 0.02378412081009668\n","2023-10-25 15:47:54  Epoch【22】 train_loss:0.02547  val_loss:0.0227\n","best_test_loss: 0.022701707263679607\n","2023-10-25 15:47:56  Epoch【23】 train_loss:0.02457  val_loss:0.02192\n","best_test_loss: 0.021922703225003635\n","2023-10-25 15:47:58  Epoch【24】 train_loss:0.02364  val_loss:0.02129\n","best_test_loss: 0.021291912816788838\n","2023-10-25 15:47:59  Epoch【25】 train_loss:0.02277  val_loss:0.02073\n","best_test_loss: 0.020729425606196342\n","2023-10-25 15:48:01  Epoch【26】 train_loss:0.02198  val_loss:0.02021\n","best_test_loss: 0.020208841311218945\n","2023-10-25 15:48:03  Epoch【27】 train_loss:0.0213  val_loss:0.0197\n","best_test_loss: 0.019698547604291336\n","2023-10-25 15:48:05  Epoch【28】 train_loss:0.02069  val_loss:0.01921\n","best_test_loss: 0.01920925291336101\n","2023-10-25 15:48:07  Epoch【29】 train_loss:0.02013  val_loss:0.01872\n","best_test_loss: 0.018717652875120224\n","2023-10-25 15:48:09  Epoch【30】 train_loss:0.01972  val_loss:0.01825\n","best_test_loss: 0.0182520914417894\n","2023-10-25 15:48:11  Epoch【31】 train_loss:0.01928  val_loss:0.0178\n","best_test_loss: 0.01779572584706804\n","2023-10-25 15:48:13  Epoch【32】 train_loss:0.01884  val_loss:0.01734\n","best_test_loss: 0.01734452828279008\n","2023-10-25 15:48:14  Epoch【33】 train_loss:0.01845  val_loss:0.01689\n","best_test_loss: 0.01689215709009896\n","2023-10-25 15:48:16  Epoch【34】 train_loss:0.0181  val_loss:0.01645\n","best_test_loss: 0.016454636767182663\n","2023-10-25 15:48:18  Epoch【35】 train_loss:0.0178  val_loss:0.0161\n","best_test_loss: 0.016098418313523998\n","2023-10-25 15:48:20  Epoch【36】 train_loss:0.01753  val_loss:0.01573\n","best_test_loss: 0.015734528272372227\n","2023-10-25 15:48:22  Epoch【37】 train_loss:0.01724  val_loss:0.01532\n","best_test_loss: 0.015323862109495245\n","2023-10-25 15:48:24  Epoch【38】 train_loss:0.01695  val_loss:0.01489\n","best_test_loss: 0.01488732921121561\n","2023-10-25 15:48:26  Epoch【39】 train_loss:0.01666  val_loss:0.01448\n","best_test_loss: 0.014477130329317373\n","2023-10-25 15:48:27  Epoch【40】 train_loss:0.01645  val_loss:0.01416\n","best_test_loss: 0.014160832572404457\n","2023-10-25 15:48:29  Epoch【41】 train_loss:0.01623  val_loss:0.01387\n","best_test_loss: 0.01386753319884124\n","2023-10-25 15:48:31  Epoch【42】 train_loss:0.01593  val_loss:0.01344\n","best_test_loss: 0.013441615737974644\n","2023-10-25 15:48:33  Epoch【43】 train_loss:0.01576  val_loss:0.01326\n","best_test_loss: 0.013262134126347044\n","2023-10-25 15:48:35  Epoch【44】 train_loss:0.01552  val_loss:0.01293\n","best_test_loss: 0.01292768991349832\n","2023-10-25 15:48:37  Epoch【45】 train_loss:0.01536  val_loss:0.01271\n","best_test_loss: 0.012707853041913198\n","2023-10-25 15:48:40  Epoch【46】 train_loss:0.01521  val_loss:0.01253\n","best_test_loss: 0.012534927212349747\n","2023-10-25 15:48:42  Epoch【47】 train_loss:0.01509  val_loss:0.01233\n","best_test_loss: 0.012330330300914205\n","2023-10-25 15:48:43  Epoch【48】 train_loss:0.01495  val_loss:0.0122\n","best_test_loss: 0.012204760786793802\n","2023-10-25 15:48:45  Epoch【49】 train_loss:0.0148  val_loss:0.01197\n","best_test_loss: 0.011972089743484621\n","2023-10-25 15:48:47  Epoch【50】 train_loss:0.01469  val_loss:0.01183\n","best_test_loss: 0.01182913672907845\n","2023-10-25 15:48:49  Epoch【51】 train_loss:0.01462  val_loss:0.01174\n","best_test_loss: 0.011737447758407696\n","2023-10-25 15:48:51  Epoch【52】 train_loss:0.01451  val_loss:0.01158\n","best_test_loss: 0.011577961257780376\n","2023-10-25 15:48:53  Epoch【53】 train_loss:0.01441  val_loss:0.01148\n","best_test_loss: 0.011478334847513748\n","2023-10-25 15:48:55  Epoch【54】 train_loss:0.01431  val_loss:0.01133\n","best_test_loss: 0.011334980493816345\n","2023-10-25 15:48:57  Epoch【55】 train_loss:0.01425  val_loss:0.01124\n","best_test_loss: 0.011235534476683191\n","2023-10-25 15:48:59  Epoch【56】 train_loss:0.01416  val_loss:0.01116\n","best_test_loss: 0.011162831286049408\n","2023-10-25 15:49:01  Epoch【57】 train_loss:0.01417  val_loss:0.01097\n","best_test_loss: 0.01097348336454319\n","2023-10-25 15:49:03  Epoch【58】 train_loss:0.01432  val_loss:0.01125\n","2023-10-25 15:49:05  Epoch【59】 train_loss:0.01414  val_loss:0.0108\n","best_test_loss: 0.010798707422192978\n","2023-10-25 15:49:07  Epoch【60】 train_loss:0.01407  val_loss:0.01064\n","best_test_loss: 0.010641081805300453\n","2023-10-25 15:49:08  Epoch【61】 train_loss:0.01386  val_loss:0.01086\n","2023-10-25 15:49:10  Epoch【62】 train_loss:0.0137  val_loss:0.01069\n","2023-10-25 15:49:12  Epoch【63】 train_loss:0.01355  val_loss:0.01045\n","best_test_loss: 0.010447191074490547\n","2023-10-25 15:49:14  Epoch【64】 train_loss:0.01346  val_loss:0.01031\n","best_test_loss: 0.010309834883588812\n","2023-10-25 15:49:16  Epoch【65】 train_loss:0.01334  val_loss:0.01021\n","best_test_loss: 0.010213604419613663\n","2023-10-25 15:49:17  Epoch【66】 train_loss:0.01324  val_loss:0.01012\n","best_test_loss: 0.010117237030973902\n","2023-10-25 15:49:19  Epoch【67】 train_loss:0.01316  val_loss:0.01001\n","best_test_loss: 0.010010632119424965\n","2023-10-25 15:49:21  Epoch【68】 train_loss:0.01305  val_loss:0.0099\n","best_test_loss: 0.00990371353438367\n","2023-10-25 15:49:23  Epoch【69】 train_loss:0.01295  val_loss:0.0098\n","best_test_loss: 0.009803443742187126\n","2023-10-25 15:49:25  Epoch【70】 train_loss:0.01284  val_loss:0.0097\n","best_test_loss: 0.009704167367485554\n","2023-10-25 15:49:27  Epoch【71】 train_loss:0.01274  val_loss:0.0096\n","best_test_loss: 0.00960161056086097\n","2023-10-25 15:49:29  Epoch【72】 train_loss:0.01263  val_loss:0.0095\n","best_test_loss: 0.009498591636024092\n","2023-10-25 15:49:30  Epoch【73】 train_loss:0.01251  val_loss:0.00939\n","best_test_loss: 0.009394991424177651\n","2023-10-25 15:49:32  Epoch【74】 train_loss:0.0124  val_loss:0.00929\n","best_test_loss: 0.009291367048559629\n","2023-10-25 15:49:34  Epoch【75】 train_loss:0.0123  val_loss:0.00919\n","best_test_loss: 0.009189694158166\n","2023-10-25 15:49:36  Epoch【76】 train_loss:0.01219  val_loss:0.00909\n","best_test_loss: 0.009086676356990052\n","2023-10-25 15:49:38  Epoch【77】 train_loss:0.0121  val_loss:0.00898\n","best_test_loss: 0.008983085508984716\n","2023-10-25 15:49:40  Epoch【78】 train_loss:0.01199  val_loss:0.00888\n","best_test_loss: 0.008882418801279171\n","2023-10-25 15:49:42  Epoch【79】 train_loss:0.01189  val_loss:0.00878\n","best_test_loss: 0.008783797622131913\n","2023-10-25 15:49:44  Epoch【80】 train_loss:0.01178  val_loss:0.00869\n","best_test_loss: 0.00868926553622536\n","2023-10-25 15:49:45  Epoch【81】 train_loss:0.01167  val_loss:0.0086\n","best_test_loss: 0.008596909303299111\n","2023-10-25 15:49:47  Epoch【82】 train_loss:0.01156  val_loss:0.0085\n","best_test_loss: 0.008504895361788247\n","2023-10-25 15:49:49  Epoch【83】 train_loss:0.01146  val_loss:0.00842\n","best_test_loss: 0.008416372088148542\n","2023-10-25 15:49:51  Epoch【84】 train_loss:0.01136  val_loss:0.00833\n","best_test_loss: 0.008329126255019852\n","2023-10-25 15:49:53  Epoch【85】 train_loss:0.01126  val_loss:0.00824\n","best_test_loss: 0.008244847403029385\n","2023-10-25 15:49:55  Epoch【86】 train_loss:0.01116  val_loss:0.00816\n","best_test_loss: 0.008161367899130868\n","2023-10-25 15:49:57  Epoch【87】 train_loss:0.01107  val_loss:0.00808\n","best_test_loss: 0.008081521494718998\n","2023-10-25 15:49:58  Epoch【88】 train_loss:0.01097  val_loss:0.008\n","best_test_loss: 0.008003775546651172\n","2023-10-25 15:50:00  Epoch【89】 train_loss:0.01088  val_loss:0.00793\n","best_test_loss: 0.007927896677637878\n","2023-10-25 15:50:02  Epoch【90】 train_loss:0.01079  val_loss:0.00786\n","best_test_loss: 0.007856294654471718\n","2023-10-25 15:50:04  Epoch【91】 train_loss:0.01072  val_loss:0.00779\n","best_test_loss: 0.007785664689119743\n","2023-10-25 15:50:06  Epoch【92】 train_loss:0.01063  val_loss:0.00772\n","best_test_loss: 0.007717231683352072\n","2023-10-25 15:50:08  Epoch【93】 train_loss:0.01055  val_loss:0.00765\n","best_test_loss: 0.007651372428011635\n","2023-10-25 15:50:10  Epoch【94】 train_loss:0.01044  val_loss:0.00758\n","best_test_loss: 0.007583952579728287\n","2023-10-25 15:50:11  Epoch【95】 train_loss:0.01033  val_loss:0.00752\n","best_test_loss: 0.007521591037916748\n","2023-10-25 15:50:13  Epoch【96】 train_loss:0.01025  val_loss:0.00746\n","best_test_loss: 0.007462233689654133\n","2023-10-25 15:50:15  Epoch【97】 train_loss:0.01023  val_loss:0.0074\n","best_test_loss: 0.00740485764918444\n","2023-10-25 15:50:17  Epoch【98】 train_loss:0.01018  val_loss:0.00735\n","best_test_loss: 0.007348529387103475\n","2023-10-25 15:50:19  Epoch【99】 train_loss:0.00999  val_loss:0.00731\n","best_test_loss: 0.007314193366176408\n","2023-10-25 15:50:21  Epoch【100】 train_loss:0.00995  val_loss:0.00725\n","best_test_loss: 0.00724685944252364\n","2023-10-25 15:50:23  Epoch【101】 train_loss:0.01012  val_loss:0.00724\n","best_test_loss: 0.00724313188465717\n","2023-10-25 15:50:24  Epoch【102】 train_loss:0.00975  val_loss:0.00718\n","best_test_loss: 0.007182792659201052\n","2023-10-25 15:50:26  Epoch【103】 train_loss:0.00976  val_loss:0.00714\n","best_test_loss: 0.007139785883381315\n","2023-10-25 15:50:28  Epoch【104】 train_loss:0.01008  val_loss:0.00718\n","2023-10-25 15:50:30  Epoch【105】 train_loss:0.00953  val_loss:0.00702\n","best_test_loss: 0.007020091597476731\n","2023-10-25 15:50:32  Epoch【106】 train_loss:0.00956  val_loss:0.00703\n","2023-10-25 15:50:34  Epoch【107】 train_loss:0.0097  val_loss:0.00696\n","best_test_loss: 0.006963303130443977\n","2023-10-25 15:50:37  Epoch【108】 train_loss:0.00955  val_loss:0.00683\n","best_test_loss: 0.006828952661674955\n","2023-10-25 15:50:38  Epoch【109】 train_loss:0.00943  val_loss:0.00704\n","2023-10-25 15:50:40  Epoch【110】 train_loss:0.00934  val_loss:0.00677\n","best_test_loss: 0.006767850176876654\n","2023-10-25 15:50:42  Epoch【111】 train_loss:0.00973  val_loss:0.00682\n","2023-10-25 15:50:44  Epoch【112】 train_loss:0.00906  val_loss:0.00665\n","best_test_loss: 0.006647193510814205\n","2023-10-25 15:50:46  Epoch【113】 train_loss:0.00906  val_loss:0.00667\n","2023-10-25 15:50:47  Epoch【114】 train_loss:0.00903  val_loss:0.00654\n","best_test_loss: 0.006541303531064287\n","2023-10-25 15:50:49  Epoch【115】 train_loss:0.00906  val_loss:0.0065\n","best_test_loss: 0.006501939400787587\n","2023-10-25 15:50:51  Epoch【116】 train_loss:0.00881  val_loss:0.00641\n","best_test_loss: 0.006409794557839632\n","2023-10-25 15:50:53  Epoch【117】 train_loss:0.00869  val_loss:0.0064\n","best_test_loss: 0.006396251933082291\n","2023-10-25 15:50:55  Epoch【118】 train_loss:0.00866  val_loss:0.00633\n","best_test_loss: 0.006327879584758826\n","2023-10-25 15:50:57  Epoch【119】 train_loss:0.00864  val_loss:0.00627\n","best_test_loss: 0.006271558044397313\n","2023-10-25 15:50:59  Epoch【120】 train_loss:0.00852  val_loss:0.00622\n","best_test_loss: 0.006219213879059838\n","2023-10-25 15:51:00  Epoch【121】 train_loss:0.00841  val_loss:0.00618\n","best_test_loss: 0.006182100714953697\n","2023-10-25 15:51:02  Epoch【122】 train_loss:0.00836  val_loss:0.00613\n","best_test_loss: 0.006127022778736832\n","2023-10-25 15:51:04  Epoch【123】 train_loss:0.0083  val_loss:0.00608\n","best_test_loss: 0.006076914425331937\n","2023-10-25 15:51:06  Epoch【124】 train_loss:0.00818  val_loss:0.00604\n","best_test_loss: 0.00604421359932293\n","2023-10-25 15:51:08  Epoch【125】 train_loss:0.00812  val_loss:0.00599\n","best_test_loss: 0.00599254227405333\n","2023-10-25 15:51:10  Epoch【126】 train_loss:0.00815  val_loss:0.00595\n","best_test_loss: 0.005954916042316219\n","2023-10-25 15:51:12  Epoch【127】 train_loss:0.00794  val_loss:0.00592\n","best_test_loss: 0.005917654251274856\n","2023-10-25 15:51:14  Epoch【128】 train_loss:0.00787  val_loss:0.00587\n","best_test_loss: 0.005866270289634881\n","2023-10-25 15:51:16  Epoch【129】 train_loss:0.00804  val_loss:0.00585\n","best_test_loss: 0.005847692940100704\n","2023-10-25 15:51:17  Epoch【130】 train_loss:0.00774  val_loss:0.00584\n","best_test_loss: 0.005839830335310619\n","2023-10-25 15:51:19  Epoch【131】 train_loss:0.00766  val_loss:0.00574\n","best_test_loss: 0.0057394909166285524\n","2023-10-25 15:51:21  Epoch【132】 train_loss:0.00809  val_loss:0.00579\n","2023-10-25 15:51:23  Epoch【133】 train_loss:0.00762  val_loss:0.00587\n","2023-10-25 15:51:25  Epoch【134】 train_loss:0.00753  val_loss:0.00567\n","best_test_loss: 0.005671057759014809\n","2023-10-25 15:51:26  Epoch【135】 train_loss:0.00826  val_loss:0.00579\n","2023-10-25 15:51:28  Epoch【136】 train_loss:0.00778  val_loss:0.00607\n","2023-10-25 15:51:30  Epoch【137】 train_loss:0.00733  val_loss:0.00555\n","best_test_loss: 0.005554883194196484\n","2023-10-25 15:51:32  Epoch【138】 train_loss:0.00883  val_loss:0.00604\n","2023-10-25 15:51:34  Epoch【139】 train_loss:0.00756  val_loss:0.0059\n","2023-10-25 15:51:35  Epoch【140】 train_loss:0.00724  val_loss:0.00552\n","best_test_loss: 0.005521028378532957\n","2023-10-25 15:51:37  Epoch【141】 train_loss:0.00879  val_loss:0.00624\n","2023-10-25 15:51:39  Epoch【142】 train_loss:0.00721  val_loss:0.00541\n","best_test_loss: 0.005412221633140807\n","2023-10-25 15:51:41  Epoch【143】 train_loss:0.00731  val_loss:0.00568\n","2023-10-25 15:51:43  Epoch【144】 train_loss:0.00715  val_loss:0.00537\n","best_test_loss: 0.005372513328557429\n","2023-10-25 15:51:44  Epoch【145】 train_loss:0.00763  val_loss:0.00547\n","2023-10-25 15:51:46  Epoch【146】 train_loss:0.0068  val_loss:0.00531\n","best_test_loss: 0.005307753514700934\n","2023-10-25 15:51:49  Epoch【147】 train_loss:0.0067  val_loss:0.00522\n","best_test_loss: 0.005216281476390102\n","2023-10-25 15:51:51  Epoch【148】 train_loss:0.00725  val_loss:0.00539\n","2023-10-25 15:51:53  Epoch【149】 train_loss:0.00675  val_loss:0.0051\n","best_test_loss: 0.005103216248402453\n","2023-10-25 15:51:55  Epoch【150】 train_loss:0.00667  val_loss:0.00526\n","2023-10-25 15:51:57  Epoch【151】 train_loss:0.00647  val_loss:0.00503\n","best_test_loss: 0.005026670355025841\n","2023-10-25 15:51:59  Epoch【152】 train_loss:0.00714  val_loss:0.00525\n","2023-10-25 15:52:01  Epoch【153】 train_loss:0.00634  val_loss:0.00502\n","best_test_loss: 0.005024738098575693\n","2023-10-25 15:52:03  Epoch【154】 train_loss:0.00638  val_loss:0.00508\n","2023-10-25 15:52:05  Epoch【155】 train_loss:0.0067  val_loss:0.00512\n","2023-10-25 15:52:07  Epoch【156】 train_loss:0.00655  val_loss:0.00493\n","best_test_loss: 0.00493402323057956\n","2023-10-25 15:52:09  Epoch【157】 train_loss:0.0064  val_loss:0.00515\n","2023-10-25 15:52:11  Epoch【158】 train_loss:0.00604  val_loss:0.00481\n","best_test_loss: 0.004814635238448239\n","2023-10-25 15:52:13  Epoch【159】 train_loss:0.00711  val_loss:0.00523\n","2023-10-25 15:52:15  Epoch【160】 train_loss:0.00598  val_loss:0.00477\n","best_test_loss: 0.004773920609453774\n","2023-10-25 15:52:17  Epoch【161】 train_loss:0.00633  val_loss:0.00508\n","2023-10-25 15:52:19  Epoch【162】 train_loss:0.00633  val_loss:0.0049\n","2023-10-25 15:52:21  Epoch【163】 train_loss:0.00661  val_loss:0.00489\n","2023-10-25 15:52:23  Epoch【164】 train_loss:0.00613  val_loss:0.005\n","2023-10-25 15:52:25  Epoch【165】 train_loss:0.00577  val_loss:0.00465\n","best_test_loss: 0.00465157059142771\n","2023-10-25 15:52:27  Epoch【166】 train_loss:0.00702  val_loss:0.00516\n","2023-10-25 15:52:29  Epoch【167】 train_loss:0.00571  val_loss:0.00454\n","best_test_loss: 0.004537857719697058\n","2023-10-25 15:52:31  Epoch【168】 train_loss:0.00611  val_loss:0.00493\n","2023-10-25 15:52:33  Epoch【169】 train_loss:0.00585  val_loss:0.00461\n","2023-10-25 15:52:34  Epoch【170】 train_loss:0.00639  val_loss:0.00477\n","2023-10-25 15:52:36  Epoch【171】 train_loss:0.00563  val_loss:0.00464\n","2023-10-25 15:52:38  Epoch【172】 train_loss:0.00548  val_loss:0.00449\n","best_test_loss: 0.004486707980623064\n","2023-10-25 15:52:40  Epoch【173】 train_loss:0.00623  val_loss:0.00475\n","2023-10-25 15:52:41  Epoch【174】 train_loss:0.00549  val_loss:0.00437\n","best_test_loss: 0.004371614696498475\n","2023-10-25 15:52:43  Epoch【175】 train_loss:0.00559  val_loss:0.00463\n","2023-10-25 15:52:45  Epoch【176】 train_loss:0.00533  val_loss:0.00433\n","best_test_loss: 0.004331175830093739\n","2023-10-25 15:52:47  Epoch【177】 train_loss:0.0062  val_loss:0.00469\n","2023-10-25 15:52:49  Epoch【178】 train_loss:0.00513  val_loss:0.00429\n","best_test_loss: 0.00429408190994645\n","2023-10-25 15:52:51  Epoch【179】 train_loss:0.00544  val_loss:0.0045\n","2023-10-25 15:52:52  Epoch【180】 train_loss:0.00557  val_loss:0.00444\n","2023-10-25 15:52:54  Epoch【181】 train_loss:0.00573  val_loss:0.00441\n","2023-10-25 15:52:56  Epoch【182】 train_loss:0.00526  val_loss:0.00447\n","2023-10-25 15:52:58  Epoch【183】 train_loss:0.005  val_loss:0.0042\n","best_test_loss: 0.004199561278295258\n","2023-10-25 15:53:00  Epoch【184】 train_loss:0.00613  val_loss:0.00467\n","2023-10-25 15:53:02  Epoch【185】 train_loss:0.00496  val_loss:0.00409\n","best_test_loss: 0.0040861566692752685\n","2023-10-25 15:53:04  Epoch【186】 train_loss:0.00526  val_loss:0.00445\n","2023-10-25 15:53:05  Epoch【187】 train_loss:0.00486  val_loss:0.00407\n","best_test_loss: 0.004073189460146038\n","2023-10-25 15:53:07  Epoch【188】 train_loss:0.00569  val_loss:0.00441\n","2023-10-25 15:53:09  Epoch【189】 train_loss:0.00467  val_loss:0.00405\n","best_test_loss: 0.004051565632993436\n","2023-10-25 15:53:11  Epoch【190】 train_loss:0.00481  val_loss:0.00414\n","2023-10-25 15:53:13  Epoch【191】 train_loss:0.00514  val_loss:0.0042\n","2023-10-25 15:53:14  Epoch【192】 train_loss:0.00499  val_loss:0.00406\n","2023-10-25 15:53:16  Epoch【193】 train_loss:0.00471  val_loss:0.00414\n","2023-10-25 15:53:18  Epoch【194】 train_loss:0.00445  val_loss:0.0039\n","best_test_loss: 0.0039011837216094136\n","2023-10-25 15:53:20  Epoch【195】 train_loss:0.00539  val_loss:0.0043\n","2023-10-25 15:53:22  Epoch【196】 train_loss:0.00447  val_loss:0.00384\n","best_test_loss: 0.0038369263081973336\n","2023-10-25 15:53:23  Epoch【197】 train_loss:0.00471  val_loss:0.00416\n","2023-10-25 15:53:25  Epoch【198】 train_loss:0.0043  val_loss:0.00379\n","best_test_loss: 0.003794381015367158\n","2023-10-25 15:53:27  Epoch【199】 train_loss:0.00532  val_loss:0.00424\n","2023-10-25 15:53:29  Epoch【200】 train_loss:0.00415  val_loss:0.00375\n","best_test_loss: 0.003746621217072496\n","2023-10-25 15:53:31  Epoch【201】 train_loss:0.0045  val_loss:0.00401\n","2023-10-25 15:53:33  Epoch【202】 train_loss:0.00438  val_loss:0.00382\n","2023-10-25 15:53:34  Epoch【203】 train_loss:0.00486  val_loss:0.004\n","2023-10-25 15:53:36  Epoch【204】 train_loss:0.00406  val_loss:0.00375\n","2023-10-25 15:53:38  Epoch【205】 train_loss:0.00416  val_loss:0.00379\n","2023-10-25 15:53:40  Epoch【206】 train_loss:0.00447  val_loss:0.00387\n","2023-10-25 15:53:42  Epoch【207】 train_loss:0.0044  val_loss:0.00377\n","2023-10-25 15:53:44  Epoch【208】 train_loss:0.00401  val_loss:0.00376\n","2023-10-25 15:53:46  Epoch【209】 train_loss:0.00387  val_loss:0.00362\n","best_test_loss: 0.003615272889399658\n","2023-10-25 15:53:48  Epoch【210】 train_loss:0.00462  val_loss:0.00392\n","2023-10-25 15:53:50  Epoch【211】 train_loss:0.004  val_loss:0.00359\n","best_test_loss: 0.0035855015157721937\n","2023-10-25 15:53:52  Epoch【212】 train_loss:0.00402  val_loss:0.00379\n","2023-10-25 15:53:53  Epoch【213】 train_loss:0.00365  val_loss:0.00348\n","best_test_loss: 0.0034751749632920582\n","2023-10-25 15:53:55  Epoch【214】 train_loss:0.00465  val_loss:0.00393\n","2023-10-25 15:53:57  Epoch【215】 train_loss:0.00364  val_loss:0.00343\n","best_test_loss: 0.0034349552259537513\n","2023-10-25 15:53:59  Epoch【216】 train_loss:0.00394  val_loss:0.00374\n","2023-10-25 15:54:01  Epoch【217】 train_loss:0.00351  val_loss:0.00339\n","best_test_loss: 0.003389069289171501\n","2023-10-25 15:54:03  Epoch【218】 train_loss:0.00451  val_loss:0.00385\n","2023-10-25 15:54:05  Epoch【219】 train_loss:0.00341  val_loss:0.00334\n","best_test_loss: 0.0033411165755039647\n","2023-10-25 15:54:07  Epoch【220】 train_loss:0.00378  val_loss:0.00365\n","2023-10-25 15:54:08  Epoch【221】 train_loss:0.0034  val_loss:0.00333\n","best_test_loss: 0.0033306414615767803\n","2023-10-25 15:54:10  Epoch【222】 train_loss:0.00431  val_loss:0.00375\n","2023-10-25 15:54:12  Epoch【223】 train_loss:0.00323  val_loss:0.00327\n","best_test_loss: 0.003267156752332559\n","2023-10-25 15:54:14  Epoch【224】 train_loss:0.00362  val_loss:0.00356\n","2023-10-25 15:54:16  Epoch【225】 train_loss:0.00328  val_loss:0.00327\n","2023-10-25 15:54:17  Epoch【226】 train_loss:0.00415  val_loss:0.00366\n","2023-10-25 15:54:19  Epoch【227】 train_loss:0.00308  val_loss:0.00319\n","best_test_loss: 0.0031949431944193075\n","2023-10-25 15:54:21  Epoch【228】 train_loss:0.00348  val_loss:0.00349\n","2023-10-25 15:54:23  Epoch【229】 train_loss:0.00313  val_loss:0.00319\n","best_test_loss: 0.003188498090426235\n","2023-10-25 15:54:25  Epoch【230】 train_loss:0.004  val_loss:0.00359\n","2023-10-25 15:54:27  Epoch【231】 train_loss:0.00295  val_loss:0.00312\n","best_test_loss: 0.0031198382468732157\n","2023-10-25 15:54:29  Epoch【232】 train_loss:0.00335  val_loss:0.00342\n","2023-10-25 15:54:31  Epoch【233】 train_loss:0.00296  val_loss:0.0031\n","best_test_loss: 0.003098983274590548\n","2023-10-25 15:54:33  Epoch【234】 train_loss:0.00384  val_loss:0.00352\n","2023-10-25 15:54:35  Epoch【235】 train_loss:0.00288  val_loss:0.00306\n","best_test_loss: 0.003059473278446366\n","2023-10-25 15:54:37  Epoch【236】 train_loss:0.00319  val_loss:0.00334\n","2023-10-25 15:54:39  Epoch【237】 train_loss:0.00282  val_loss:0.00303\n","best_test_loss: 0.0030295493526627188\n","2023-10-25 15:54:40  Epoch【238】 train_loss:0.00365  val_loss:0.00343\n","2023-10-25 15:54:42  Epoch【239】 train_loss:0.00294  val_loss:0.00306\n","2023-10-25 15:54:44  Epoch【240】 train_loss:0.00298  val_loss:0.00323\n","2023-10-25 15:54:46  Epoch【241】 train_loss:0.00277  val_loss:0.00303\n","best_test_loss: 0.0030281258134535797\n","2023-10-25 15:54:48  Epoch【242】 train_loss:0.00333  val_loss:0.00327\n","2023-10-25 15:54:49  Epoch【243】 train_loss:0.00315  val_loss:0.00314\n","2023-10-25 15:54:51  Epoch【244】 train_loss:0.00269  val_loss:0.00304\n","2023-10-25 15:54:53  Epoch【245】 train_loss:0.00284  val_loss:0.0031\n","2023-10-25 15:54:55  Epoch【246】 train_loss:0.00286  val_loss:0.00302\n","best_test_loss: 0.0030210571105429985\n","2023-10-25 15:54:57  Epoch【247】 train_loss:0.00338  val_loss:0.00325\n","2023-10-25 15:54:58  Epoch【248】 train_loss:0.00244  val_loss:0.00285\n","best_test_loss: 0.0028484386882906697\n","2023-10-25 15:55:00  Epoch【249】 train_loss:0.00286  val_loss:0.00313\n","2023-10-25 15:55:02  Epoch【250】 train_loss:0.00246  val_loss:0.00282\n","best_test_loss: 0.0028156363202825837\n","2023-10-25 15:55:04  Epoch【251】 train_loss:0.00332  val_loss:0.00323\n","2023-10-25 15:55:06  Epoch【252】 train_loss:0.00249  val_loss:0.00281\n","best_test_loss: 0.002809664528088077\n","2023-10-25 15:55:08  Epoch【253】 train_loss:0.00263  val_loss:0.003\n","2023-10-25 15:55:09  Epoch【254】 train_loss:0.00241  val_loss:0.00281\n","best_test_loss: 0.0028093420374004736\n","2023-10-25 15:55:11  Epoch【255】 train_loss:0.00284  val_loss:0.00298\n","2023-10-25 15:55:13  Epoch【256】 train_loss:0.00286  val_loss:0.00297\n","2023-10-25 15:55:15  Epoch【257】 train_loss:0.00225  val_loss:0.00274\n","best_test_loss: 0.0027434258321907532\n","2023-10-25 15:55:17  Epoch【258】 train_loss:0.00256  val_loss:0.00292\n","2023-10-25 15:55:18  Epoch【259】 train_loss:0.00228  val_loss:0.00269\n","best_test_loss: 0.0026920671301448474\n","2023-10-25 15:55:20  Epoch【260】 train_loss:0.00305  val_loss:0.00306\n","2023-10-25 15:55:22  Epoch【261】 train_loss:0.0023  val_loss:0.00268\n","best_test_loss: 0.0026812771102413535\n","2023-10-25 15:55:24  Epoch【262】 train_loss:0.00238  val_loss:0.00283\n","2023-10-25 15:55:26  Epoch【263】 train_loss:0.00226  val_loss:0.00271\n","2023-10-25 15:55:27  Epoch【264】 train_loss:0.0025  val_loss:0.00278\n","2023-10-25 15:55:29  Epoch【265】 train_loss:0.00283  val_loss:0.00292\n","2023-10-25 15:55:31  Epoch【266】 train_loss:0.00204  val_loss:0.00257\n","best_test_loss: 0.00257128571494199\n","2023-10-25 15:55:33  Epoch【267】 train_loss:0.00242  val_loss:0.00283\n","2023-10-25 15:55:35  Epoch【268】 train_loss:0.00205  val_loss:0.00255\n","best_test_loss: 0.0025541728321948776\n","2023-10-25 15:55:37  Epoch【269】 train_loss:0.00272  val_loss:0.00287\n","2023-10-25 15:55:39  Epoch【270】 train_loss:0.00249  val_loss:0.00273\n","2023-10-25 15:55:40  Epoch【271】 train_loss:0.00201  val_loss:0.00257\n","2023-10-25 15:55:42  Epoch【272】 train_loss:0.00232  val_loss:0.00274\n","2023-10-25 15:55:44  Epoch【273】 train_loss:0.002  val_loss:0.00249\n","best_test_loss: 0.0024939075810834765\n","2023-10-25 15:55:46  Epoch【274】 train_loss:0.00279  val_loss:0.00288\n","2023-10-25 15:55:48  Epoch【275】 train_loss:0.0022  val_loss:0.00256\n","2023-10-25 15:55:49  Epoch【276】 train_loss:0.00201  val_loss:0.00257\n","2023-10-25 15:55:51  Epoch【277】 train_loss:0.00216  val_loss:0.00263\n","2023-10-25 15:55:53  Epoch【278】 train_loss:0.00198  val_loss:0.00246\n","best_test_loss: 0.0024629264396777294\n","2023-10-25 15:55:55  Epoch【279】 train_loss:0.00268  val_loss:0.0028\n","2023-10-25 15:55:56  Epoch【280】 train_loss:0.00202  val_loss:0.00246\n","best_test_loss: 0.0024603686482726557\n","2023-10-25 15:55:58  Epoch【281】 train_loss:0.00196  val_loss:0.00252\n","2023-10-25 15:56:00  Epoch【282】 train_loss:0.00204  val_loss:0.00253\n","2023-10-25 15:56:02  Epoch【283】 train_loss:0.00192  val_loss:0.00241\n","best_test_loss: 0.002410730713998656\n","2023-10-25 15:56:04  Epoch【284】 train_loss:0.00256  val_loss:0.00272\n","2023-10-25 15:56:05  Epoch【285】 train_loss:0.00195  val_loss:0.0024\n","best_test_loss: 0.0024045189721104893\n","2023-10-25 15:56:08  Epoch【286】 train_loss:0.00187  val_loss:0.00244\n","2023-10-25 15:56:10  Epoch【287】 train_loss:0.00198  val_loss:0.00248\n","2023-10-25 15:56:11  Epoch【288】 train_loss:0.00181  val_loss:0.00233\n","best_test_loss: 0.0023332814098862204\n","2023-10-25 15:56:13  Epoch【289】 train_loss:0.00245  val_loss:0.00265\n","2023-10-25 15:56:15  Epoch【290】 train_loss:0.00198  val_loss:0.00239\n","2023-10-25 15:56:17  Epoch【291】 train_loss:0.00174  val_loss:0.00234\n","2023-10-25 15:56:19  Epoch【292】 train_loss:0.00197  val_loss:0.00246\n","2023-10-25 15:56:20  Epoch【293】 train_loss:0.00169  val_loss:0.00225\n","best_test_loss: 0.0022543074244268887\n","2023-10-25 15:56:22  Epoch【294】 train_loss:0.00229  val_loss:0.00254\n","2023-10-25 15:56:24  Epoch【295】 train_loss:0.0021  val_loss:0.00243\n","2023-10-25 15:56:26  Epoch【296】 train_loss:0.00161  val_loss:0.00223\n","best_test_loss: 0.00223324925173074\n","2023-10-25 15:56:28  Epoch【297】 train_loss:0.00192  val_loss:0.00243\n","2023-10-25 15:56:29  Epoch【298】 train_loss:0.00164  val_loss:0.00222\n","best_test_loss: 0.002223535043531624\n","2023-10-25 15:56:31  Epoch【299】 train_loss:0.002  val_loss:0.00238\n","2023-10-25 15:56:33  Epoch【300】 train_loss:0.00225  val_loss:0.00249\n","2023-10-25 15:56:35  Epoch【301】 train_loss:0.00159  val_loss:0.00217\n","best_test_loss: 0.002173949445537089\n","2023-10-25 15:56:36  Epoch【302】 train_loss:0.00177  val_loss:0.00233\n","2023-10-25 15:56:38  Epoch【303】 train_loss:0.00172  val_loss:0.00227\n","2023-10-25 15:56:40  Epoch【304】 train_loss:0.00166  val_loss:0.00219\n","2023-10-25 15:56:42  Epoch【305】 train_loss:0.00224  val_loss:0.00247\n","2023-10-25 15:56:43  Epoch【306】 train_loss:0.00181  val_loss:0.00224\n","2023-10-25 15:56:45  Epoch【307】 train_loss:0.00153  val_loss:0.00215\n","best_test_loss: 0.002152882736560929\n","2023-10-25 15:56:47  Epoch【308】 train_loss:0.0018  val_loss:0.00231\n","2023-10-25 15:56:49  Epoch【309】 train_loss:0.00151  val_loss:0.00211\n","best_test_loss: 0.0021053837743871236\n","2023-10-25 15:56:51  Epoch【310】 train_loss:0.00188  val_loss:0.00227\n","2023-10-25 15:56:52  Epoch【311】 train_loss:0.00211  val_loss:0.00238\n","2023-10-25 15:56:54  Epoch【312】 train_loss:0.00151  val_loss:0.00208\n","best_test_loss: 0.00207874359836316\n","2023-10-25 15:56:56  Epoch【313】 train_loss:0.0016  val_loss:0.00219\n","2023-10-25 15:56:58  Epoch【314】 train_loss:0.00167  val_loss:0.0022\n","2023-10-25 15:56:59  Epoch【315】 train_loss:0.00146  val_loss:0.00204\n","best_test_loss: 0.002044829147685643\n","2023-10-25 15:57:01  Epoch【316】 train_loss:0.00199  val_loss:0.00231\n","2023-10-25 15:57:03  Epoch【317】 train_loss:0.0019  val_loss:0.00224\n","2023-10-25 15:57:05  Epoch【318】 train_loss:0.00139  val_loss:0.00201\n","best_test_loss: 0.0020068438556652677\n","2023-10-25 15:57:07  Epoch【319】 train_loss:0.00162  val_loss:0.00217\n","2023-10-25 15:57:08  Epoch【320】 train_loss:0.00152  val_loss:0.00209\n","2023-10-25 15:57:10  Epoch【321】 train_loss:0.00146  val_loss:0.00202\n","2023-10-25 15:57:12  Epoch【322】 train_loss:0.00197  val_loss:0.00227\n","2023-10-25 15:57:14  Epoch【323】 train_loss:0.00174  val_loss:0.00214\n","2023-10-25 15:57:15  Epoch【324】 train_loss:0.00133  val_loss:0.00196\n","best_test_loss: 0.001959079770249841\n","2023-10-25 15:57:17  Epoch【325】 train_loss:0.00157  val_loss:0.00212\n","2023-10-25 15:57:19  Epoch【326】 train_loss:0.00144  val_loss:0.00202\n","2023-10-25 15:57:21  Epoch【327】 train_loss:0.00142  val_loss:0.00197\n","2023-10-25 15:57:23  Epoch【328】 train_loss:0.0019  val_loss:0.00221\n","2023-10-25 15:57:24  Epoch【329】 train_loss:0.00167  val_loss:0.00208\n","2023-10-25 15:57:26  Epoch【330】 train_loss:0.00127  val_loss:0.00191\n","best_test_loss: 0.0019060903578065336\n","2023-10-25 15:57:28  Epoch【331】 train_loss:0.0015  val_loss:0.00206\n","2023-10-25 15:57:30  Epoch【332】 train_loss:0.00141  val_loss:0.00198\n","2023-10-25 15:57:31  Epoch【333】 train_loss:0.00132  val_loss:0.0019\n","best_test_loss: 0.0019027520712647263\n","2023-10-25 15:57:33  Epoch【334】 train_loss:0.00179  val_loss:0.00214\n","2023-10-25 15:57:35  Epoch【335】 train_loss:0.00171  val_loss:0.00208\n","2023-10-25 15:57:37  Epoch【336】 train_loss:0.00124  val_loss:0.00186\n","best_test_loss: 0.0018570206606375944\n","2023-10-25 15:57:39  Epoch【337】 train_loss:0.00139  val_loss:0.00197\n","2023-10-25 15:57:40  Epoch【338】 train_loss:0.00142  val_loss:0.00198\n","2023-10-25 15:57:42  Epoch【339】 train_loss:0.00122  val_loss:0.00183\n","best_test_loss: 0.0018334108994484109\n","2023-10-25 15:57:44  Epoch【340】 train_loss:0.0016  val_loss:0.00202\n","2023-10-25 15:57:46  Epoch【341】 train_loss:0.00178  val_loss:0.0021\n","2023-10-25 15:57:48  Epoch【342】 train_loss:0.0013  val_loss:0.00185\n","2023-10-25 15:57:49  Epoch【343】 train_loss:0.00123  val_loss:0.00185\n","2023-10-25 15:57:51  Epoch【344】 train_loss:0.00142  val_loss:0.00196\n","2023-10-25 15:57:53  Epoch【345】 train_loss:0.0012  val_loss:0.00182\n","best_test_loss: 0.001815703773425649\n","2023-10-25 15:57:55  Epoch【346】 train_loss:0.00132  val_loss:0.00185\n","2023-10-25 15:57:57  Epoch【347】 train_loss:0.00173  val_loss:0.00206\n","2023-10-25 15:57:58  Epoch【348】 train_loss:0.00149  val_loss:0.00193\n","2023-10-25 15:58:00  Epoch【349】 train_loss:0.00112  val_loss:0.00175\n","best_test_loss: 0.0017535675357541313\n","2023-10-25 15:58:02  Epoch【350】 train_loss:0.00129  val_loss:0.00187\n","2023-10-25 15:58:05  Epoch【351】 train_loss:0.00129  val_loss:0.00186\n","2023-10-25 15:58:07  Epoch【352】 train_loss:0.00111  val_loss:0.00173\n","best_test_loss: 0.0017327721162354978\n","2023-10-25 15:58:09  Epoch【353】 train_loss:0.00144  val_loss:0.00189\n","2023-10-25 15:58:11  Epoch【354】 train_loss:0.00166  val_loss:0.002\n","2023-10-25 15:58:12  Epoch【355】 train_loss:0.00127  val_loss:0.00179\n","2023-10-25 15:58:14  Epoch【356】 train_loss:0.00108  val_loss:0.00172\n","best_test_loss: 0.0017194082041817676\n","2023-10-25 15:58:16  Epoch【357】 train_loss:0.00128  val_loss:0.00184\n","2023-10-25 15:58:18  Epoch【358】 train_loss:0.00117  val_loss:0.00176\n","2023-10-25 15:58:19  Epoch【359】 train_loss:0.00108  val_loss:0.00169\n","best_test_loss: 0.00169358927624948\n","2023-10-25 15:58:21  Epoch【360】 train_loss:0.00146  val_loss:0.00189\n","2023-10-25 15:58:23  Epoch【361】 train_loss:0.00156  val_loss:0.00193\n","2023-10-25 15:58:25  Epoch【362】 train_loss:0.00115  val_loss:0.00171\n","2023-10-25 15:58:27  Epoch【363】 train_loss:0.00104  val_loss:0.00168\n","best_test_loss: 0.001680280428642974\n","2023-10-25 15:58:28  Epoch【364】 train_loss:0.00122  val_loss:0.00179\n","2023-10-25 15:58:30  Epoch【365】 train_loss:0.00109  val_loss:0.0017\n","2023-10-25 15:58:32  Epoch【366】 train_loss:0.00104  val_loss:0.00165\n","best_test_loss: 0.0016485392938509744\n","2023-10-25 15:58:34  Epoch【367】 train_loss:0.00141  val_loss:0.00184\n","2023-10-25 15:58:36  Epoch【368】 train_loss:0.00151  val_loss:0.00188\n","2023-10-25 15:58:37  Epoch【369】 train_loss:0.00112  val_loss:0.00167\n","2023-10-25 15:58:39  Epoch【370】 train_loss:0.00098  val_loss:0.00162\n","best_test_loss: 0.001623900402236082\n","2023-10-25 15:58:41  Epoch【371】 train_loss:0.00116  val_loss:0.00173\n","2023-10-25 15:58:43  Epoch【372】 train_loss:0.00107  val_loss:0.00166\n","2023-10-25 15:58:45  Epoch【373】 train_loss:0.00097  val_loss:0.00159\n","best_test_loss: 0.0015898598421304284\n","2023-10-25 15:58:46  Epoch【374】 train_loss:0.0013  val_loss:0.00175\n","2023-10-25 15:58:48  Epoch【375】 train_loss:0.00149  val_loss:0.00185\n","2023-10-25 15:58:50  Epoch【376】 train_loss:0.00116  val_loss:0.00167\n","2023-10-25 15:58:52  Epoch【377】 train_loss:0.00092  val_loss:0.00156\n","best_test_loss: 0.0015610922899846312\n","2023-10-25 15:58:54  Epoch【378】 train_loss:0.00107  val_loss:0.00166\n","2023-10-25 15:58:55  Epoch【379】 train_loss:0.00108  val_loss:0.00165\n","2023-10-25 15:58:57  Epoch【380】 train_loss:0.00091  val_loss:0.00154\n","best_test_loss: 0.0015407883076240187\n","2023-10-25 15:58:59  Epoch【381】 train_loss:0.0011  val_loss:0.00163\n","2023-10-25 15:59:01  Epoch【382】 train_loss:0.00144  val_loss:0.0018\n","2023-10-25 15:59:02  Epoch【383】 train_loss:0.00129  val_loss:0.00172\n","2023-10-25 15:59:04  Epoch【384】 train_loss:0.00093  val_loss:0.00153\n","best_test_loss: 0.001529954511763366\n","2023-10-25 15:59:06  Epoch【385】 train_loss:0.00094  val_loss:0.00156\n","2023-10-25 15:59:08  Epoch【386】 train_loss:0.00107  val_loss:0.00163\n","2023-10-25 15:59:09  Epoch【387】 train_loss:0.00093  val_loss:0.00154\n","2023-10-25 15:59:11  Epoch【388】 train_loss:0.0009  val_loss:0.0015\n","best_test_loss: 0.0015036403669712497\n","2023-10-25 15:59:13  Epoch【389】 train_loss:0.00122  val_loss:0.00167\n","2023-10-25 15:59:15  Epoch【390】 train_loss:0.00139  val_loss:0.00175\n","2023-10-25 15:59:17  Epoch【391】 train_loss:0.00109  val_loss:0.00159\n","2023-10-25 15:59:19  Epoch【392】 train_loss:0.00084  val_loss:0.00147\n","best_test_loss: 0.0014652674562176285\n","2023-10-25 15:59:21  Epoch【393】 train_loss:0.00095  val_loss:0.00154\n","2023-10-25 15:59:24  Epoch【394】 train_loss:0.001  val_loss:0.00157\n","2023-10-25 15:59:26  Epoch【395】 train_loss:0.00084  val_loss:0.00146\n","best_test_loss: 0.0014626967578964388\n","2023-10-25 15:59:28  Epoch【396】 train_loss:0.00091  val_loss:0.00148\n","2023-10-25 15:59:30  Epoch【397】 train_loss:0.00125  val_loss:0.00166\n","2023-10-25 15:59:32  Epoch【398】 train_loss:0.00132  val_loss:0.00169\n","2023-10-25 15:59:33  Epoch【399】 train_loss:0.001  val_loss:0.00152\n","2023-10-25 15:59:35  Epoch【400】 train_loss:0.00079  val_loss:0.00142\n","best_test_loss: 0.0014189907081384697\n","2023-10-25 15:59:37  Epoch【401】 train_loss:0.00091  val_loss:0.00149\n","2023-10-25 15:59:39  Epoch【402】 train_loss:0.00095  val_loss:0.00151\n","2023-10-25 15:59:40  Epoch【403】 train_loss:0.0008  val_loss:0.00141\n","best_test_loss: 0.0014120761568293624\n","2023-10-25 15:59:42  Epoch【404】 train_loss:0.00086  val_loss:0.00144\n","2023-10-25 15:59:44  Epoch【405】 train_loss:0.00119  val_loss:0.00161\n","2023-10-25 15:59:46  Epoch【406】 train_loss:0.00128  val_loss:0.00165\n","2023-10-25 15:59:48  Epoch【407】 train_loss:0.00099  val_loss:0.00149\n","2023-10-25 15:59:49  Epoch【408】 train_loss:0.00076  val_loss:0.00137\n","best_test_loss: 0.0013735960490758653\n","2023-10-25 15:59:51  Epoch【409】 train_loss:0.00083  val_loss:0.00143\n","2023-10-25 15:59:53  Epoch【410】 train_loss:0.0009  val_loss:0.00146\n","2023-10-25 15:59:55  Epoch【411】 train_loss:0.00078  val_loss:0.00138\n","2023-10-25 15:59:57  Epoch【412】 train_loss:0.00077  val_loss:0.00136\n","best_test_loss: 0.0013638027531155587\n","2023-10-25 15:59:59  Epoch【413】 train_loss:0.00105  val_loss:0.00151\n","2023-10-25 16:00:00  Epoch【414】 train_loss:0.00125  val_loss:0.00161\n","2023-10-25 16:00:02  Epoch【415】 train_loss:0.00106  val_loss:0.00151\n","2023-10-25 16:00:04  Epoch【416】 train_loss:0.00077  val_loss:0.00135\n","best_test_loss: 0.0013530510477721691\n","2023-10-25 16:00:06  Epoch【417】 train_loss:0.00073  val_loss:0.00134\n","best_test_loss: 0.0013438414405201279\n","2023-10-25 16:00:07  Epoch【418】 train_loss:0.00084  val_loss:0.00141\n","2023-10-25 16:00:09  Epoch【419】 train_loss:0.0008  val_loss:0.00138\n","2023-10-25 16:00:11  Epoch【420】 train_loss:0.00069  val_loss:0.00131\n","best_test_loss: 0.0013075106854439186\n","2023-10-25 16:00:13  Epoch【421】 train_loss:0.00083  val_loss:0.00137\n","2023-10-25 16:00:15  Epoch【422】 train_loss:0.00112  val_loss:0.00153\n","2023-10-25 16:00:16  Epoch【423】 train_loss:0.00117  val_loss:0.00155\n","2023-10-25 16:00:18  Epoch【424】 train_loss:0.00091  val_loss:0.0014\n","2023-10-25 16:00:20  Epoch【425】 train_loss:0.00069  val_loss:0.00129\n","best_test_loss: 0.001289018489288814\n","2023-10-25 16:00:22  Epoch【426】 train_loss:0.00072  val_loss:0.00132\n","2023-10-25 16:00:24  Epoch【427】 train_loss:0.0008  val_loss:0.00136\n","2023-10-25 16:00:25  Epoch【428】 train_loss:0.00073  val_loss:0.00131\n","2023-10-25 16:00:27  Epoch【429】 train_loss:0.00066  val_loss:0.00126\n","best_test_loss: 0.0012637357475519505\n","2023-10-25 16:00:29  Epoch【430】 train_loss:0.00083  val_loss:0.00135\n","2023-10-25 16:00:31  Epoch【431】 train_loss:0.0011  val_loss:0.0015\n","2023-10-25 16:00:33  Epoch【432】 train_loss:0.00112  val_loss:0.0015\n","2023-10-25 16:00:35  Epoch【433】 train_loss:0.00086  val_loss:0.00136\n","2023-10-25 16:00:37  Epoch【434】 train_loss:0.00065  val_loss:0.00125\n","best_test_loss: 0.0012471659416484688\n","2023-10-25 16:00:39  Epoch【435】 train_loss:0.00068  val_loss:0.00127\n","2023-10-25 16:00:40  Epoch【436】 train_loss:0.00076  val_loss:0.00131\n","2023-10-25 16:00:42  Epoch【437】 train_loss:0.0007  val_loss:0.00128\n","2023-10-25 16:00:44  Epoch【438】 train_loss:0.00063  val_loss:0.00122\n","best_test_loss: 0.0012218768698284807\n","2023-10-25 16:00:46  Epoch【439】 train_loss:0.00075  val_loss:0.00128\n","2023-10-25 16:00:47  Epoch【440】 train_loss:0.00101  val_loss:0.00142\n","2023-10-25 16:00:49  Epoch【441】 train_loss:0.0011  val_loss:0.00147\n","2023-10-25 16:00:51  Epoch【442】 train_loss:0.0009  val_loss:0.00136\n","2023-10-25 16:00:53  Epoch【443】 train_loss:0.00066  val_loss:0.00123\n","2023-10-25 16:00:55  Epoch【444】 train_loss:0.00062  val_loss:0.00121\n","best_test_loss: 0.0012062983954345564\n","2023-10-25 16:00:56  Epoch【445】 train_loss:0.0007  val_loss:0.00125\n","2023-10-25 16:00:58  Epoch【446】 train_loss:0.0007  val_loss:0.00125\n","2023-10-25 16:01:00  Epoch【447】 train_loss:0.00062  val_loss:0.0012\n","best_test_loss: 0.0011960562728547855\n","2023-10-25 16:01:02  Epoch【448】 train_loss:0.00062  val_loss:0.00119\n","best_test_loss: 0.0011927005252800882\n","2023-10-25 16:01:04  Epoch【449】 train_loss:0.00083  val_loss:0.0013\n","2023-10-25 16:01:06  Epoch【450】 train_loss:0.00104  val_loss:0.00141\n","2023-10-25 16:01:07  Epoch【451】 train_loss:0.00101  val_loss:0.00139\n","2023-10-25 16:01:09  Epoch【452】 train_loss:0.00077  val_loss:0.00126\n","2023-10-25 16:01:11  Epoch【453】 train_loss:0.0006  val_loss:0.00117\n","best_test_loss: 0.001167759076570687\n","2023-10-25 16:01:13  Epoch【454】 train_loss:0.0006  val_loss:0.00117\n","2023-10-25 16:01:14  Epoch【455】 train_loss:0.00067  val_loss:0.00121\n","2023-10-25 16:01:16  Epoch【456】 train_loss:0.00066  val_loss:0.0012\n","2023-10-25 16:01:18  Epoch【457】 train_loss:0.00058  val_loss:0.00115\n","best_test_loss: 0.001148762948856608\n","2023-10-25 16:01:20  Epoch【458】 train_loss:0.00061  val_loss:0.00116\n","2023-10-25 16:01:22  Epoch【459】 train_loss:0.00081  val_loss:0.00127\n","2023-10-25 16:01:24  Epoch【460】 train_loss:0.001  val_loss:0.00137\n","2023-10-25 16:01:26  Epoch【461】 train_loss:0.00097  val_loss:0.00135\n","2023-10-25 16:01:28  Epoch【462】 train_loss:0.00075  val_loss:0.00123\n","2023-10-25 16:01:29  Epoch【463】 train_loss:0.00058  val_loss:0.00113\n","best_test_loss: 0.0011328935721868893\n","2023-10-25 16:01:31  Epoch【464】 train_loss:0.00056  val_loss:0.00113\n","best_test_loss: 0.0011269300979936656\n","2023-10-25 16:01:33  Epoch【465】 train_loss:0.00063  val_loss:0.00116\n","2023-10-25 16:01:35  Epoch【466】 train_loss:0.00064  val_loss:0.00116\n","2023-10-25 16:01:37  Epoch【467】 train_loss:0.00057  val_loss:0.00112\n","best_test_loss: 0.0011196959518280853\n","2023-10-25 16:01:39  Epoch【468】 train_loss:0.00055  val_loss:0.00111\n","best_test_loss: 0.0011063344433711356\n","2023-10-25 16:01:41  Epoch【469】 train_loss:0.0007  val_loss:0.00118\n","2023-10-25 16:01:43  Epoch【470】 train_loss:0.00091  val_loss:0.0013\n","2023-10-25 16:01:45  Epoch【471】 train_loss:0.00097  val_loss:0.00133\n","2023-10-25 16:01:46  Epoch【472】 train_loss:0.00082  val_loss:0.00125\n","2023-10-25 16:01:48  Epoch【473】 train_loss:0.00062  val_loss:0.00113\n","2023-10-25 16:01:50  Epoch【474】 train_loss:0.00053  val_loss:0.00108\n","best_test_loss: 0.0010805855639299134\n","2023-10-25 16:01:52  Epoch【475】 train_loss:0.00056  val_loss:0.0011\n","2023-10-25 16:01:54  Epoch【476】 train_loss:0.00061  val_loss:0.00113\n","2023-10-25 16:01:55  Epoch【477】 train_loss:0.00059  val_loss:0.00111\n","2023-10-25 16:01:57  Epoch【478】 train_loss:0.00053  val_loss:0.00107\n","best_test_loss: 0.0010701433582099803\n","2023-10-25 16:01:59  Epoch【479】 train_loss:0.00055  val_loss:0.00108\n","2023-10-25 16:02:01  Epoch【480】 train_loss:0.00072  val_loss:0.00117\n","2023-10-25 16:02:02  Epoch【481】 train_loss:0.00089  val_loss:0.00126\n","2023-10-25 16:02:04  Epoch【482】 train_loss:0.00091  val_loss:0.00128\n","2023-10-25 16:02:06  Epoch【483】 train_loss:0.00076  val_loss:0.00119\n","2023-10-25 16:02:08  Epoch【484】 train_loss:0.00058  val_loss:0.00109\n","2023-10-25 16:02:10  Epoch【485】 train_loss:0.0005  val_loss:0.00104\n","best_test_loss: 0.0010420437509943124\n","2023-10-25 16:02:12  Epoch【486】 train_loss:0.00053  val_loss:0.00106\n","2023-10-25 16:02:14  Epoch【487】 train_loss:0.00058  val_loss:0.00108\n","2023-10-25 16:02:15  Epoch【488】 train_loss:0.00057  val_loss:0.00107\n","2023-10-25 16:02:17  Epoch【489】 train_loss:0.00051  val_loss:0.00104\n","best_test_loss: 0.001038700676872395\n","2023-10-25 16:02:19  Epoch【490】 train_loss:0.00052  val_loss:0.00104\n","best_test_loss: 0.0010361042400093181\n","2023-10-25 16:02:21  Epoch【491】 train_loss:0.00064  val_loss:0.0011\n","2023-10-25 16:02:23  Epoch【492】 train_loss:0.00081  val_loss:0.0012\n","2023-10-25 16:02:25  Epoch【493】 train_loss:0.00088  val_loss:0.00124\n","2023-10-25 16:02:26  Epoch【494】 train_loss:0.00078  val_loss:0.00118\n","2023-10-25 16:02:28  Epoch【495】 train_loss:0.00061  val_loss:0.00108\n","2023-10-25 16:02:30  Epoch【496】 train_loss:0.0005  val_loss:0.00102\n","best_test_loss: 0.0010163660778704545\n","2023-10-25 16:02:32  Epoch【497】 train_loss:0.00048  val_loss:0.00101\n","best_test_loss: 0.001006356586003676\n","2023-10-25 16:02:34  Epoch【498】 train_loss:0.00053  val_loss:0.00103\n","2023-10-25 16:02:35  Epoch【499】 train_loss:0.00056  val_loss:0.00104\n"]}],"source":["train_val()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1698056020384,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"3pxGaXdMw6Tg"},"outputs":[],"source":["####test部分\n","def test():\n","    model.load_state_dict(torch.load('./logs/best_Transformer_trainModel.pth'))\n","    model.to(device)\n","    model.eval()\n","    y_pred = []\n","    for X_test,y_test in test_loader:\n","        X_tt = X_test.clone().detach().requires_grad_(True)\n","        # Y_tt = y_test.clone().detach().requires_grad_(True)\n","        output = model(X_tt.to(device)).to(device)\n","        # output = output.cpu().detach().numpy()[0]\n","        y_pred.append(output)\n","        # Y_tt = Y_tt.cpu().detach().numpy()[0] #将Y_tt从GPU移动到CPU，并释放GPU上的内存,并保存第一个值（这里本来是为了做对比留下来的，但是后面直接取值label_test，最后没用到）\n","    # print('test预测')\n","    pred = [tensor.detach().cpu().numpy() for tensor in y_pred]\n","    pred = np.array(pred)\n","    y_preds = pred\n","    return y_preds"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["lag: 1\n"]}],"source":["print('lag:',lag)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1698056020384,"user":{"displayName":"李敏芳","userId":"00450754197420987513"},"user_tz":-480},"id":"UpjdZ3KRZKwy"},"outputs":[],"source":["preds = test()\n","preds = preds.reshape(-1, 24)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"SCUwY4ih305P"},"outputs":[{"name":"stdout","output_type":"stream","text":["preds: [[0.9970426  0.9999771  0.99946845 ... 0.99534535 1.0155811  1.0129863 ]\n"," [1.0029305  1.0096799  0.99860317 ... 0.9786899  1.0222156  1.0397909 ]\n"," [1.001255   0.9993527  1.0016567  ... 0.992277   1.0138973  1.0500742 ]\n"," ...\n"," [1.0006012  0.99981546 1.000529   ... 0.98730683 1.0162315  1.0347909 ]\n"," [0.99995047 1.0010455  0.99757874 ... 0.9964837  1.0223261  0.9711082 ]\n"," [1.0026038  0.9986069  0.99717957 ... 0.458621   0.52386266 0.5158485 ]]\n","preds.shape: (1458, 24)\n","y_cs: (1458, 24)\n"]}],"source":["print('preds:',preds)\n","print('preds.shape:',preds.shape)\n","print('y_cs:',y_cs.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Ep8TOYD-3-C9"},"outputs":[{"name":"stdout","output_type":"stream","text":["    Time[min]  MAD[%]       R2  RMSD[%]\n","K                                      \n","1          15    1.56  0.99931     4.43\n","2          30    1.84  0.99885     5.63\n","3          45    1.66  0.99927     4.37\n","4          60    1.38  0.99950     3.51\n","5          75    2.56  0.99748     7.61\n","6          90    4.34  0.99216    13.01\n","7         105    3.74  0.99363    11.49\n","8         120    4.32  0.99153    13.09\n","9         135    3.73  0.99443    10.51\n","10        150    3.69  0.99476    10.20\n","11        165    3.21  0.99609     8.86\n","12        180    3.64  0.99476    10.47\n","13        195    3.13  0.99640     8.92\n","14        210    3.19  0.99617     9.41\n","15        225    1.92  0.99879     5.46\n","16        240    1.06  0.99976     2.51\n","17        255    1.41  0.99950     3.70\n","18        270    1.57  0.99927     4.55\n","19        285    1.75  0.99902     5.36\n","20        300    1.85  0.99895     5.61\n","21        315    2.30  0.99843     6.86\n","22        330    2.26  0.99866     6.36\n","23        345    1.47  0.99961     3.42\n","24        360   10.66  0.96902    30.16\n"]}],"source":["# transform to GHI\n","\n","y_pred2 = np.multiply(preds, y_cs)\n","\n","y_test2 = np.multiply(label_test,y_cs)\n","y_test2 =y_test2.numpy()\n","# save results\n","results = {'K':[],'Time[min]':[],'MAD[%]':[],'R2':[],'RMSD[%]':[]}\n","for i in range(K):\n","    results['K'].append(i+1)\n","    results['Time[min]'].append((i+1)*15)\n","    results['MAD[%]'].append(np.round(mad(y_pred2[:,i],y_test2[:,i]),2))\n","    results['R2'].append(np.round(r2(y_pred2[:,i],y_test2[:,i]),5))\n","    results['RMSD[%]'].append(np.round(rmsd(y_pred2[:,i],y_test2[:,i]),2))\n","\n","# create results dataframe\n","results = pd.DataFrame(results)\n","results = results.set_index('K')\n","print(results.head(K))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNdc/O2AIS1FgyQbqDk26vg","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
